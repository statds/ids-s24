[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-s24.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2023 and Spring 2022 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nOur mid-term project on rat sightings in New York City is to be showcased in a virtual session at the NYC Open Data Week, 2024, the week following the Spring Break. Four students will be selected to present topics on the mid-term project.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Add yours through a pull request; note the syntax of nested list in Markdown.\n\nChugh, Charitarth\n\nGet better at analyzing data/features\nLearn about more xgboost & gradient boosted trees.\n\nDennison, Jack\n\nLearn how to use Git and GitHub\nBe able to apply my skills in Python and Git to data analytics tasks\n\nElliott, Matt\n\nFaciliate myself into becoming a Data Scientist\nLearn new skills such as Quarto and GitHub\n\nLee, Joshua\n\nImprove model optimization techniques\nlearn how to conduct better feature engineering\nlearn how to perform better model selection and feature selection\nlearn how to deploy ml models and processes to the cloud\n\nMori, Abigail\n\nBecome proficient using Git\nLearn how to properly communiacte statistical evidence and findings\n\nMassad, Olivia\n\nBe able to use Git effectively\nGain knowledge about Data Science and its importance\n\nNguyen, Leon\n\nBecome proficient in utilizing Git and GitHub workflow processes\nDevelop proficiency in Quarto and Python packages\nCreate a data science project start to finish for portfolio work\n\nPatel, Pratham\n\nBecome more proficient and efficient with GitHub and Python\nGet a deeper understanding and appreciate of the Data Science workflow\nUnderstand collaboration and project creation on GitHub\n\nPerez, Isabelle\n\nBecome comfortable working with git and quarto\nLearn data management strategies and the relevant programming skills\n\nPugh, Alex\n\nIncrease my knowledge of Git and Python\nLearn to efficiently clean a data set\n\nQualls, William\n\nBetter understand the Data Science Pipeline\nGain practical knowledge with tools such as Github that aren’t covered in other classes\n\nSchober, Henry\n\nBe more proficient in Git and Python\nDeepen my understanding of Data Science\n\nTaki, William\n\nGet comfortable with Git and Python\nUse the learnings from this class to help with STAT 33494W\n\nWoo, Madison\n\nBe able to comfortably use Git and Python\nLearn about project managment and data science\n\nXie, Vincent\n\nBecome more proficient with Git.\nLearn how to create a proper data science project.\nBe introduced to core concepts in data science.\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\nYankson, Emmanuel\n\nGet better with python\nGet an A in STAT 3255\n\nZhang, Xingye\n\nGet better with computers.\nGet an A in STAT 3255.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\nrandom.seed(4737 + 8852 + 3196 + 2344 + 47) # jointly set by the class on 01/24/2024\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Elliott,Matt A',\n 'Wu,Weijia',\n 'Lek,Victor Khun',\n 'Taki,William Hiroyasu',\n 'Schober,Henry',\n 'Lee,Joshua Jian',\n 'Patel,Pratham Subhas',\n 'Li,Ge',\n 'Zhang,Xingye',\n 'Dennison,Jack Thomas',\n 'Massad,Olivia Grace',\n 'Perez,Isabelle Daenerys Halpine',\n 'Yankson,Emmanuel Opoku',\n 'Li,David',\n 'Mori,Abigail Kanoelani Shim',\n 'Nguyen,Leon Duc',\n 'Pugh,Alex',\n 'Chugh,Charitarth',\n 'Xie,Vincent',\n 'Vijayaraghavendra,Jyothsna',\n 'Qualls,William Wayne',\n 'Woo,Madison Nicole',\n 'Hook,Braedon',\n 'Chowaniec,Amelia Elizabeth']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nDescriptive statistics\nStatistical hypothesis tests\nStatistical modeling\nData visualization\nAccessing census and ACS data\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/07\nMatt Elliott\nData science communication skills\n\n\n02/12\nDr. Haim Bar\nDatabase management\n\n\n02/19\nWillam Taki\nVisualization with matplotlib\n\n\n02/19\nJoshua Lee\nDescriptive Statistics\n\n\n02/07\nWeijia Wu\nVisualizaiton with matplotlib and seaborn\n\n\n02/21\nPratham Patel\nHandling spatial data with geopandas\n\n\n02/21\nOlivia Massad\nGrammar of Graphics plotnine\n\n\n02/26\nXingye Zhang\nData visualizing NYC rodent dataset\n\n\n02/28\nJack Dennison\nGeographic Data Analysis\n\n\n02/28\nIsabelle Perez\nStatistical hypothesis tests scypy.stats\n\n\n03/04\nEmmanuel Yankson\nRandom Forest\n\n\n03/04\nDavid Li\n\n\n\n03/06\nAbigail Mori\nHandling spatial data with geopandas\n\n\n03/06\nLeon Nguyen\nStatistical Modeling with statsmodels\n\n\n03/25\nAlex Pugh\n\n\n\n03/25\nCharitath Chugh\nPyTorch\n\n\n03/27\nVincent Xie\nDatabase Operations with SQL\n\n\n03/27\nGe Li\nAnimation\n\n\n04/01\nWilliam Qualls\nWeb Scraping\n\n\n04/01\nMadison Woo\nData Science Ethics\n\n\n04/03\nBraedon Hook\n\n\n\n04/03\n\n\n\n\n04/08\n\n\n\n\n04/08\n\n\n\n\n04/10\n\n\n\n\n04/10\n\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/15\nMatt Elliott; Weijia Wu; William Taki; Joshua Lee; Pratham Patel\n\n\n04/17\nOlivia Massad; Ge Li; Xingye Zhang; Jack Dennison; Isabelle Perez\n\n\n04/22\nEmmanual Yankson; Davi Li; Abigail Mori; Leon Nguyen; Alex Pugh\n\n\n04/24\nCharitath Chugh; Vincent Xie; Madison Woo; Braedon Hook\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nA template for homework is in this repo: hwtemp.qmd.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-2.5348863 ,  2.1189087 ,  4.83327938,  6.44154669,  5.57171101,\n        4.25033713, -1.46731659,  2.17816408,  9.91171522, -3.2258789 ])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.05244999, 0.09969151, 0.07760749, 0.05384142, 0.0669444 ,\n       0.08513786, 0.06849947, 0.09963669, 0.01410347, 0.04248244])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n7.56 µs ± 312 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.83 µs ± 56.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n52.1 µs ± 1.4 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 243, 'switch': 370}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n5449703168\n5449703168\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4407640272\n4517736560\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Data Import/Export",
    "section": "",
    "text": "5.1 Using the Pandas Package\nThe pandas library simplifies data manipulation and analysis. It’s especially handy for dealing with CSV files.\nimport pandas as pd\n\n# Define the file name\ncsvnm = \"data/rodent_2022-2023.csv\"\n\n# Specify the strings that indicate missing values\n# Q: How would you know these?\nna_values = [\n    \"\",\n    \"0 Unspecified\",\n    \"N/A\",\n    \"na\",\n    \"na na\",\n    \"Unspecified\",\n    \"UNKNOWN\",\n]\n\ndef custom_date_parser(x):\n    return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n\n# Read the CSV file\ndf = pd.read_csv(\n    csvnm,\n    na_values = na_values,\n    parse_dates = ['Created Date', 'Closed Date'], \n    date_parser = custom_date_parser,\n    dtype = {'Latitude': 'float32', 'Longitude': 'float32'},\n)\n\n# Strip leading and trailing whitespace from the column names\ndf.columns = df.columns.str.strip()\ndf.columns = df.columns.str.replace(' ', '_', regex = False).str.lower()\n\n# Drop the 'Location' since it is redundant\n# df.drop(columns=['Location'], inplace=True)\nThe pandas package also provides some utility functions for quick summaries about the data frame.\ndf.shape\ndf.describe()\ndf.isnull().sum()\n\nunique_key                            0\ncreated_date                          0\nclosed_date                        2787\nagency                                0\nagency_name                           0\ncomplaint_type                        0\ndescriptor                            0\nlocation_type                         0\nincident_zip                          0\nincident_address                      0\nstreet_name                           0\ncross_street_1                       90\ncross_street_2                       55\nintersection_street_1               104\nintersection_street_2                69\naddress_type                          0\ncity                               1384\nlandmark                           3820\nfacility_type                     82869\nstatus                                2\ndue_date                          82869\nresolution_description             2787\nresolution_action_updated_date     2787\ncommunity_board                       8\nbbl                                2914\nborough                               8\nx_coordinate_(state_plane)          520\ny_coordinate_(state_plane)          520\nopen_data_channel_type                0\npark_facility_name                82869\npark_borough                          8\nvehicle_type                      82869\ntaxi_company_borough              82869\ntaxi_pick_up_location             82869\nbridge_highway_name               82869\nbridge_highway_direction          82869\nroad_ramp                         82869\nbridge_highway_segment            82869\nlatitude                            520\nlongitude                           520\nlocation                            520\nzip_codes                           622\ncommunity_districts                 526\nborough_boundaries                  526\ncity_council_districts              526\npolice_precincts                    526\npolice_precinct                     526\ndtype: int64\nWhat are the unique values of descriptor?\ndf.descriptor.unique()\n\narray(['Rat Sighting', 'Mouse Sighting', 'Condition Attracting Rodents',\n       'Signs of Rodents', 'Rodent Bite - PCS Only'], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#filling-missing-values",
    "href": "import.html#filling-missing-values",
    "title": "5  Data Import/Export",
    "section": "5.2 Filling Missing Values",
    "text": "5.2 Filling Missing Values\nIf geocodes are available but zip code is missing, we can use reverse geocoding to fill the zip code. This process involves querying a geocoding service with latitude and longitude to get the corresponding address details, including the ZIP code. This can be done with package geopy, which needs to be installed first: pip install geopy.\n\nimport pandas as pd\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut, GeocoderServiceError\n\n# Initialize the geocoder\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n\n# Function for reverse geocoding\ndef reverse_geocode(lat, lon):\n    try:\n        location = geolocator.reverse((lat, lon), exactly_one=True)\n        address = location.raw.get('address', {})\n        zip_code = address.get('postcode')\n        return zip_code\n    except (GeocoderTimedOut, GeocoderServiceError):\n        # Handle errors or timeouts\n        return None\n\n# Apply reverse geocoding to fill missing ZIP codes\nfor index, row in df.iterrows():\n    if pd.isnull(row['incident_zip']) and pd.notnull(row['latitude']) and pd.notnull(row['longitude']):\n        df.at[index, 'incident_zip'] = reverse_geocode(row['latitude'], row['longitude'])\n\n# Note: This can be slow for large datasets due to API rate\n# limits and network latency",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#using-appache-arrow-library",
    "href": "import.html#using-appache-arrow-library",
    "title": "5  Data Import/Export",
    "section": "5.3 Using Appache Arrow Library",
    "text": "5.3 Using Appache Arrow Library\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R.\n\ndf.to_feather('data/rodent_2022-2023.feather')\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/rodent_2022-2023.feather\")\ndff.shape\n\n(82869, 47)\n\n\nBenefits of Using Feather:\n\nEfficiency: Feather is designed to support fast reading and writing of data frames, making it ideal for analytical workflows that need to exchange large datasets between Python and R.\nCompatibility: Maintains data type integrity across Python and R, ensuring that numbers, strings, and dates/times are correctly handled and preserved.\nSimplicity: The API for reading and writing Feather files is straightforward, making it accessible to users with varying levels of programming expertise.\n\nBy using Feather format for data storage, you leverage a modern approach optimized for speed and compatibility, significantly enhancing the performance of data-intensive applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#accessing-the-census-data-with-uszipcode",
    "href": "import.html#accessing-the-census-data-with-uszipcode",
    "title": "5  Data Import/Export",
    "section": "5.4 Accessing the Census Data with uszipcode",
    "text": "5.4 Accessing the Census Data with uszipcode\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the incident_zip column is clean and consistent.\n\nprint(df['incident_zip'].isnull().sum())\n# Standardize to 5-digit codes, if necessary\ndf['incident_zip'] = df['incident_zip'].astype(str).str.zfill(5) \n\n0\n\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment by pip install uszipcode.\nNow let’s work on the rodent sightings data.\nWe will first clean the incident_zip column to ensure it only contains valid ZIP codes. Then, we will use a vectorized approach to fetch the required data for each unique ZIP code and merge this information back into the original DataFrame.\n\n# Remove rows where 'incident_zip' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['incident_zip']).copy()\nvalid_zip_df['incident_zip'] = valid_zip_df['incident_zip'].astype(str).str.zfill(5)\nunique_zips = valid_zip_df['incident_zip'].unique()\n\nSince uszipcode doesn’t inherently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple times.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'incident_zip'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"incident_zip\")\n\nmerged_df.columns\n\nIndex(['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',\n       'complaint_type', 'descriptor', 'location_type', 'incident_zip',\n       'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n       'intersection_street_1', 'intersection_street_2', 'address_type',\n       'city', 'landmark', 'facility_type', 'status', 'due_date',\n       'resolution_description', 'resolution_action_updated_date',\n       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',\n       'y_coordinate_(state_plane)', 'open_data_channel_type',\n       'park_facility_name', 'park_borough', 'vehicle_type',\n       'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name',\n       'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment',\n       'latitude', 'longitude', 'location', 'zip_codes', 'community_districts',\n       'borough_boundaries', 'city_council_districts', 'police_precincts',\n       'police_precinct', 'median_home_value', 'median_household_income'],\n      dtype='object')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "6  Communicating Data Science",
    "section": "",
    "text": "6.1 Data Science Communication Skills\nThis section was written by Matt Elliott.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communicating Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#data-science-communication-skills",
    "href": "communication.html#data-science-communication-skills",
    "title": "6  Communicating Data Science",
    "section": "",
    "text": "6.1.1 Introduction\nHi! My name is Matt Elliott and I decided to start this presentation off by introducing myself and what I hope to become moving forward! I am currently a Senior aiming to graduate in Fall 2024 with a Bachelor’s of Arts in Individualized Data Science under the CLAS’ IISP program. My primary advisor is our professor Jun Yan! Moving forward, I hope to learn valuable skill sets and ideas from both this course and the Data Science field. Even using Quarto instead of typical Google Slides is a step for me in creating new skills! The topic I chose to discuss today in class is “Data Science Communication Skills” . I find this to be one of the most crucial topics to discuss about Data Science; since Data Scientists are often the glue that keeps projects, companies, and ideas together.\n\n\n6.1.2 Describing Data Science and its Rise\n\nAccording to IBM, Data Science “combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI), and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data.\nThese insights are then used to “guide decision making” and create “strategic planning”\nAccording to the U.S. Bureau of Labor Statistics, Data Science is\n“projected to grow 35 percent from 2022 to 2032, much faster than the average for all occupations”\nThe median annual pay for Data Scientists in May 2022 was around $103,500, showing its high demand in a monetary light\n\n\n\n\nData Science\n\n\n\n\n6.1.3 Why does Communication Matter?\n\nCommunication matters in any professional setting in our lives, and especially in a field that can be extremely confusing and perplexing to those who are viewing it from an outside perspective.\nThe Data Incubator states that “You may work alongside data analysts or other scientists as part of a team, especially when handling large datasets or working on big projects. Beyond this, you may also frequently work with other teams of professionals who don’t work with data. Thus, it’s essential to be an excellent communicator to work with others effectively”. This is an important idea as being able to be a cooperative person will create partnerships that flow in the correct manner.\nData Scientists often present insights to other partners in order to facilitate goals and achievements in a professional setting.\nKaren Church for Medium writes that “Communication enables data scientists to gather all the necessary information, clarify needs and expectations of stakeholders, and align their work with broader business goals.”\n\n\n\n\nCommunication\n\n\n\n\n6.1.4 Inherent Communication Skills\n\nUsing the right communication methods\nFriendliness\nConfidence\nVolume and tone\nEmpathy\nRespect\nCues\n\n\n\n\nInherent Communication\n\n\n\n\n6.1.5 Identify your audience\n\nThe Data Incubator states that “Transferring knowledge across departments is crucial, so it’s vital to share insights and analyses in simple, clear terms that don’t overwhelm individuals with jargon or technical details.”\nIdentifying your audience and speaking their language is an important step, as it can vary to low familiarity to full comprehension of the topic\nIn a 2018 HBR.org article, Hugo Bowne-Anderson interviewed 35 data scientists on his podcast and found that their main issues were: “lack of management/financial support,” “lack of clear questions to answer,” “results not used by decision makers,” and “explaining data science to others.” from Harvard Business Review\nKnowing your audience in this situation can cover your back on how Data Scientists are treated in the field, communicating creates cooperation that can lead to the avoidance of these issues found.\n\n\n\n\nAudience\n\n\n\n\n6.1.6 Data Applictions\n\nData scientists have their hands full as many different fields and professions can use data analytics and information to facilitate their operations\nThese fields include:\n\nHealthcare\nMedia/Entertainment\nRetail\nTelecommunication\nAutomotive\nDigital Marketing\nCyber security\n\nData science communication skills vary in these fields, as some may prefer verbal information or visual information.\n\n\n\n\nApplications\n\n\n\n\n6.1.7 Storytelling\n\nStorytelling in the context of Data Science gives the audience a shared goal within understanding the topics and information given.\nThe goals of promoting improved customer service, innovation, or operation optimization need to be conveyed in a manner that is direct and professional.\nAccording to Sonali Verghese for Medium, here are the possibilities of telling a story within the Data Science field:\n\nExplain how you arrived at a particular conclusion\nJustify rationally why you approached a problem in a specific manner\nConvey interesting insights in a way that gets people to think or act differently\nPersuade your audience that your results are conclusive and can be turned into something actionable\nExpress why your findings are valuable and how they fit into the overall picture\n\nThis is an inspiring quotation that I found while researching this presentation. Valentin Mucke for Medium: “Data science is about humans. Data scientists must remember that, and not just when presenting to people with a non-technical background. It’s important to find common ground with everyone you work with to build trust and move forward effectively.\n\n\n\n\nStorytelling\n\n\n\n\n6.1.8 Data Visualization\n\nThe idea of expressing data through visualization has been a vital step for Data Scientists in the field\nExamples of Data Visualization for Data Scientists are:\n\nMany of these forms of visualization can be combined with other learned skill sets that will be mentioned in the next topic\n\n\n\n\n\nVisuals\n\n\n\n\n6.1.9 Usable Skill Sets for Data Communication\n\nCoding languages: Python, Structured Query Language, R, Visual Basic for Applications, Julia\nStatistical programming: the process of using computer programming languages to analyze and manipulate data for statistical purposes\nStatistics and probability: help predict the likelihood of future events and understand patterns in data\nMachine learning/Artificial intelligence: automates the data analysis process and makes predictions in real-time without human involvement, leading to further building and training of a data model to make real-time predictions\nStatistical visualization: the graphical representation of information and data that uses visual elements like charts, graphs, and maps, and tools to provide an accessible ways to see and understand trends, outliers, and patterns in data\nData management: process of collecting, storing, organizing and maintaining data to ensure that it is accurate and accessible to those who need it reliably throughout the data science project lifecycle\n\n\n\n\nskillsets\n\n\n\n\n6.1.10 Gather questions and Feedback\n\nAccording to the Data Incubator, a step to take before finalizing the project or an end of a report is to “consider soliciting direct feedback from your audience. It doesn’t matter if you have to prompt them to ask you questions or if they’re impatient to put your knowledge to the test—this form of interaction can help you improve your communication skills and establish a successful career as a data scientist.”\nBeing able to interact with your audience gives them a better understanding of the topic at hand, and can help avoid ambiguity that would occur if communication was not present\n\n\n\n\nFeedback\n\n\n\n\n6.1.11 Sources\nhttps://towardsdatascience.com/tell-stories-with-data-communication-in-data-science-5266f7671d7\nhttps://hbr.org/2019/01/data-science-and-the-art-of-persuasion\nhttps://towardsdatascience.com/communicating-as-a-data-scientist-why-it-matters-and-how-to-do-it-well-f1c34d28c7c4\nhttps://www.thedataincubator.com/blog/2022/10/13/improve-your-data-science-communication/\nhttps://emeritus.org/in/learn/why-communication-skills-are-important-for-a-data-analyst/\nhttps://medium.com/intercom-rad/the-most-underrated-skill-in-data-science-communication-7ed2fab82801\nhttps://www.ibm.com/topics/data-science\nhttps://www.bls.gov/ooh/math/data-scientists.htm\nhttps://medium.com/analytics-vidhya/introduction-to-data-science-28deb32878e7\nhttps://blog.jostle.me/customerresources/3-actionable-communication-tips\nhttps://www.breathehr.com/en-gb/blog/topic/employee-performance/effective-communication-is-key-to-your-business-success\nhttps://ideas.ted.com/before-your-next-presentation-or-speech-heres-the-first-thing-you-must-think-about/\nhttps://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/12/Data-Science-Applications-Edureka.jpg  https://lectera.com/info/storage/img/20210805/fa586bb6c04bf0989d70_808xFull.jpg\nhttps://thenewstack.io/7-best-practices-for-data-visualization/\nhttps://www.learningtree.com/blog/the-6-major-skill-areas-of-data-science/\nhttps://www.poynter.org/reporting-editing/2019/cohort4/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communicating Data Science</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "",
    "text": "7.1 Descriptive Statistics\nWhen you first begin working with a new dataset, it is important to develop an understanding of the data’s overall behavior. This is important for both understanding numerical and categorical data.\nFor numeric data, we can develop this understanding through the use of descriptive statistics. The goal of descriptive statistics is to understand three primary elements of a given variable [2]:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#descriptive-statistics",
    "href": "eda.html#descriptive-statistics",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "",
    "text": "Presented by Joshua Lee\n\n\n\n\ndistribution\ncentral tendency\nvariability\n\n\n7.1.1 Variable Distributions\nEvery random variable is given by a probability distribution, which is “a mathematical function that describes the probability of different possible values of a variable” [3].\nThere are a few common types of distributions which appear frequently in real-world data [3]:\n\nUniform:\nPoisson:\nBinomial:\nNormal and Standard Normal:\nGamma:\nChi-squared:\nExponential\nBeta\nT-distribution\nF-distribution\n\nUnderstanding the distribution of different variables in a given dataset can inform how we may decide to transform that data. For example, in the context of the rodent data, we are interested in the patterns which are associated with “rodent” complaints which occur.\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\ndata = pd.read_feather(\"data/rodent_2022-2023.feather\")\n\nNow that we have read in the data, we can examine the distributions of several important variables. Namely, let us examine a numerical variable which is associated with rodent sightings:\n\ndata.head(2).T\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nunique_key\n59893776\n59887523\n\n\ncreated_date\n2023-12-31 23:05:41\n2023-12-31 22:19:22\n\n\nclosed_date\n2023-12-31 23:05:41\n2024-01-03 08:47:02\n\n\nagency\nDOHMH\nDOHMH\n\n\nagency_name\nDepartment of Health and Mental Hygiene\nDepartment of Health and Mental Hygiene\n\n\ncomplaint_type\nRodent\nRodent\n\n\ndescriptor\nRat Sighting\nRat Sighting\n\n\nlocation_type\n3+ Family Apt. Building\nCommercial Building\n\n\nincident_zip\n11216\n10028\n\n\nincident_address\n265 PUTNAM AVENUE\n1538 THIRD AVENUE\n\n\nstreet_name\nPUTNAM AVENUE\nTHIRD AVENUE\n\n\ncross_street_1\nBEDFORD AVENUE\nEAST 86 STREET\n\n\ncross_street_2\nNOSTRAND AVENUE\nEAST 87 STREET\n\n\nintersection_street_1\nBEDFORD AVENUE\nEAST 86 STREET\n\n\nintersection_street_2\nNOSTRAND AVENUE\nEAST 87 STREET\n\n\naddress_type\nADDRESS\nADDRESS\n\n\ncity\nBROOKLYN\nNEW YORK\n\n\nlandmark\nPUTNAM AVENUE\n3 AVENUE\n\n\nfacility_type\nNaN\nNaN\n\n\nstatus\nClosed\nClosed\n\n\ndue_date\nNaN\nNaN\n\n\nresolution_description\nThe Department of Health and Mental Hygiene fo...\nThis service request was closed because the De...\n\n\nresolution_action_updated_date\n12/31/2023 11:05:41 PM\n12/31/2023 10:19:22 PM\n\n\ncommunity_board\n03 BROOKLYN\n08 MANHATTAN\n\n\nbbl\n3018220072.0\n1015157503.0\n\n\nborough\nBROOKLYN\nMANHATTAN\n\n\nx_coordinate_(state_plane)\n997661.0\n997076.0\n\n\ny_coordinate_(state_plane)\n188427.0\n223179.0\n\n\nopen_data_channel_type\nMOBILE\nMOBILE\n\n\npark_facility_name\nNaN\nNaN\n\n\npark_borough\nBROOKLYN\nMANHATTAN\n\n\nvehicle_type\nNaN\nNaN\n\n\ntaxi_company_borough\nNaN\nNaN\n\n\ntaxi_pick_up_location\nNaN\nNaN\n\n\nbridge_highway_name\nNaN\nNaN\n\n\nbridge_highway_direction\nNaN\nNaN\n\n\nroad_ramp\nNaN\nNaN\n\n\nbridge_highway_segment\nNaN\nNaN\n\n\nlatitude\n40.683857\n40.779243\n\n\nlongitude\n-73.951645\n-73.95369\n\n\nlocation\n(40.683855196486164, -73.95164557951071)\n(40.77924175816874, -73.95368859796383)\n\n\nzip_codes\n17618.0\n10099.0\n\n\ncommunity_districts\n69.0\n23.0\n\n\nborough_boundaries\n2.0\n4.0\n\n\ncity_council_districts\n49.0\n1.0\n\n\npolice_precincts\n51.0\n11.0\n\n\npolice_precinct\n51.0\n11.0\n\n\n\n\n\n\n\n\nIn this dataset, the most relevant numerical data to consider is the time between the opening of a rodent complaint and its closing. All of the other relevant variables are either geospatial or categorical:\n\n# convert strings into datetime objects\ndata[\"closed_date\"] =  pd.to_datetime(data[\"closed_date\"],\n                                     format=\"%m/%d/%Y %I:%M:%S %p\")\ndata[\"created_date\"] = pd.to_datetime(data[\"created_date\"],\n                                      format=\"%m/%d/%Y %I:%M:%S %p\")\n\ndata[\"time_dif\"] = data[\"closed_date\"] - data[\"created_date\"]\n\n# set the time delta as the number of hours difference\ndata[\"time_dif\"] = data[\"time_dif\"].dt.total_seconds()/3600\ndata[\"time_dif\"]\n\n0         0.000000\n1        58.461111\n2         0.000000\n3        60.344722\n4              NaN\n           ...    \n82864    51.862222\n82865     0.000000\n82866    57.840278\n82867    58.551944\n82868     0.000000\nName: time_dif, Length: 82869, dtype: float64\n\n\nNow we have a column describing the time difference between when a complaint is opened and closed. We can plot this distribution with plotly to provide a better visual representation of the distribution:\n\nNote, every value in the data is shifted up 1 for plotting purposes. Fitting an exponential distribution with parameter \\(\\lambda=0\\) exactly is not possible to fit precisely due to divide by \\(0\\) errors. Additionally, this plot ignores the location parameter provided by output from stats.expon.fit() since the mean brought up significantly by outliers at the asbolute extremes of the distribution (the higher end).\n\n\nimport plotly.graph_objects as go\nfrom scipy import stats\n\n# add a 1 to avoid weird zero errors\nresponse_dat2 = data[\"time_dif\"].dropna() + 1\n\nhist2 = go.Histogram(x=response_dat2, \n                    nbinsx=2500, \n                    opacity=0.75, \n                    name='response time', \n                    histnorm='probability density')\n\n# Calculate KDE\nscale, loc = stats.expon.fit(response_dat2.values)\nx_range = np.linspace(min(response_dat2), max(response_dat2), 10000)\nfitted_vals = stats.expon.pdf(x_range, loc=0.2, scale=scale)\nfitted_dist = go.Scatter(x=x_range, y=fitted_vals, mode=\"lines\", \n                         name=\"Fitted Exponential Distribution\")\n\n# Create a layout\nlayout = go.Layout(title='Complaint Response Time Histogram and Density',\n                   xaxis=dict(title='Complaint Response Time (hours)', range=[0,100]),\n                   yaxis=dict(title='Density', range=[0,0.2]),\n                   bargap=0.1\n                  )\n\n# Create a figure and add both the histogram and KDE\nfig = go.Figure(data=[hist2, fitted_dist], layout=layout)\n\n# Show the figure\nfig.show()\n\n                                                \n\n\nAs you can see, there is a strong right skew (the majority of observations are concentrated at the lower end of the distribution, but there are a few observations at the extreme right end).\nHere, we use pandas plotting to generate a density estimation curve.\n\nx_range = np.linspace(response_dat2.min(), response_dat2.max(), 1000)\nresponse_dat2.plot.kde(ind=x_range)\n\n\n\n\n\n\n\n\nWe can compare this density curve to plots of the exponential distribution, and see that this variable (complaint response times) closely match an exponential distribution with a very high \\(\\lambda\\) parameter value. Below is a figure displaying a series of exponential distributions for different values of \\(\\lambda\\):\n\nimport matplotlib.pyplot as plt\n\n# Define the lambda parameters\nlambdas = [0.5, 1, 2, 4, 8]\n\n# Define the x range\nx = np.linspace(0, 2*np.pi, 1000)\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Plot the exponential distribution for each lambda\nfor lam in lambdas:\n    y = lam * np.exp(-lam * x)\n    plt.plot(x, y, label=f'λ = {lam}')\n\n# Set the x-axis labels\nplt.xticks([np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], ['π/2', 'π', '3π/2', '2π'])\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.1.2 Central Tendency Measures\nNow that we have examined the distribution of the response time, it is appropriate to investigate the important measures of central tendency for the data.\nThere are three main measures of central tendency which are used:\n\nMean: The average or expected value of a random variable\n\n\\(\\overline{X} = (1/n)\\sum_{i=1}^{n} X_{i}\\) (where \\(X_{i}\\text{s}\\) are independent random samples from the same distribution)\n\nMedian: exact middle value of a random variable [5]\n\nFor even \\(n\\), \\(\\overset{\\sim}{X} = (1/2)[X_{(n/2+1)} + X_{(n/2)}]\\)\nFor odd \\(n\\), \\(\\overset{\\sim}{X} = X_{([n+1]/2)}\\)\n\nMode: the most frequently occurring value of a random variable\n\nFor the given variable (complaint response time), we can find each of the respective statistics using pandas:\n\nNOTE: pandas.Series.mode() returns the most commonly occurring value in the Series, or a Series of the most commonly occurring values if there is a tie between multiple values. It does not calculate multiple modes in the case of a multi-modal distribution. Here, Series.mode() returns \\(0\\) and \\(0.000\\dots\\) so I elected to choose the first element of that series for display.\n\n\ncentral_tendency = pd.Series(\n    {\"Mean\": response_dat2.mean(), \n     \"Median\": response_dat2.median(), \n     \"Mode\": response_dat2.mode().iloc[0]}\n)\ncentral_tendency\n\nMean      53.346003\nMedian     1.000000\nMode       1.000000\ndtype: float64\n\n\nAs you can see, the most commonly occurring value (as is obvious from the density plot) is 0. This means that the time between when a rodent sighting complaint is filed and responded to (or closed) is most likely to be 0. Additionally, it implies that more than half of all data points have a complaint response time of zero since the median is zero as well.\nIt makes sense that the mean is greater than the median in this case since the distribution is exponential and skewed to the right.\n\n\n7.1.3 Variability Measures\nAs with central tendency, there are also several relevant measures of variance [2]. These include:\n\nrange: \\(X_{(n)} - X_{(1)}\\) - the difference between the greatest observed value and the smallest one.\nstandard deviation: \\(S = \\sqrt{(1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}}\\) - the average difference of values from the observed mean of a sample.\nvariance: Square of the standard deviation of a sample \\(S^{2} = (1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}\\)\nInterquartile Range: \\(X_{[3/4]} - X_{[1/4]}\\) where \\(X_{[p]}\\) is the \\(p\\text{th}\\) sample quantile - A measure of the difference between the 1st and third quantiles of a distribution\n\nWe can easily calculate all of these values using pandas in python [6]\n\nquartiles = response_dat2.quantile([0.25, 0.75])\niqr = quartiles[0.75] - quartiles[0.25]\n\nvariability = pd.Series(\n    {\"range\": response_dat2.max() - response_dat2.min(), \n     \"standard deviation\": response_dat2.std(), \n     \"variance\": response_dat2.std()**2, \n     \"IQR\": iqr}\n)\nvariability\n\nrange                  6530.811944\nstandard deviation      183.977103\nvariance              33847.574408\nIQR                      16.492986\ndtype: float64\n\n\nWe can also use the interquartile range as a means to obtain a rudimentary measure of outliers in the data. Specifically, any observations which are a distance of \\(1.5 * IQR\\) beyond the third or first quartiles.\nSeeing as the first quartile is also the minimum in this, case we only need to be concerned with outliers at the higher end of the spectrum. We calculate the upper fence for outliers as follows [5]:\n\\(\\text{upper fence } = X_{[0.75]} + 1.5\\cdot IQR\\)\n\nupper_lim = quartiles[0.75] + 1.5*iqr\n\noutliers = response_dat2[response_dat2 &gt; upper_lim]\noutliers\n\n1          59.461111\n3          61.344722\n5         188.193889\n10         92.917222\n12        206.714722\n            ...     \n82857    1110.238611\n82860      52.264444\n82864      52.862222\n82866      58.840278\n82867      59.551944\nName: time_dif, Length: 15011, dtype: float64\n\n\nGiven the exponential nature of the distribution, it would be interesting to examine the patterns which occur in categorical variables to see if there may be any connections between those variables and the response time. It may also be useful to examine relationships between geospatial data and the response time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#univariate-categorical-descriptive-statistics",
    "href": "eda.html#univariate-categorical-descriptive-statistics",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.2 Univariate Categorical Descriptive Statistics",
    "text": "7.2 Univariate Categorical Descriptive Statistics\nDescriptive statistics for categorical data are primarily aimed at understanding the rates of occurrence for different categorical variables. These include the following measures [7]:\n\nfrequencies: number of occurrences\npercentages / relative frequencies: the percentage of observations which have a given value for a categorical variable\n\nThese sorts of metrics are often best represented by frequency distribution tables, pie-charts, and bar charts:\nFor example, let us examine the categorical variable “Borough” from the rodent data:\n\n# create a frequency distribution table\ncounts = data[\"borough\"].value_counts()\nproportions = counts/len(data)\ncumulative_proportions = proportions.cumsum()\n\nfrequency_table = pd.DataFrame(\n                    {\"Counts\": counts, \n                    \"Proportions\": proportions, \n                    \"Cumulative Proportion\": cumulative_proportions}\n)\nfrequency_table\n\n\n\n\n\n\n\n\n\nCounts\nProportions\nCumulative Proportion\n\n\n\n\nBROOKLYN\n30796\n0.371623\n0.371623\n\n\nMANHATTAN\n22641\n0.273214\n0.644837\n\n\nQUEENS\n13978\n0.168676\n0.813513\n\n\nBRONX\n12829\n0.154811\n0.968323\n\n\nSTATEN ISLAND\n2617\n0.031580\n0.999903\n\n\n\n\n\n\n\n\nThis table demonstrates that the most significant proportion of rodent sightings occurred in the borough of Brooklyn. Additionally, it indicates that Manhattan and Brooklyn collectively represent more than half of all rodent sightings which occur, while Staten Island in particular represents a relatively small proportion.\nWe can also use bar chart to represent this data:\n\n# Create a bar chart\nfig = go.Figure(data=[go.Bar(x=counts.index, y=counts.values)])\n\n# Show the figure\nfig.show()\n\n                                                \n\n\nA pie-chart also serves as a good representation of the relative frequencies of categories:\n\nfig = go.Figure(data=[go.Pie(labels=counts.index, values=counts.values, hole=.2)])\n\n# Show the figure\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#chi-squared-significance-tests-contingency-table-testing",
    "href": "eda.html#chi-squared-significance-tests-contingency-table-testing",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.3 Chi-Squared Significance Tests (Contingency Table Testing)",
    "text": "7.3 Chi-Squared Significance Tests (Contingency Table Testing)\nIn order to determine whether there exists a dependence between several categorical variables, we can use chi-squared contingency table testing. This is also referred to as the chi-squared test of independence [8]. We will examine this topic by investigating the relationship between the borough and the complaint descriptor variables in the rodents data.\nThe first step in conducting a Chi-squared significance test is to construct a contingency table.\n\ncontingency tables are frequency tables of two variables which are presented simultaneously [8].\n\nThis can be accomplished in python by utilizing the pd.crosstab() function\n\n# produce a contingency table for viewing\ncontingency_table_view = pd.crosstab(data[\"borough\"], \n                                     data[\"descriptor\"], \n                                     margins=True)\n\n# produce a contingency table for calculations\ncontingency_table = pd.crosstab(data[\"borough\"], \n                                     data[\"descriptor\"], \n                                     margins=False)\n\ncontingency_table_view\n\n\n\n\n\n\n\n\ndescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\nAll\n\n\nborough\n\n\n\n\n\n\n\n\n\n\nBRONX\n2395\n1381\n7745\n8\n1300\n12829\n\n\nBROOKLYN\n5332\n2075\n20323\n12\n3054\n30796\n\n\nMANHATTAN\n2818\n1758\n14036\n2\n4027\n22641\n\n\nQUEENS\n3105\n1219\n8449\n4\n1201\n13978\n\n\nSTATEN ISLAND\n795\n184\n1391\n0\n247\n2617\n\n\nAll\n14445\n6617\n51944\n26\n9829\n82861\n\n\n\n\n\n\n\n\nNow that we have constructed the contingency table, we are ready to begin conducting the signficance tests (for independence of Borough and Descriptor). This requires that we compute the chi-squared statistic.\nThere are multiple steps to computing the chi-squared statistic for this test, but the general test-statistic is computed as follows:\n\\[\\chi_{rows-1 * cols-1}^{2} = \\sum_{cells} \\frac{(O - E)^{2}}{E}\\]\nHere, \\(E = \\text{row sum} * \\text{col sum}/N\\) stands for the expected value of each cell, and \\(O\\) refers to the observed values. Note that \\(N\\) refers to the total observations (the right and lower-most cell value in the contingency table above)\nFirst, let’s calculate the expected values. This can be accomplished by performing the outer product of row sums and column sums for the contingency table:\n\\[\n\\begin{align}\n\\text{row\\_margins} = \\langle r_{1}, r_{2}, \\dots, r_{n}\\rangle \\\\\n\\text{col\\_margins} = \\langle c_{1}, c_{2}, \\dots, c_{m}\\rangle \\\\\n\\text{row\\_margins} \\otimes \\text{col\\_margins} = \\left[\\begin{array}{cccc}\n    r_{1}c_{1} & r_{1}c_{2} & \\dots & r_{1}c_{m} \\\\\n    r_{2}c_{1} & r_{2}c_{2} & \\dots & r_{2}c_{m} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    r_{n}c_{1} & r_{n}c_{2} & \\dots & r_{n}c_{m}\n\\end{array}\\right]\n\\end{align}\n\\]\nIn python this is calculated as:\n\nrow_margins = contingency_table_view[\"All\"]\ncol_margins = contingency_table_view.T[\"All\"]\ntotal = contingency_table_view[\"All\"][\"All\"]\n\nexpected = np.outer(row_margins, col_margins)/total\npd.DataFrame(expected, columns=contingency_table_view.columns).set_index(\n    contingency_table_view.index\n)\n\n\n\n\n\n\n\n\ndescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\nAll\n\n\nborough\n\n\n\n\n\n\n\n\n\n\nBRONX\n2236.455087\n1024.480672\n8042.258433\n4.025464\n1521.780343\n12829.0\n\n\nBROOKLYN\n5368.607910\n2459.264696\n19305.432278\n9.663123\n3653.031993\n30796.0\n\n\nMANHATTAN\n3946.962322\n1808.033900\n14193.216399\n7.104259\n2685.683120\n22641.0\n\n\nQUEENS\n2436.758065\n1116.235937\n8762.544888\n4.385996\n1658.075114\n13978.0\n\n\nSTATEN ISLAND\n456.216616\n208.984794\n1640.548002\n0.821158\n310.429430\n2617.0\n\n\nAll\n14445.000000\n6617.000000\n51944.000000\n26.000000\n9829.000000\n82861.0\n\n\n\n\n\n\n\n\nThe chi-squared statistic can be calculated directly from the (component-wise) squared difference between the original contingency table and the expected values presented above divided by the total number of observations. However, we can also use the scipy.stats package to perform the contingency test automatically.\nBefore performing this test, let us also examine the relavent hypotheses to this significance test.\n\\[\n\\begin{align}\n& H_{0}: \\text{Rodent complaint type reported and Borough are independent} \\\\\n& H_{1}: H_{0} \\text{ is false.}\n\\end{align}\n\\]\nWe assume a significance level of \\(\\alpha=0.05\\) for this test:\n\nNOTE: the contingency table without row margins is used for calculating the chi-squared test.\n\n\nfrom scipy.stats import chi2_contingency\n\nchi2_val, p, dof, expected = chi2_contingency(contingency_table)\n\npd.Series({\n    \"Chi-Squared Statistic\": chi2_val, \n    \"P-value\": p, \n    \"degrees of freedom\": dof\n})\n\nChi-Squared Statistic    2031.14984\nP-value                     0.00000\ndegrees of freedom         16.00000\ndtype: float64\n\n\nNow we can create a plot to demonstrate the location of the chi-squared statistic with respect to the chi-squared distribution\n\nx = np.arange(0, 45, 0.001)\n# x2 = np.arange(59, 60, 0.001)\n\nplt.plot(x, stats.chi2.pdf(x, df=20), label=\"df: 20\", color=\"red\")\n# plt.fill_between(x, x**4, color=\"red\", alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n&lt;!–\n\nfrom scipy.stats import chi2\n\nmax_chi_val = 59.0\nx_range = np.arange(0, 60, 0.001)\nfig = px.histogram(x=x_range, \n                   y=chi2.pdf(x_range, df=dof), \n                   labels={\"x\":\"Chi-Squared Value\", \n                           \"y\":\"Density\"}, \n                   title=\"Chi-Squared Distribution (df = {})\".format(dof))\n# create a a scatter plot of values from chi2 to chi2 (a single point)\n# and going from 0 to the y value at the critical point - a vertical\n# line\nfig.add_trace(go.Scatter(x=[max_chi_val, max_chi_val],\n                         y=[0,chi2.pdf(max_chi_val, df=dof)], \n              mode=\"lines\", \n              name=\"Critical Value\", \n              line=dict(color=\"red\", dash=\"dash\")))\nfig.update_layout(shapes=[dict(type=\"rect\", \n                               x0=max_chi_val, \n                               x1=20, \n                               y0=0,\n                               y1=chi2.pdf(max_chi_val, df=dof), \n                          fillcolor=\"rgba(0, 100, 80, 0.2)\", \n                          line=dict(width=0))], \n                  annotations=[dict(x=max_chi_val + 0.5, \n                                    y=0.02, \n                                    text=\"Area of Interest\", \n                                    showarrow=False, \n                                    font=dict(size=10, color=\"black\"))])\nfig.show()\n\n                                                \n\n\nAs you can see from the figure, the critical value we obtain (2034) is exceptionally far beyond the bounds of the distribution, that there must be a significant dependence relationship between the borough and the rodent incident type which is reported.\nMoreover, the p-value returned for this test is 0.00000, meaning that there is virtually 0 probability that such observations would be made given that the borough and rodent incident type reported were independent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#sources",
    "href": "eda.html#sources",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.4 Sources",
    "text": "7.4 Sources\n\ntowardsdatascience.com - Exploratory data analysis\nscribbr.com - Descriptive statistics\nscribbr.com - Probability Distributions\nmathisfun.com - Median definition\nstats.libretexts - outliers and sample quantiles\ndatagy.io - calculating IQR in python\ncurtin university - descriptive statistics for categorical data\ndwstockburger.com - hypothesis testing with contingency tables\naskpython.com - chi-squared testing in python\nsphweb - Hypotheses for chi-squared tests\n\nThis section was written by Isabelle Perez.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.5 Introduction",
    "text": "7.5 Introduction\nHello! My name is Isabelle Perez and I am a junior Mathematics-Statistics major and Computer Science minor. I am interested in learning about data science topics and have an interest in how statistics can improve sports analytics, specifically in baseball. Today’s topic is the scipy.stats package and the many hypothesis tests that we can perform using it.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#scipy.stats",
    "href": "eda.html#scipy.stats",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.6 Scipy.stats",
    "text": "7.6 Scipy.stats\nThe package scipy.stats is a subpackage of Scipy and contains many methods useful for statistics such as probability distributions, summary statistics, statistical tests, etc. The focus of this presentation will be on some of the many hypothesis tests that can be easily conducted using scipy.stats and will provide examples of situations in which they may be useful.\nFirstly, ensure Scipy is installed by using pip install scipy or\nconda install -c anaconda scipy.\nTo import the package, use the command import scipy.stats.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#basic-statistical-hypothesis-tests",
    "href": "eda.html#basic-statistical-hypothesis-tests",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.7 Basic Statistical Hypothesis Tests",
    "text": "7.7 Basic Statistical Hypothesis Tests\n\n7.7.1 Two-sample t-test\nH0: \\(\\mu_1 = \\mu_2\\)\nH1: \\(\\mu_1 \\neq\\) or \\(&gt;\\) or \\(&lt;\\) \\(\\mu_2\\)\n\n\nCode: scipy.stats.ttest_ind(sample_1, sample_2)\nAssumptions: Observations are independent and identically distributed (i.i.d), normally distributed, and the two samples have equal variances.\nOptional Parameters:\n\nnan_policy can be set to propagate (return nan), raise (raise ValueError),\nor omit (ignore null values).\nalternative can be two-sided (default), less, or greater.\nequal_var is a boolean representing whether the variances of the two samples are equal\n(default is True).\naxis defines the axis along which the statistic should be computed (default is 0).\n\nReturns: The t-statisic, a corresponding p-value, and the degrees of freedom.\n\n\n7.7.2 Paired t-test\nH0: \\(\\mu_1 = \\mu_2\\)\nH1: \\(\\mu_1 \\neq\\) or \\(&gt;\\) or \\(&lt;\\) \\(\\mu_2\\)\n\n\nCode: scipy.stats.ttest_rel(sample_1, sample_2)\nAssumptions: Observations are i.i.d, normally distributed, and related, and the two samples have equal variances. The input arrays must also be of the same size since the observations are paired.\nOptional Parameters: Can use nan_policy or alternative.\nReturns: The t-statisic, a corresponding p-value, and the degrees of freedom. Also has a method called confidence_interval with input parameter confidence_level that returns a tuple with the confidence interval around the difference in population means of the two samples.\n\n\n7.7.3 ANOVA\nH0: \\(\\mu_1 = \\mu_2 = ... = \\mu_n\\)\nH1: at least two \\(\\mu_i\\) are not equal\n\n\nCode: scipy.stats.f_oneway(sample_1, sample_2, ..., sample_n)\nAssumptions: Samples are i.i.d., normally distributed, and the samples have equal variances.\nErrors:\n\nRaises ConstantInputWarning if all values in each of the inputs are identical.\nRaises DegenerateDataWarning if any input has length \\(0\\) or all inputs have length \\(1\\).\n\nReturns: The F-statistic and a corresponding p-value.\n\n\n7.7.4 Example: Comparison of Mean Response Times by Borough\nLooking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests,\nthere are many ways a two-sample t-test may be useful. For example, we can consider samples drawn from different boroughs and perform this hypothesis test to identify whether their mean response times differ. If so, this may suggest that some boroughs are being underserviced.\n\nimport pandas as pd \nimport numpy as np \nimport scipy.stats   \n\n# read in file \ndf = pd.read_csv('data/rodent_2022-2023.csv')  \n\n# data cleaning - change dates to timestamp object\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\ndf['Closed Date'] = pd.to_datetime(df['Closed Date'])\n\n# add column Response Time \ndf['Response Time'] = df['Closed Date'] - df['Created Date']\n\n# convert data to total seconds\ndf['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600)    \n\nSince the two-sample t-test assumes the data is drawn from a normal distribution,\nwe need to ensure the samples we are comparing are normally distributed. According to the Central Limit theorem, the distribution of sample means from repeated samples of a population will be roughly normal. Therefore, we can take 100 samples of each borough’s response times, measure the mean of each sample, and perform the hypothesis test on the arrays of sample means.\n\nimport matplotlib.pyplot as plt \n\n# select Bronx and Queens boroughs \ndf_mhtn = df[df['Borough'] == 'MANHATTAN']['Response Time'] \ndf_queens = df[df['Borough'] == 'QUEENS']['Response Time']  \n\nmhtn_means = []\nqueens_means = []\n\n# create samples of sampling means \nfor i in range(100): \n  sample1 = df_mhtn.sample(1000, replace = True)\n  mhtn_means.append(sample1.mean())\n\n  sample2 = df_queens.sample(1000, replace = True) \n  queens_means.append(sample2.mean())  \n\n# plot distribution of sample means for Manhattan\nplt.hist(mhtn_means)\nplt.xlabel('Mean Response Times for Manhattan')\nplt.ylabel('Value Counts')\nplt.show()\n\n# plot distribution of sample means for Queens \nplt.hist(queens_means) \nplt.xlabel('Mean Response Times for Queens')\nplt.ylabel('Value Counts')\nplt.show() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe also need to check if the variances of the two samples are equal.\n\n# convert to numpy array \nmhtn_means = np.array(mhtn_means)\nqueens_means = np.array(queens_means)\n\nprint('Mean, variance for Manhattan', (mhtn_means.mean(), mhtn_means.std() ** 2))\nprint('Mean, variance for Queens:', (queens_means.mean(), queens_means.std() ** 2))\n\nMean, variance for Manhattan (43.92172655422854, 34.121283058192006)\nMean, variance for Queens: (69.79914979401184, 43.548495570278796)\n\n\nSince the ratio of the variances is less than \\(2\\), it is safe to assume equal variances.\n\nresult_ttest = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True)\n\nprint('t-statistic:', result_ttest.statistic)\nprint('p-value:', result_ttest.pvalue) \n# print('degrees of freedom:', result_ttest.df) \n\nt-statistic: -29.215450846557832\np-value: 1.0159036578474894e-73\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that there is a statistically significant difference in the mean of sample means drawn from rodent sighting response times for Manhattan compared to Queens.\n\nresult_ttest2 = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True, \n                                                            alternative = 'less') \n\nprint('t-statistic:', result_ttest2.statistic)\nprint('p-value:', result_ttest2.pvalue) \n# print('degrees of freedom:', result_ttest2.df) \n\nt-statistic: -29.215450846557832\np-value: 5.079518289237447e-74\n\n\nWe can also set the alternative equal to less to test if the mean of sample means drawn from the Manhattan response times is less than that of sample means drawn from Queens response times. At the alpha level of \\(0.05\\), we can also reject this null hypothesis and conclude that the mean of sample means is less for Manhattan than it is for Queens.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#normality",
    "href": "eda.html#normality",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.8 Normality",
    "text": "7.8 Normality\n\n7.8.1 Shapiro-Wilk Test\nH0: data is drawn from a normal distribution\nH1: data is not drawn from a normal distribution\n\n\nCode: scipy.stats.shapiro(sample)\nAssumptions: Observations are i.i.d.\nReturns: The test statistic and corresponding p-value.\n\nMore appropriate for smaller sample sizes (\\(&lt;50\\)).\nThe closer the test statistic is to \\(1\\), the closer it is to a normal distribution, with \\(1\\) being a perfect match.\n\n\n\n7.8.2 NormalTest\nH0: data is drawn from a normal distribution\nH1: data is not drawn from a normal distribution\n\n\nCode: scipy.stats.normaltest(sample)\nAssumptions: Observations are i.i.d.\nOptional Parameters: Can use nan_policy.\nReturns: The test-statistic \\(s^2 + k^2\\), where \\(s^2\\) is from the skewtest and \\(k\\) is from the kurtosistest, and a corresponding p-value\nThis test is based on D’Agostino and Pearson’s test which combines skew and kurtosis (heaviness of the tail or how much data resides in the tails). The test compares the skewness and kurtosis of the sample to that of a normal distribution, which are \\(0\\) and \\(3\\), respectively.\n\n\n7.8.3 Example: Distribution of Response Times\nIt can be useful to identify the distribution of a population because it gives us the ability to summarize the data more efficiently. We can identify whether or not the distribution of a sample of response times\nfrom the rodent sighting dataset is normal by conducting a normality test using scipy.stats.\n\n# take a sample from Response Time column \nresp_time_samp = df['Response Time'].sample(10000, random_state = 0)  \n\nresults_norm = scipy.stats.normaltest(resp_time_samp, nan_policy = 'propagate')\n\nprint('test statistic:', results_norm.statistic) \nprint('p-value:', results_norm.pvalue)\n\ntest statistic: nan\np-value: nan\n\n\nBecause there are null values in the sample data, if we set the nan_policy to propagate, both the test statistic and p-value will return as nan. If we still want to obtain results when there is missing data, we must set the nan_policy to omit.\n\nresults_norm2 = scipy.stats.normaltest(resp_time_samp, nan_policy = 'omit') \n\nprint('test statistic:', results_norm2.statistic) \nprint('p-value:', results_norm2.pvalue)\n\ntest statistic: 12530.153548983568\np-value: 0.0\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that the data is not drawn from a normal distribution. We can further show this by plotting the data in a histogram.\n\nbins = [i for i in range(int(resp_time_samp.min()), int(resp_time_samp.max()), 300)]\n\nplt.hist(resp_time_samp, bins = bins)\nplt.xlabel('Response Times')\nplt.ylabel('Value Counts')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#correlation",
    "href": "eda.html#correlation",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.9 Correlation",
    "text": "7.9 Correlation\n\n7.9.1 Pearson’s Correlation\nH0: the correlations is \\(0\\)\nH1: the correlations is \\(\\neq\\), \\(&lt;\\), or \\(&gt; 0\\)\n\n\nCode: scipy.stats.pearsonr(sample_1, sample_2)\nAssumptions: Observations are i.i.d, normally distributed, and the two samples have equal variances.\nOptional Parameters: Can use alternative.\nErrors:\n\nRaises ConstantInputWarning if either input has all constant values.\nRaises NearConstantInputWarning if np.linalg.norm(x - mean(x)) &lt; 1e-13 * np.abs(mean(x)).\n\nReturns: The correlation coefficient and a corresponding p-value. It also has the confidence_interval method.\n\n\n7.9.2 Chi-Squared Test\nH0: the two variables are independent of one another\nH1: a dependency exists between the two variables\n\n\nCode: scipy.stats.chi2_contingency(table)\nAssumptions: The cells in the table contain frequencies, the levels of each variable are mutually exclusive, and observations are independent. [2]\nReturns: The test statistic, a corresponding p-value, the degrees of freedom, and an array of expected frequencies from the table.\n\ndof = table.size - sum(table.shape) + table.ndim - 1\n\n\n\n7.9.3 Example: Analyzing the Relationship Between Season and Response Time\nOne way in which the chi-squared test may prove useful with the 2022-2023 311 Service Request rodent sighting data is by allowing us to identify dependencies between different variables, categorical ones in particular. For example, we can choose a borough and test whether the season in which the request was created is independent of the type of sighting, using the Descriptor column.\n\n# return season based off month of created date\ndef get_season(month): \n  if month in [3, 4, 5]:\n    return 'Spring'\n  elif month in [6, 7, 8]: \n    return 'Summer'\n  elif month in [9, 10, 11]: \n    return 'Fall'\n  elif month in [12, 1, 2]:\n    return 'Winter' \n\n# add column for season \ndf['Season'] = df['Created Date'].dt.month.apply(get_season)\n\n# create df for Brooklyn \ndf_brklyn = df[df['Borough'] == 'BROOKLYN'] \n\nfreq_table_2 = pd.crosstab(df_brklyn.Season, df_brklyn.Descriptor) \n\nfreq_table_2\n\n\n\n\n\n\n\n\nDescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\n\n\nSeason\n\n\n\n\n\n\n\n\n\nFall\n1282\n464\n4653\n0\n710\n\n\nSpring\n1395\n579\n5396\n4\n850\n\n\nSummer\n1732\n509\n6631\n5\n881\n\n\nWinter\n923\n523\n3643\n3\n613\n\n\n\n\n\n\n\n\n\nresults_chi2 = scipy.stats.chi2_contingency(freq_table_2)\n\nprint('test statistic:', results_chi2.statistic)\nprint('p-value:', results_chi2.pvalue)\nprint('degrees of freedom:', results_chi2.dof) \n\ntest statistic: 120.07130925971065\np-value: 5.981181861531126e-20\ndegrees of freedom: 12\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that the Season and Descriptor columns are indeed dependent for Brooklyn. This can also be confirmed by plotting the descriptor frequencies in a stacked bar chart, where the four seasons represent different colored bars.\n\nx_labels = ['Condition', 'M.Sighting', 'R.Sighting', 'Bite', 'Signs of Rodents']\n\nfreq_table_2.rename(columns = {'Condition Attracting Rodents': 'Condition', \n                      'Rat Sighting': 'R.Sighting', 'Mouse Sighting': 'M.Sighting', \n                      'Rodent Bite - PCS Only': 'Bite'},\n                      inplace = True)\n\nfreq_table_2.T.plot(kind = 'bar', stacked = True) \n\n\n\n\n\n\n\n\nThe bar chart above shows that the ranking of each season by number of rodent sightings is consistent across all five types of rodent sightings. This further suggests that there exists dependency between season and rodent sighting in Brooklyn.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#nonparametric-hypothesis-tests",
    "href": "eda.html#nonparametric-hypothesis-tests",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.10 Nonparametric Hypothesis Tests",
    "text": "7.10 Nonparametric Hypothesis Tests\n\n7.10.1 Mann-Whitney U (Wilcoxon Rank-Sum) Test\nH0: distribution of population 1 \\(=\\) distribution of population 2\nH1: distribution of population 1 \\(\\neq\\) or \\(&gt;\\) or \\(&lt;\\) distribution of population 2\n\n\nCode: scipy.stats.mannwhitneyu(sample_1, sample_2)\nAssumptions: Observations are independent and ordinal.\nOptional Parameters:\n\n\nalternative can allow us to test if one sample has a distribution that is stochastically less than or greater than that of the second sample.\n\nCan use nan_policy.\nmethod selects how the p-value is calculated and can be set to asymptotic, exact, or auto.\n\nasymptotic corrects for ties and compares the standardized test statistic to the normal distribution.\nexact does not correct for ties and computes the exact p-value.\nauto (default) chooses exact when there are no ties and the size of one sample is \\(&lt;=8\\), asymptotic otherwise.\n\n\n\nReturns: The Mann-Whitney U Statistic corresponding with the first sample and a corresponding p-value.\n\n\nThe statistic corresponding to the second sample is not returned but can be calculated as sample_1.shape * sample_2.shape - U1 where U1 is the test statistic associated with sample_1.\nFor large sample sizes, the distribution can be assumed to be approximately normal, so the statisic can be measured as \\(z = \\frac{U-\\mu_{U}}{\\sigma_{U}}\\).\nTo adjust for ties, the standard deviation is calculated as follows:\n\n\\(\\sigma_{U} = \\sqrt{\\frac{n_{1}n_{2}}{12}((n + 1) -\n\\frac{\\sum_{k = 1}^{K}(t^{3}_{k} - t_{k})}{n(n - 1)})}\\), where \\(t_{k}\\) is the number of ties.\n\nNon-parametric version of the two-sample t-test.\nIf the underlying distributions have similar shapes, the test is essentially a comparison of medians. [5]\n\n\n\n\n7.10.2 Wilcoxon Signed-Rank test\nH0: distribution of population 1 \\(=\\) distribution of population 2\nH1: distribution of population 1 \\(\\neq\\) or \\(&gt;\\) or \\(&lt;\\) distribution of population 2\n\n\nCode: scipy.stats.wilcoxon(sample_1, sample_2) or\nscipy.statss.wilcoxon(sample_diff, None)\nAssumptions: Observations are independent, ordinal, and the samples are paired.\nOptional Parameters:\n\nzero-method chooses how to handle pairs with the same value.\n\nwilcox (default) doesn’t include these pairs.\npratt drops ranks of pairs whose difference is \\(0\\).\nzsplit includes pairs and assigns half the ranks into the positive group and the other half in the negative group.\n\nCan use alternative and nan_policy.\nalternative allows us to identify whether the distribution of the difference is stochastically greater than or less than a distribution symmetric about \\(0\\).\nmethod selects how the p-value is calculated.\n\nexact computes the exact p-value.\napprox finds the p-value by standardizing the test statistic.\nauto (default) chooses exact when the sizes of the samples are \\(&lt;=50\\) and approx otherwise.\n\n\nReturns: The test statistic, a corresponding p-value, and the calculated z-score when the method is approx.\n\nNon-parametric version of the paired t-test.\n\n\n\n7.10.3 Kruskal-Wallis H-Test\nH0: all populations have the same distribution\nH1: \\(&gt;=2\\) populations are distributed differently\n\n\nCode: scipy.stats.kruskal(sample_1, ..., sample_n)\nAssumptions: Observations are independent, ordinal, and each sample has \\(&gt;=5\\) observations. [3]\nOptional Parameters: Can use nan_policy.\nReturns: The Kruskal-Wallis H-statisic (corrected for ties) and a corresponding p-value.\n\nNon-parametric version of ANOVA.\n\n\n\n7.10.4 Example: Distribution of Response Times for 2022 vs. 2023\nWe can use the Mann-Whitney test to compare the distributions of response times from our rodent data. For example, we can split the data into two groups, one for 2022 and the other for 2023, to compare their distributions.\n\n# create dfs for 2022 and 2023\ndf_2022 = df[df['Created Date'].dt.year == 2022]['Response Time'] \ndf_2023 = df[df['Created Date'].dt.year == 2023]['Response Time']\n\n# perform test with H_0 df_2022 &gt; df_2023\nresults_mw = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n                                                      alternative = 'greater')\n\n# perform test with H_0 df_2022 &lt; df_2023\nresults_mw2 = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n                                                        alternative = 'less')\n\nprint('test statistic:', results_mw.statistic)\nprint('p-value:', results_mw.pvalue)\nprint()\nprint('test statistic:', results_mw2.statistic)\nprint('p-value:', results_mw2.pvalue)\n\ntest statistic: 744300262.0\np-value: 1.0\n\ntest statistic: 744300262.0\np-value: 3.931744717049885e-98\n\n\nAt an alpha level of \\(0.05\\), the p-value of \\(1\\) is too large to reject the null hypothesis, therefore we cannot conclude that the distribution of response times for 2022 is stochastically greater than that for 2023. But when we set the alternative to less, our p-value is small enough to conclude that the distribution of response times for 2022 is stochastically greater than the distribution of response times for 2023.\n\nbins = [i for i in range(5, 500, 50)]\nplt.hist(df_2022, label = 2022, bins = bins, color = 'red') \nplt.hist(df_2023, label = 2023, bins = bins, color = 'blue', \n                                                  alpha = 0.5)\n\nplt.legend()\nplt.show()  \n\n\n\n\n\n\n\n\nThis small subset of data confirms the results of the one-sided hypothesis test, showing that in general, the counts of response times for 2022 are greater than those for 2023, suggesting the distribution for 2022 is stochastically larger than that of 2023 data.\n\n\n7.10.5 Example: Distribution of Response Times by Season\nSimilar to the previous, example, we can use a non-parametric test to compare the distribution of response times by season. Because in this case we have four samples to compare, we need to use the Kruskal Wallis H-Test.\n\ndf_summer = df[df['Season'] == 'Summer']['Response Time']\ndf_spring = df[df['Season'] == 'Spring']['Response Time']\ndf_fall = df[df['Season'] == 'Fall']['Response Time']\ndf_winter = df[df['Season'] == 'Winter']['Response Time']\n\nresults_kw = scipy.stats.kruskal(df_summer, df_spring, df_fall, df_winter,\n                                                                nan_policy = 'omit')\n\nprint('test statistic:', results_kw.statistic) \nprint('p-value:', results_kw.pvalue)\n\ntest statistic: 7.626073129122622\np-value: 0.05440606295505631\n\n\nAt an alpha level of \\(0.05\\), the p-value of \\(0.0496\\) is just small enough to reject the null hypothesis, suggesting that the distribution of response times differs by season, but not by much.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#references",
    "href": "eda.html#references",
    "title": "Hypothesis Testing with scipy.stats",
    "section": "7.11 References",
    "text": "7.11 References\n\nhttps://docs.scipy.org/doc/scipy/reference/stats.html (scipy.stats documentation)\nhttps://libguides.library.kent.edu/SPSS/ChiSquare\nhttps://library.virginia.edu/data/articles/getting-started-with-the-kruskal-wallis-test\nhttps://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/\nhttps://library.virginia.edu/data/articles/the-wilcoxon-rank-sum-test",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "8  Visualization",
    "section": "",
    "text": "9 Data Visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nurl = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/cleaning_data_rodent3.csv'\n\ndf = pd.read_csv(url)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#introduction",
    "href": "visualization.html#introduction",
    "title": "8  Visualization",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nHi Class, my name is Weijia Wu and I’m a senior double majored in Applied Math and Statistics. The following shows a basic concepts of data visulization in python.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#matplotlib",
    "href": "visualization.html#matplotlib",
    "title": "8  Visualization",
    "section": "9.2 Matplotlib",
    "text": "9.2 Matplotlib\nMatplotlib is a desktop plotting package designed for plotting and arranging data visually in Python, usually in two-dimensional. It was created by Dr. John Hunter in 2003 as an alternative to Matlab to facilitate scientific computation and data visualization in Python.\nMatplotlib is widely used because of its simplicity and effectiveness.\n\n9.2.1 Installation of Matplotlib\nThe library can be installed by typing pip install matplotlib in your terminal\npip install matplotlib\n\n\n9.2.2 Line Plot\n\n9.2.2.1 Single plot with pyplot submodule\nLet’s Start with an sample Line Plot example:\n\nt = range(0, 10) \nr = [i**2 for i in t]\n\nplt.figure(figsize=(4, 4)) \n# Width and height in inches\nplt.plot(t, r)\nplt.title('Line Plot Example')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.2.2.2 x-label, y-label, and grid:\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r)\nplt.title('Line Plot Example2')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\n9.2.2.3 Add legend:\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r)\nplt.title('Line Plot Example3')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\nplt.legend()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nTo add a legend to a plot in Matplotlib, you can use the legend() function.\nA legend is a small area on the plot that describes each element of the graph.\nTo effectively use the legend, you typically need to label the elements of the plot that you want to appear in the legend using the label parameter when plotting them.\n\nplt.legend(loc='lower right', title='Legend Title', fontsize='small')\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nThe help(plt.legend) command in Python is used to display the documentation for the legend function from the Matplotlib library. This documentation includes a description of what the function does, the parameters it accepts, and other relevant information such as return values and examples of how to use the function.\n\nhelp(plt.legend)\n\nHelp on function legend in module matplotlib.pyplot:\n\nlegend(*args, **kwargs)\n    Place a legend on the Axes.\n    \n    Call signatures::\n    \n        legend()\n        legend(handles, labels)\n        legend(handles=handles)\n        legend(labels)\n    \n    The call signatures correspond to the following different ways to use\n    this method:\n    \n    **1. Automatic detection of elements to be shown in the legend**\n    \n    The elements to be added to the legend are automatically determined,\n    when you do not pass in any extra arguments.\n    \n    In this case, the labels are taken from the artist. You can specify\n    them either at artist creation or by calling the\n    :meth:`~.Artist.set_label` method on the artist::\n    \n        ax.plot([1, 2, 3], label='Inline label')\n        ax.legend()\n    \n    or::\n    \n        line, = ax.plot([1, 2, 3])\n        line.set_label('Label via method')\n        ax.legend()\n    \n    .. note::\n        Specific artists can be excluded from the automatic legend element\n        selection by using a label starting with an underscore, \"_\".\n        A string starting with an underscore is the default label for all\n        artists, so calling `.Axes.legend` without any arguments and\n        without setting the labels manually will result in no legend being\n        drawn.\n    \n    \n    **2. Explicitly listing the artists and labels in the legend**\n    \n    For full control of which artists have a legend entry, it is possible\n    to pass an iterable of legend artists followed by an iterable of\n    legend labels respectively::\n    \n        ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n    \n    \n    **3. Explicitly listing the artists in the legend**\n    \n    This is similar to 2, but the labels are taken from the artists'\n    label properties. Example::\n    \n        line1, = ax.plot([1, 2, 3], label='label1')\n        line2, = ax.plot([1, 2, 3], label='label2')\n        ax.legend(handles=[line1, line2])\n    \n    \n    **4. Labeling existing plot elements**\n    \n    .. admonition:: Discouraged\n    \n        This call signature is discouraged, because the relation between\n        plot elements and labels is only implicit by their order and can\n        easily be mixed up.\n    \n    To make a legend for all artists on an Axes, call this function with\n    an iterable of strings, one for each legend item. For example::\n    \n        ax.plot([1, 2, 3])\n        ax.plot([5, 6, 7])\n        ax.legend(['First line', 'Second line'])\n    \n    \n    Parameters\n    ----------\n    handles : sequence of `.Artist`, optional\n        A list of Artists (lines, patches) to be added to the legend.\n        Use this together with *labels*, if you need full control on what\n        is shown in the legend and the automatic mechanism described above\n        is not sufficient.\n    \n        The length of handles and labels should be the same in this\n        case. If they are not, they are truncated to the smaller length.\n    \n    labels : list of str, optional\n        A list of labels to show next to the artists.\n        Use this together with *handles*, if you need full control on what\n        is shown in the legend and the automatic mechanism described above\n        is not sufficient.\n    \n    Returns\n    -------\n    `~matplotlib.legend.Legend`\n    \n    Other Parameters\n    ----------------\n    \n    loc : str or pair of floats, default: :rc:`legend.loc` ('best' for axes, 'upper right' for figures)\n        The location of the legend.\n    \n        The strings\n        ``'upper left', 'upper right', 'lower left', 'lower right'``\n        place the legend at the corresponding corner of the axes/figure.\n    \n        The strings\n        ``'upper center', 'lower center', 'center left', 'center right'``\n        place the legend at the center of the corresponding edge of the\n        axes/figure.\n    \n        The string ``'center'`` places the legend at the center of the axes/figure.\n    \n        The string ``'best'`` places the legend at the location, among the nine\n        locations defined so far, with the minimum overlap with other drawn\n        artists.  This option can be quite slow for plots with large amounts of\n        data; your plotting speed may benefit from providing a specific location.\n    \n        The location can also be a 2-tuple giving the coordinates of the lower-left\n        corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n        will be ignored).\n    \n        For back-compatibility, ``'center right'`` (but no other location) can also\n        be spelled ``'right'``, and each \"string\" locations can also be given as a\n        numeric value:\n    \n            ===============   =============\n            Location String   Location Code\n            ===============   =============\n            'best'            0\n            'upper right'     1\n            'upper left'      2\n            'lower left'      3\n            'lower right'     4\n            'right'           5\n            'center left'     6\n            'center right'    7\n            'lower center'    8\n            'upper center'    9\n            'center'          10\n            ===============   =============\n    \n    bbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n        Box that is used to position the legend in conjunction with *loc*.\n        Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n        `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n        placement of the legend.\n    \n        Bbox coordinates are interpreted in the coordinate system given by\n        *bbox_transform*, with the default transform\n        Axes or Figure coordinates, depending on which ``legend`` is called.\n    \n        If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n        ``(x, y, width, height)`` that the legend is placed in.\n        To put the legend in the best location in the bottom right\n        quadrant of the axes (or figure)::\n    \n            loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n    \n        A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n        x, y.  For example, to put the legend's upper right-hand corner in the\n        center of the axes (or figure) the following keywords can be used::\n    \n            loc='upper right', bbox_to_anchor=(0.5, 0.5)\n    \n    ncols : int, default: 1\n        The number of columns that the legend has.\n    \n        For backward compatibility, the spelling *ncol* is also supported\n        but it is discouraged. If both are given, *ncols* takes precedence.\n    \n    prop : None or `matplotlib.font_manager.FontProperties` or dict\n        The font properties of the legend. If None (default), the current\n        :data:`matplotlib.rcParams` will be used.\n    \n    fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n        The font size of the legend. If the value is numeric the size will be the\n        absolute font size in points. String values are relative to the current\n        default font size. This argument is only used if *prop* is not specified.\n    \n    labelcolor : str or list, default: :rc:`legend.labelcolor`\n        The color of the text in the legend. Either a valid color string\n        (for example, 'red'), or a list of color strings. The labelcolor can\n        also be made to match the color of the line or marker using 'linecolor',\n        'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n    \n        Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n        use :rc:`text.color`.\n    \n    numpoints : int, default: :rc:`legend.numpoints`\n        The number of marker points in the legend when creating a legend\n        entry for a `.Line2D` (line).\n    \n    scatterpoints : int, default: :rc:`legend.scatterpoints`\n        The number of marker points in the legend when creating\n        a legend entry for a `.PathCollection` (scatter plot).\n    \n    scatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n        The vertical offset (relative to the font size) for the markers\n        created for a scatter plot legend entry. 0.0 is at the base the\n        legend text, and 1.0 is at the top. To draw all markers at the\n        same height, set to ``[0.5]``.\n    \n    markerscale : float, default: :rc:`legend.markerscale`\n        The relative size of legend markers compared with the originally\n        drawn ones.\n    \n    markerfirst : bool, default: True\n        If *True*, legend marker is placed to the left of the legend label.\n        If *False*, legend marker is placed to the right of the legend label.\n    \n    frameon : bool, default: :rc:`legend.frameon`\n        Whether the legend should be drawn on a patch (frame).\n    \n    fancybox : bool, default: :rc:`legend.fancybox`\n        Whether round edges should be enabled around the `.FancyBboxPatch` which\n        makes up the legend's background.\n    \n    shadow : bool, default: :rc:`legend.shadow`\n        Whether to draw a shadow behind the legend.\n    \n    framealpha : float, default: :rc:`legend.framealpha`\n        The alpha transparency of the legend's background.\n        If *shadow* is activated and *framealpha* is ``None``, the default value is\n        ignored.\n    \n    facecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n        The legend's background color.\n        If ``\"inherit\"``, use :rc:`axes.facecolor`.\n    \n    edgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n        The legend's background patch edge color.\n        If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n    \n    mode : {\"expand\", None}\n        If *mode* is set to ``\"expand\"`` the legend will be horizontally\n        expanded to fill the axes area (or *bbox_to_anchor* if defines\n        the legend's size).\n    \n    bbox_transform : None or `matplotlib.transforms.Transform`\n        The transform for the bounding box (*bbox_to_anchor*). For a value\n        of ``None`` (default) the Axes'\n        :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n    \n    title : str or None\n        The legend's title. Default is no title (``None``).\n    \n    title_fontproperties : None or `matplotlib.font_manager.FontProperties` or dict\n        The font properties of the legend's title. If None (default), the\n        *title_fontsize* argument will be used if present; if *title_fontsize* is\n        also None, the current :rc:`legend.title_fontsize` will be used.\n    \n    title_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n        The font size of the legend's title.\n        Note: This cannot be combined with *title_fontproperties*. If you want\n        to set the fontsize alongside other font properties, use the *size*\n        parameter in *title_fontproperties*.\n    \n    alignment : {'center', 'left', 'right'}, default: 'center'\n        The alignment of the legend title and the box of entries. The entries\n        are aligned as a single block, so that markers always lined up.\n    \n    borderpad : float, default: :rc:`legend.borderpad`\n        The fractional whitespace inside the legend border, in font-size units.\n    \n    labelspacing : float, default: :rc:`legend.labelspacing`\n        The vertical space between the legend entries, in font-size units.\n    \n    handlelength : float, default: :rc:`legend.handlelength`\n        The length of the legend handles, in font-size units.\n    \n    handleheight : float, default: :rc:`legend.handleheight`\n        The height of the legend handles, in font-size units.\n    \n    handletextpad : float, default: :rc:`legend.handletextpad`\n        The pad between the legend handle and text, in font-size units.\n    \n    borderaxespad : float, default: :rc:`legend.borderaxespad`\n        The pad between the axes and legend border, in font-size units.\n    \n    columnspacing : float, default: :rc:`legend.columnspacing`\n        The spacing between columns, in font-size units.\n    \n    handler_map : dict or None\n        The custom dictionary mapping instances or types to a legend\n        handler. This *handler_map* updates the default handler map\n        found at `matplotlib.legend.Legend.get_legend_handler_map`.\n    \n    \n    See Also\n    --------\n    .Figure.legend\n    \n    Notes\n    -----\n    Some artists are not supported by this function.  See\n    :doc:`/tutorials/intermediate/legend_guide` for details.\n    \n    Examples\n    --------\n    .. plot:: gallery/text_labels_and_annotations/legend.py\n\n\n\n\n\n9.2.2.4 Colors, Markers, and Line Styles\nIf we want two plots in the same, we need to find a way to make the distinction between them.\n\nr2 = [i**3 for i in t]\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r, linestyle = '--', color = 'r', marker = 'o', label = 'r')\nplt.plot(t, r2, linestyle = '-', color = 'b', marker = 'v', label = 'r2')\n\nplt.title('Line Plot Example2')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nUse linestyle, color, and Markers to set linestyles:\n\nhelp(plt.plot)\n\nHelp on function plot in module matplotlib.pyplot:\n\nplot(*args, scalex=True, scaley=True, data=None, **kwargs)\n    Plot y versus x as lines and/or markers.\n    \n    Call signatures::\n    \n        plot([x], y, [fmt], *, data=None, **kwargs)\n        plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n    \n    The coordinates of the points or line nodes are given by *x*, *y*.\n    \n    The optional parameter *fmt* is a convenient way for defining basic\n    formatting like color, marker and linestyle. It's a shortcut string\n    notation described in the *Notes* section below.\n    \n    &gt;&gt;&gt; plot(x, y)        # plot x and y using default line style and color\n    &gt;&gt;&gt; plot(x, y, 'bo')  # plot x and y using blue circle markers\n    &gt;&gt;&gt; plot(y)           # plot y using x as index array 0..N-1\n    &gt;&gt;&gt; plot(y, 'r+')     # ditto, but with red plusses\n    \n    You can use `.Line2D` properties as keyword arguments for more\n    control on the appearance. Line properties and *fmt* can be mixed.\n    The following two calls yield identical results:\n    \n    &gt;&gt;&gt; plot(x, y, 'go--', linewidth=2, markersize=12)\n    &gt;&gt;&gt; plot(x, y, color='green', marker='o', linestyle='dashed',\n    ...      linewidth=2, markersize=12)\n    \n    When conflicting with *fmt*, keyword arguments take precedence.\n    \n    \n    **Plotting labelled data**\n    \n    There's a convenient way for plotting objects with labelled data (i.e.\n    data that can be accessed by index ``obj['y']``). Instead of giving\n    the data in *x* and *y*, you can provide the object in the *data*\n    parameter and just give the labels for *x* and *y*::\n    \n    &gt;&gt;&gt; plot('xlabel', 'ylabel', data=obj)\n    \n    All indexable objects are supported. This could e.g. be a `dict`, a\n    `pandas.DataFrame` or a structured numpy array.\n    \n    \n    **Plotting multiple sets of data**\n    \n    There are various ways to plot multiple sets of data.\n    \n    - The most straight forward way is just to call `plot` multiple times.\n      Example:\n    \n      &gt;&gt;&gt; plot(x1, y1, 'bo')\n      &gt;&gt;&gt; plot(x2, y2, 'go')\n    \n    - If *x* and/or *y* are 2D arrays a separate data set will be drawn\n      for every column. If both *x* and *y* are 2D, they must have the\n      same shape. If only one of them is 2D with shape (N, m) the other\n      must have length N and will be used for every data set m.\n    \n      Example:\n    \n      &gt;&gt;&gt; x = [1, 2, 3]\n      &gt;&gt;&gt; y = np.array([[1, 2], [3, 4], [5, 6]])\n      &gt;&gt;&gt; plot(x, y)\n    \n      is equivalent to:\n    \n      &gt;&gt;&gt; for col in range(y.shape[1]):\n      ...     plot(x, y[:, col])\n    \n    - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n      groups::\n    \n      &gt;&gt;&gt; plot(x1, y1, 'g^', x2, y2, 'g-')\n    \n      In this case, any additional keyword argument applies to all\n      datasets. Also this syntax cannot be combined with the *data*\n      parameter.\n    \n    By default, each line is assigned a different style specified by a\n    'style cycle'. The *fmt* and line property parameters are only\n    necessary if you want explicit deviations from these defaults.\n    Alternatively, you can also change the style cycle using\n    :rc:`axes.prop_cycle`.\n    \n    \n    Parameters\n    ----------\n    x, y : array-like or scalar\n        The horizontal / vertical coordinates of the data points.\n        *x* values are optional and default to ``range(len(y))``.\n    \n        Commonly, these parameters are 1D arrays.\n    \n        They can also be scalars, or two-dimensional (in that case, the\n        columns represent separate data sets).\n    \n        These arguments cannot be passed as keywords.\n    \n    fmt : str, optional\n        A format string, e.g. 'ro' for red circles. See the *Notes*\n        section for a full description of the format strings.\n    \n        Format strings are just an abbreviation for quickly setting\n        basic line properties. All of these and more can also be\n        controlled by keyword arguments.\n    \n        This argument cannot be passed as keyword.\n    \n    data : indexable object, optional\n        An object with labelled data. If given, provide the label names to\n        plot in *x* and *y*.\n    \n        .. note::\n            Technically there's a slight ambiguity in calls where the\n            second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n            could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n            the former interpretation is chosen, but a warning is issued.\n            You may suppress the warning by adding an empty format string\n            ``plot('n', 'o', '', data=obj)``.\n    \n    Returns\n    -------\n    list of `.Line2D`\n        A list of lines representing the plotted data.\n    \n    Other Parameters\n    ----------------\n    scalex, scaley : bool, default: True\n        These parameters determine if the view limits are adapted to the\n        data limits. The values are passed on to\n        `~.axes.Axes.autoscale_view`.\n    \n    **kwargs : `.Line2D` properties, optional\n        *kwargs* are used to specify properties like a line label (for\n        auto legends), linewidth, antialiasing, marker face color.\n        Example::\n    \n        &gt;&gt;&gt; plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n        &gt;&gt;&gt; plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n    \n        If you specify multiple lines with one plot call, the kwargs apply\n        to all those lines. In case the label object is iterable, each\n        element is used as labels for each set of data.\n    \n        Here is a list of available `.Line2D` properties:\n    \n        Properties:\n        agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n        alpha: scalar or None\n        animated: bool\n        antialiased or aa: bool\n        clip_box: `.Bbox`\n        clip_on: bool\n        clip_path: Patch or (Path, Transform) or None\n        color or c: color\n        dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n        dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n        dashes: sequence of floats (on/off ink in points) or (None, None)\n        data: (2, N) array or two 1D arrays\n        drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n        figure: `.Figure`\n        fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n        gapcolor: color or None\n        gid: str\n        in_layout: bool\n        label: object\n        linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n        linewidth or lw: float\n        marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n        markeredgecolor or mec: color\n        markeredgewidth or mew: float\n        markerfacecolor or mfc: color\n        markerfacecoloralt or mfcalt: color\n        markersize or ms: float\n        markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n        mouseover: bool\n        path_effects: `.AbstractPathEffect`\n        picker: float or callable[[Artist, Event], tuple[bool, dict]]\n        pickradius: unknown\n        rasterized: bool\n        sketch_params: (scale: float, length: float, randomness: float)\n        snap: bool or None\n        solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n        solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n        transform: unknown\n        url: str\n        visible: bool\n        xdata: 1D array\n        ydata: 1D array\n        zorder: float\n    \n    See Also\n    --------\n    scatter : XY scatter plot with markers of varying size and/or color (\n        sometimes also called bubble chart).\n    \n    Notes\n    -----\n    **Format Strings**\n    \n    A format string consists of a part for color, marker and line::\n    \n        fmt = '[marker][line][color]'\n    \n    Each of them is optional. If not provided, the value from the style\n    cycle is used. Exception: If ``line`` is given, but no ``marker``,\n    the data will be a line without markers.\n    \n    Other combinations such as ``[color][marker][line]`` are also\n    supported, but note that their parsing may be ambiguous.\n    \n    **Markers**\n    \n    =============   ===============================\n    character       description\n    =============   ===============================\n    ``'.'``         point marker\n    ``','``         pixel marker\n    ``'o'``         circle marker\n    ``'v'``         triangle_down marker\n    ``'^'``         triangle_up marker\n    ``'&lt;'``         triangle_left marker\n    ``'&gt;'``         triangle_right marker\n    ``'1'``         tri_down marker\n    ``'2'``         tri_up marker\n    ``'3'``         tri_left marker\n    ``'4'``         tri_right marker\n    ``'8'``         octagon marker\n    ``'s'``         square marker\n    ``'p'``         pentagon marker\n    ``'P'``         plus (filled) marker\n    ``'*'``         star marker\n    ``'h'``         hexagon1 marker\n    ``'H'``         hexagon2 marker\n    ``'+'``         plus marker\n    ``'x'``         x marker\n    ``'X'``         x (filled) marker\n    ``'D'``         diamond marker\n    ``'d'``         thin_diamond marker\n    ``'|'``         vline marker\n    ``'_'``         hline marker\n    =============   ===============================\n    \n    **Line Styles**\n    \n    =============    ===============================\n    character        description\n    =============    ===============================\n    ``'-'``          solid line style\n    ``'--'``         dashed line style\n    ``'-.'``         dash-dot line style\n    ``':'``          dotted line style\n    =============    ===============================\n    \n    Example format strings::\n    \n        'b'    # blue markers with default shape\n        'or'   # red circles\n        '-g'   # green solid line\n        '--'   # dashed line with default color\n        '^k:'  # black triangle_up markers connected by a dotted line\n    \n    **Colors**\n    \n    The supported color abbreviations are the single letter codes\n    \n    =============    ===============================\n    character        color\n    =============    ===============================\n    ``'b'``          blue\n    ``'g'``          green\n    ``'r'``          red\n    ``'c'``          cyan\n    ``'m'``          magenta\n    ``'y'``          yellow\n    ``'k'``          black\n    ``'w'``          white\n    =============    ===============================\n    \n    and the ``'CN'`` colors that index into the default property cycle.\n    \n    If the color is the only part of the format string, you can\n    additionally use any  `matplotlib.colors` spec, e.g. full names\n    (``'green'``) or hex strings (``'#008000'``).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#example-with-rodent-data",
    "href": "visualization.html#example-with-rodent-data",
    "title": "8  Visualization",
    "section": "9.3 Example with rodent data:",
    "text": "9.3 Example with rodent data:\nLet’s use our rodent data to demonstrate the Monthly Reported data:\n\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\n\ndf['Month'] = df['Created Date'].dt.to_period('M')\nmonthly_counts = df.groupby('Month').size()\n\nplt.figure(figsize=(10, 8))\nmonthly_counts.plot(kind='line')\nplt.title('Monthly Report Count')\nplt.xlabel('Month')\nplt.ylabel('Number of Reports')\nplt.grid(True)\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the number of rodents in each month’s report, and we can draw the following conclusions: rodent sights occur mostly in the spring and summer, and they fall dramatically after the start of autumn (post-August).\n\n9.3.1 Scatter plot\n\nnp.random.seed(8465);\n\nx = np.random.uniform(0, 3, 10);\ny = np.random.uniform(0, 3, 10);\nz = np.random.uniform(0, 3, 10);\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3.2 Bar Plot\n\nborough_counts = df['Borough'].value_counts()\n\nplt.figure(figsize=(10, 8))  \nplt.bar(borough_counts.index, borough_counts.values, color='green')\nplt.xlabel('Borough')  \nplt.ylabel('Number of Rodent Sightings')  \nplt.title('Rodent Sightings by Borough') \nplt.xticks(rotation=45)  # Rotate the X axis by 45 degrees to show the long labels\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3.3 Multiple plots using subplots submodule\n\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\ndf['Date'] = df['Created Date'].dt.date\ndaily_reports = df.groupby(['Date', 'Incident Zip']).size().reset_index(name='Counts')\nsample_zip = daily_reports['Incident Zip'].dropna().iloc[0]\nsample_data = daily_reports[daily_reports['Incident Zip'] == sample_zip]\n\n# 2x2 Plot\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n# Line Plot\naxs[0, 0].plot(sample_data['Date'], sample_data['Counts'], '-o', color='green')\naxs[0, 0].set_title(f'Linear Plot of Reports for Zip {sample_zip}')\naxs[0, 0].tick_params(labelrotation=45)\n\n# Box Plot\naxs[0, 1].boxplot(df['Y Coordinate (State Plane)'].dropna())\naxs[0, 1].set_title('Boxplot of Y Coordinate')\n\n# barplot\nstatus_counts = df['Status'].value_counts()\naxs[1, 0].bar(status_counts.index, status_counts.values, color='skyblue')\naxs[1, 0].set_title('Barplot of Status Counts')\naxs[1, 0].tick_params(labelrotation=45)\n\n# histogram\naxs[1, 1].hist(df['Latitude'].dropna(), bins=30, color='orange')\naxs[1, 1].set_title('Histogram of Latitude')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3.4 Save the files\nhelp(plt.savefig)allows you to save the current figure created by Matplotlib to a file. You can specify the filename and various options to control the format, quality, and layout of the output file.\n\nhelp(plt.savefig)\n\nHelp on function savefig in module matplotlib.pyplot:\n\nsavefig(*args, **kwargs)\n    Save the current figure.\n    \n    Call signature::\n    \n      savefig(fname, *, dpi='figure', format=None, metadata=None,\n              bbox_inches=None, pad_inches=0.1,\n              facecolor='auto', edgecolor='auto',\n              backend=None, **kwargs\n             )\n    \n    The available output formats depend on the backend being used.\n    \n    Parameters\n    ----------\n    fname : str or path-like or binary file-like\n        A path, or a Python file-like object, or\n        possibly some backend-dependent object such as\n        `matplotlib.backends.backend_pdf.PdfPages`.\n    \n        If *format* is set, it determines the output format, and the file\n        is saved as *fname*.  Note that *fname* is used verbatim, and there\n        is no attempt to make the extension, if any, of *fname* match\n        *format*, and no extension is appended.\n    \n        If *format* is not set, then the format is inferred from the\n        extension of *fname*, if there is one.  If *format* is not\n        set and *fname* has no extension, then the file is saved with\n        :rc:`savefig.format` and the appropriate extension is appended to\n        *fname*.\n    \n    Other Parameters\n    ----------------\n    dpi : float or 'figure', default: :rc:`savefig.dpi`\n        The resolution in dots per inch.  If 'figure', use the figure's\n        dpi value.\n    \n    format : str\n        The file format, e.g. 'png', 'pdf', 'svg', ... The behavior when\n        this is unset is documented under *fname*.\n    \n    metadata : dict, optional\n        Key/value pairs to store in the image metadata. The supported keys\n        and defaults depend on the image format and backend:\n    \n        - 'png' with Agg backend: See the parameter ``metadata`` of\n          `~.FigureCanvasAgg.print_png`.\n        - 'pdf' with pdf backend: See the parameter ``metadata`` of\n          `~.backend_pdf.PdfPages`.\n        - 'svg' with svg backend: See the parameter ``metadata`` of\n          `~.FigureCanvasSVG.print_svg`.\n        - 'eps' and 'ps' with PS backend: Only 'Creator' is supported.\n    \n    bbox_inches : str or `.Bbox`, default: :rc:`savefig.bbox`\n        Bounding box in inches: only the given portion of the figure is\n        saved.  If 'tight', try to figure out the tight bbox of the figure.\n    \n    pad_inches : float, default: :rc:`savefig.pad_inches`\n        Amount of padding around the figure when bbox_inches is 'tight'.\n    \n    facecolor : color or 'auto', default: :rc:`savefig.facecolor`\n        The facecolor of the figure.  If 'auto', use the current figure\n        facecolor.\n    \n    edgecolor : color or 'auto', default: :rc:`savefig.edgecolor`\n        The edgecolor of the figure.  If 'auto', use the current figure\n        edgecolor.\n    \n    backend : str, optional\n        Use a non-default backend to render the file, e.g. to render a\n        png file with the \"cairo\" backend rather than the default \"agg\",\n        or a pdf file with the \"pgf\" backend rather than the default\n        \"pdf\".  Note that the default backend is normally sufficient.  See\n        :ref:`the-builtin-backends` for a list of valid backends for each\n        file format.  Custom backends can be referenced as \"module://...\".\n    \n    orientation : {'landscape', 'portrait'}\n        Currently only supported by the postscript backend.\n    \n    papertype : str\n        One of 'letter', 'legal', 'executive', 'ledger', 'a0' through\n        'a10', 'b0' through 'b10'. Only supported for postscript\n        output.\n    \n    transparent : bool\n        If *True*, the Axes patches will all be transparent; the\n        Figure patch will also be transparent unless *facecolor*\n        and/or *edgecolor* are specified via kwargs.\n    \n        If *False* has no effect and the color of the Axes and\n        Figure patches are unchanged (unless the Figure patch\n        is specified via the *facecolor* and/or *edgecolor* keyword\n        arguments in which case those colors are used).\n    \n        The transparency of these patches will be restored to their\n        original values upon exit of this function.\n    \n        This is useful, for example, for displaying\n        a plot on top of a colored background on a web page.\n    \n    bbox_extra_artists : list of `~matplotlib.artist.Artist`, optional\n        A list of extra artists that will be considered when the\n        tight bbox is calculated.\n    \n    pil_kwargs : dict, optional\n        Additional keyword arguments that are passed to\n        `PIL.Image.Image.save` when saving the figure.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#pandas",
    "href": "visualization.html#pandas",
    "title": "8  Visualization",
    "section": "9.4 Pandas",
    "text": "9.4 Pandas\nPandas plotting is built on top of Matplotlib, and one of its main benefits is that it allows you to generate plots with fewer lines of code directly from Pandas data structures like DataFrames and Series. This integration simplifies the process of visualizing data for analysis.\n\n9.4.1 Line Plot\n\n9.4.1.1 Single plot\n\nmonthly_counts.plot(kind='line')\n\n\n\n\n\n\n\n\nBecause the line plot is default in pandas plots, you can omit the (kind=‘line’)\nWhen plotting with the .plot() method in Pandas, it is true that you can generate basic plots with fewer lines of code, due to the fact that Pandas automatically handles some of the basic settings, such as setting the x-axis labels automatically. However, for more detailed chart customization, such as setting gridlines, rotating x-axis labels, and so on, you may need additional Matplotlib commands to implement them.\n\nplt.figure(figsize=(10, 8))\nmonthly_counts.plot(kind='line')\n\nplt.title('Monthly Report Count')\nplt.xlabel('Month')\nplt.ylabel('Number of Reports')\nplt.grid(True)\nplt.xticks(rotation=45)\n# For longer tags, avoid overlapping\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.4.1.2 Multi-Lineplot\nThe following is showing several line plots in the same figure.\n\ncommunity_counts = df['Community Districts'].value_counts().sort_index()\ncity_council_counts = df['City Council Districts'].value_counts().sort_index()\npolice_precincts_counts = df['Police Precincts'].value_counts().sort_index()\n\ncounts_df = pd.DataFrame({\n    'Community Districts': community_counts,\n    'City Council Districts': city_council_counts,\n    'Police Precincts': police_precincts_counts\n})\ncounts_df = counts_df.fillna(0) \n#Fill missing values to 0\n\ncounts_df[['Community Districts', 'City Council Districts', 'Police Precincts']].plot() \n\n\n\n\n\n\n\n\nWhen you use the .plot() method on a Pandas DataFrame to create a multi-line plot, each line in the plot is automatically assigned a different color to help distinguish between the different data columns visually. The colors are chosen from a default color cycle provided by Matplotlib.\nIf you want to customize the color:\n\ncounts_df[['Community Districts', 'City Council Districts', 'Police Precincts']].plot(\n    color=['red', 'green', 'blue']  # Custom colors for each line\n)\n\n\n\n\n\n\n\n\n\n\n\n9.4.2 Additional arguments\nFor more info pleased check:\n\n![additional arguments](https://drive.google.com/file/d/1j5T7_VMT1Nt4myukcmar0UMcZOHqurCk/view?usp=sharing)\n\nzsh:1: bad pattern: [additional\n\n\n\nhelp(plt.plot)\n\nHelp on function plot in module matplotlib.pyplot:\n\nplot(*args, scalex=True, scaley=True, data=None, **kwargs)\n    Plot y versus x as lines and/or markers.\n    \n    Call signatures::\n    \n        plot([x], y, [fmt], *, data=None, **kwargs)\n        plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n    \n    The coordinates of the points or line nodes are given by *x*, *y*.\n    \n    The optional parameter *fmt* is a convenient way for defining basic\n    formatting like color, marker and linestyle. It's a shortcut string\n    notation described in the *Notes* section below.\n    \n    &gt;&gt;&gt; plot(x, y)        # plot x and y using default line style and color\n    &gt;&gt;&gt; plot(x, y, 'bo')  # plot x and y using blue circle markers\n    &gt;&gt;&gt; plot(y)           # plot y using x as index array 0..N-1\n    &gt;&gt;&gt; plot(y, 'r+')     # ditto, but with red plusses\n    \n    You can use `.Line2D` properties as keyword arguments for more\n    control on the appearance. Line properties and *fmt* can be mixed.\n    The following two calls yield identical results:\n    \n    &gt;&gt;&gt; plot(x, y, 'go--', linewidth=2, markersize=12)\n    &gt;&gt;&gt; plot(x, y, color='green', marker='o', linestyle='dashed',\n    ...      linewidth=2, markersize=12)\n    \n    When conflicting with *fmt*, keyword arguments take precedence.\n    \n    \n    **Plotting labelled data**\n    \n    There's a convenient way for plotting objects with labelled data (i.e.\n    data that can be accessed by index ``obj['y']``). Instead of giving\n    the data in *x* and *y*, you can provide the object in the *data*\n    parameter and just give the labels for *x* and *y*::\n    \n    &gt;&gt;&gt; plot('xlabel', 'ylabel', data=obj)\n    \n    All indexable objects are supported. This could e.g. be a `dict`, a\n    `pandas.DataFrame` or a structured numpy array.\n    \n    \n    **Plotting multiple sets of data**\n    \n    There are various ways to plot multiple sets of data.\n    \n    - The most straight forward way is just to call `plot` multiple times.\n      Example:\n    \n      &gt;&gt;&gt; plot(x1, y1, 'bo')\n      &gt;&gt;&gt; plot(x2, y2, 'go')\n    \n    - If *x* and/or *y* are 2D arrays a separate data set will be drawn\n      for every column. If both *x* and *y* are 2D, they must have the\n      same shape. If only one of them is 2D with shape (N, m) the other\n      must have length N and will be used for every data set m.\n    \n      Example:\n    \n      &gt;&gt;&gt; x = [1, 2, 3]\n      &gt;&gt;&gt; y = np.array([[1, 2], [3, 4], [5, 6]])\n      &gt;&gt;&gt; plot(x, y)\n    \n      is equivalent to:\n    \n      &gt;&gt;&gt; for col in range(y.shape[1]):\n      ...     plot(x, y[:, col])\n    \n    - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n      groups::\n    \n      &gt;&gt;&gt; plot(x1, y1, 'g^', x2, y2, 'g-')\n    \n      In this case, any additional keyword argument applies to all\n      datasets. Also this syntax cannot be combined with the *data*\n      parameter.\n    \n    By default, each line is assigned a different style specified by a\n    'style cycle'. The *fmt* and line property parameters are only\n    necessary if you want explicit deviations from these defaults.\n    Alternatively, you can also change the style cycle using\n    :rc:`axes.prop_cycle`.\n    \n    \n    Parameters\n    ----------\n    x, y : array-like or scalar\n        The horizontal / vertical coordinates of the data points.\n        *x* values are optional and default to ``range(len(y))``.\n    \n        Commonly, these parameters are 1D arrays.\n    \n        They can also be scalars, or two-dimensional (in that case, the\n        columns represent separate data sets).\n    \n        These arguments cannot be passed as keywords.\n    \n    fmt : str, optional\n        A format string, e.g. 'ro' for red circles. See the *Notes*\n        section for a full description of the format strings.\n    \n        Format strings are just an abbreviation for quickly setting\n        basic line properties. All of these and more can also be\n        controlled by keyword arguments.\n    \n        This argument cannot be passed as keyword.\n    \n    data : indexable object, optional\n        An object with labelled data. If given, provide the label names to\n        plot in *x* and *y*.\n    \n        .. note::\n            Technically there's a slight ambiguity in calls where the\n            second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n            could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n            the former interpretation is chosen, but a warning is issued.\n            You may suppress the warning by adding an empty format string\n            ``plot('n', 'o', '', data=obj)``.\n    \n    Returns\n    -------\n    list of `.Line2D`\n        A list of lines representing the plotted data.\n    \n    Other Parameters\n    ----------------\n    scalex, scaley : bool, default: True\n        These parameters determine if the view limits are adapted to the\n        data limits. The values are passed on to\n        `~.axes.Axes.autoscale_view`.\n    \n    **kwargs : `.Line2D` properties, optional\n        *kwargs* are used to specify properties like a line label (for\n        auto legends), linewidth, antialiasing, marker face color.\n        Example::\n    \n        &gt;&gt;&gt; plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n        &gt;&gt;&gt; plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n    \n        If you specify multiple lines with one plot call, the kwargs apply\n        to all those lines. In case the label object is iterable, each\n        element is used as labels for each set of data.\n    \n        Here is a list of available `.Line2D` properties:\n    \n        Properties:\n        agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n        alpha: scalar or None\n        animated: bool\n        antialiased or aa: bool\n        clip_box: `.Bbox`\n        clip_on: bool\n        clip_path: Patch or (Path, Transform) or None\n        color or c: color\n        dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n        dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n        dashes: sequence of floats (on/off ink in points) or (None, None)\n        data: (2, N) array or two 1D arrays\n        drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n        figure: `.Figure`\n        fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n        gapcolor: color or None\n        gid: str\n        in_layout: bool\n        label: object\n        linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n        linewidth or lw: float\n        marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n        markeredgecolor or mec: color\n        markeredgewidth or mew: float\n        markerfacecolor or mfc: color\n        markerfacecoloralt or mfcalt: color\n        markersize or ms: float\n        markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n        mouseover: bool\n        path_effects: `.AbstractPathEffect`\n        picker: float or callable[[Artist, Event], tuple[bool, dict]]\n        pickradius: unknown\n        rasterized: bool\n        sketch_params: (scale: float, length: float, randomness: float)\n        snap: bool or None\n        solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n        solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n        transform: unknown\n        url: str\n        visible: bool\n        xdata: 1D array\n        ydata: 1D array\n        zorder: float\n    \n    See Also\n    --------\n    scatter : XY scatter plot with markers of varying size and/or color (\n        sometimes also called bubble chart).\n    \n    Notes\n    -----\n    **Format Strings**\n    \n    A format string consists of a part for color, marker and line::\n    \n        fmt = '[marker][line][color]'\n    \n    Each of them is optional. If not provided, the value from the style\n    cycle is used. Exception: If ``line`` is given, but no ``marker``,\n    the data will be a line without markers.\n    \n    Other combinations such as ``[color][marker][line]`` are also\n    supported, but note that their parsing may be ambiguous.\n    \n    **Markers**\n    \n    =============   ===============================\n    character       description\n    =============   ===============================\n    ``'.'``         point marker\n    ``','``         pixel marker\n    ``'o'``         circle marker\n    ``'v'``         triangle_down marker\n    ``'^'``         triangle_up marker\n    ``'&lt;'``         triangle_left marker\n    ``'&gt;'``         triangle_right marker\n    ``'1'``         tri_down marker\n    ``'2'``         tri_up marker\n    ``'3'``         tri_left marker\n    ``'4'``         tri_right marker\n    ``'8'``         octagon marker\n    ``'s'``         square marker\n    ``'p'``         pentagon marker\n    ``'P'``         plus (filled) marker\n    ``'*'``         star marker\n    ``'h'``         hexagon1 marker\n    ``'H'``         hexagon2 marker\n    ``'+'``         plus marker\n    ``'x'``         x marker\n    ``'X'``         x (filled) marker\n    ``'D'``         diamond marker\n    ``'d'``         thin_diamond marker\n    ``'|'``         vline marker\n    ``'_'``         hline marker\n    =============   ===============================\n    \n    **Line Styles**\n    \n    =============    ===============================\n    character        description\n    =============    ===============================\n    ``'-'``          solid line style\n    ``'--'``         dashed line style\n    ``'-.'``         dash-dot line style\n    ``':'``          dotted line style\n    =============    ===============================\n    \n    Example format strings::\n    \n        'b'    # blue markers with default shape\n        'or'   # red circles\n        '-g'   # green solid line\n        '--'   # dashed line with default color\n        '^k:'  # black triangle_up markers connected by a dotted line\n    \n    **Colors**\n    \n    The supported color abbreviations are the single letter codes\n    \n    =============    ===============================\n    character        color\n    =============    ===============================\n    ``'b'``          blue\n    ``'g'``          green\n    ``'r'``          red\n    ``'c'``          cyan\n    ``'m'``          magenta\n    ``'y'``          yellow\n    ``'k'``          black\n    ``'w'``          white\n    =============    ===============================\n    \n    and the ``'CN'`` colors that index into the default property cycle.\n    \n    If the color is the only part of the format string, you can\n    additionally use any  `matplotlib.colors` spec, e.g. full names\n    (``'green'``) or hex strings (``'#008000'``).\n\n\n\n\n\n9.4.3 Bar Plot\nFor categorical data, one of common visualization is the barplot.\n\nGenerated using df.plot.bar() method, for horizontal version df.plot.barh().\n\n\n9.4.3.1 Side-by-side Bar Plot:\nLet’s use Borough and Location Type to generate a side-by-side bar plot, one horizontal and one vertical:\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))\n\n# Vertical bar plot for Borough counts\ndf.groupby(['Borough']).size().plot.bar(ax=axs[0], color='skyblue', rot=0)\naxs[0].set_title('Bar plot for Borough')\n\n# Horizontal bar plot for Location Type counts\ndf.groupby(['Location Type']).size().plot.barh(ax=axs[1], color='lightgreen')\naxs[1].set_title('Bar plot for Location Type')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimiliar with axs in matplotlib:\n\nnrows=1 means there will be 1 row of subplots.\nncols=2means there will be 2 columns of subplots.\n\n\n\n9.4.3.2 Grouped Bar Plot\nThis type of plot is useful for comparing the distribution within each class side by side.\n\nclass_Borough = pd.crosstab(df[\"Borough\"], df[\"Status\"])\n\nclass_Borough.plot.bar(rot=45, figsize=(10, 6))\n\n\n\n\n\n\n\n\n\n\n9.4.3.3 Stacked Bar Plot\nThis plot is useful for comparing the total counts across borough while still being able to see the proportion of each borough within each class.\n\nclass_Borough.plot.bar(stacked=True)\n\n\n\n\n\n\n\n\n\n\n\n9.4.4 Histogram and Density Plots\nFor numeric data, histogram allows us to see the distribution (center shape, skewness) of the data.\nHistogram can be generated using df.plot.hist() method\nSince we have limited numeric data in our rodent data, I used another data to present it:\n\nurl2 = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/grades_example.csv'\ndf2 = pd.read_csv(url2)\n\n\ndf2[\"Grade\"].plot.hist(bins = 10, figsize=(10, 8))\n\n\n\n\n\n\n\n\nAs can be seen from the plot, the students’ scores show a normal distribution, with most of them clustered in the 70-80 range\n\ndf2[\"Grade\"].plot.density()\n\n\n\n\n\n\n\n\n\n\n9.4.5 Scatter Plots\nWhen dealing with two variables, scatter plot allow us to examine if there is any correlation between them.\nScatter can be generated using df.plot.scatter(x = col1, y = col2) method.\n\nurl3 = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/student_example3.csv'\ndf3 = pd.read_csv(url3)\n\n\ndf3.plot.scatter(x=\"Weight\", y=\"Height\", figsize=(10,8))\n\n\n\n\n\n\n\n\nAs you can see it’s roughly a linear regression, and I’ll cover how to add a regression line in the next sns section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#seaborn",
    "href": "visualization.html#seaborn",
    "title": "8  Visualization",
    "section": "9.5 Seaborn",
    "text": "9.5 Seaborn\n\nSeaborn is designed to work directly with pandas DataFrames, making plotting more convenient by allowing direct use of DataFrame columns for specifying data in plots.\nSeaborn makes it easy to add linear regression lines and other statistical models to your charts, simplifying the process of statistical data visualization.\nSeaborn’s default styles and color are more aesthetically pleasing and modern compared to Matplotlib.\n\n\n9.5.1 Installation of Seaborn\npip install seaborn\n\n\n9.5.2 Histogram and Density Plots\n\nhelp(sns.histplot) \n\nHelp on function histplot in module seaborn.distributions:\n\nhistplot(data=None, *, x=None, y=None, hue=None, weights=None, stat='count', bins='auto', binwidth=None, binrange=None, discrete=None, cumulative=False, common_bins=True, common_norm=True, multiple='layer', element='bars', fill=True, shrink=1, kde=False, kde_kws=None, line_kws=None, thresh=0, pthresh=None, pmax=None, cbar=False, cbar_ax=None, cbar_kws=None, palette=None, hue_order=None, hue_norm=None, color=None, log_scale=None, legend=True, ax=None, **kwargs)\n    Plot univariate or bivariate histograms to show distributions of datasets.\n    \n    A histogram is a classic visualization tool that represents the distribution\n    of one or more variables by counting the number of observations that fall within\n    discrete bins.\n    \n    This function can normalize the statistic computed within each bin to estimate\n    frequency, density or probability mass, and it can add a smooth curve obtained\n    using a kernel density estimate, similar to :func:`kdeplot`.\n    \n    More information is provided in the :ref:`user guide &lt;tutorial_hist&gt;`.\n    \n    Parameters\n    ----------\n    data : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n        Input data structure. Either a long-form collection of vectors that can be\n        assigned to named variables or a wide-form dataset that will be internally\n        reshaped.\n    x, y : vectors or keys in ``data``\n        Variables that specify positions on the x and y axes.\n    hue : vector or key in ``data``\n        Semantic variable that is mapped to determine the color of plot elements.\n    weights : vector or key in ``data``\n        If provided, weight the contribution of the corresponding data points\n        towards the count in each bin by these factors.\n    stat : str\n        Aggregate statistic to compute in each bin.\n        \n        - `count`: show the number of observations in each bin\n        - `frequency`: show the number of observations divided by the bin width\n        - `probability` or `proportion`: normalize such that bar heights sum to 1\n        - `percent`: normalize such that bar heights sum to 100\n        - `density`: normalize such that the total area of the histogram equals 1\n    bins : str, number, vector, or a pair of such values\n        Generic bin parameter that can be the name of a reference rule,\n        the number of bins, or the breaks of the bins.\n        Passed to :func:`numpy.histogram_bin_edges`.\n    binwidth : number or pair of numbers\n        Width of each bin, overrides ``bins`` but can be used with\n        ``binrange``.\n    binrange : pair of numbers or a pair of pairs\n        Lowest and highest value for bin edges; can be used either\n        with ``bins`` or ``binwidth``. Defaults to data extremes.\n    discrete : bool\n        If True, default to ``binwidth=1`` and draw the bars so that they are\n        centered on their corresponding data points. This avoids \"gaps\" that may\n        otherwise appear when using discrete (integer) data.\n    cumulative : bool\n        If True, plot the cumulative counts as bins increase.\n    common_bins : bool\n        If True, use the same bins when semantic variables produce multiple\n        plots. If using a reference rule to determine the bins, it will be computed\n        with the full dataset.\n    common_norm : bool\n        If True and using a normalized statistic, the normalization will apply over\n        the full dataset. Otherwise, normalize each histogram independently.\n    multiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n        Approach to resolving multiple elements when semantic mapping creates subsets.\n        Only relevant with univariate data.\n    element : {\"bars\", \"step\", \"poly\"}\n        Visual representation of the histogram statistic.\n        Only relevant with univariate data.\n    fill : bool\n        If True, fill in the space under the histogram.\n        Only relevant with univariate data.\n    shrink : number\n        Scale the width of each bar relative to the binwidth by this factor.\n        Only relevant with univariate data.\n    kde : bool\n        If True, compute a kernel density estimate to smooth the distribution\n        and show on the plot as (one or more) line(s).\n        Only relevant with univariate data.\n    kde_kws : dict\n        Parameters that control the KDE computation, as in :func:`kdeplot`.\n    line_kws : dict\n        Parameters that control the KDE visualization, passed to\n        :meth:`matplotlib.axes.Axes.plot`.\n    thresh : number or None\n        Cells with a statistic less than or equal to this value will be transparent.\n        Only relevant with bivariate data.\n    pthresh : number or None\n        Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n        (or other statistics, when used) up to this proportion of the total will be\n        transparent.\n    pmax : number or None\n        A value in [0, 1] that sets that saturation point for the colormap at a value\n        such that cells below constitute this proportion of the total count (or\n        other statistic, when used).\n    cbar : bool\n        If True, add a colorbar to annotate the color mapping in a bivariate plot.\n        Note: Does not currently support plots with a ``hue`` variable well.\n    cbar_ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the colorbar.\n    cbar_kws : dict\n        Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\n    palette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n        Method for choosing the colors to use when mapping the ``hue`` semantic.\n        String values are passed to :func:`color_palette`. List or dict values\n        imply categorical mapping, while a colormap object implies numeric mapping.\n    hue_order : vector of strings\n        Specify the order of processing and plotting for categorical levels of the\n        ``hue`` semantic.\n    hue_norm : tuple or :class:`matplotlib.colors.Normalize`\n        Either a pair of values that set the normalization range in data units\n        or an object that will map from data units into a [0, 1] interval. Usage\n        implies numeric mapping.\n    color : :mod:`matplotlib color &lt;matplotlib.colors&gt;`\n        Single color specification for when hue mapping is not used. Otherwise, the\n        plot will try to hook into the matplotlib property cycle.\n    log_scale : bool or number, or pair of bools or numbers\n        Set axis scale(s) to log. A single value sets the data axis for any numeric\n        axes in the plot. A pair of values sets each axis independently.\n        Numeric values are interpreted as the desired base (default 10).\n        When `None` or `False`, seaborn defers to the existing Axes scale.\n    legend : bool\n        If False, suppress the legend for semantic variables.\n    ax : :class:`matplotlib.axes.Axes`\n        Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n        internally.\n    kwargs\n        Other keyword arguments are passed to one of the following matplotlib\n        functions:\n    \n        - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n        - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n        - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n        - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n    \n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n        The matplotlib axes containing the plot.\n    \n    See Also\n    --------\n    displot : Figure-level interface to distribution plot functions.\n    kdeplot : Plot univariate or bivariate distributions using kernel density estimation.\n    rugplot : Plot a tick at each observation value along the x and/or y axes.\n    ecdfplot : Plot empirical cumulative distribution functions.\n    jointplot : Draw a bivariate plot with univariate marginal distributions.\n    \n    Notes\n    -----\n    \n    The choice of bins for computing and plotting a histogram can exert\n    substantial influence on the insights that one is able to draw from the\n    visualization. If the bins are too large, they may erase important features.\n    On the other hand, bins that are too small may be dominated by random\n    variability, obscuring the shape of the true underlying distribution. The\n    default bin size is determined using a reference rule that depends on the\n    sample size and variance. This works well in many cases, (i.e., with\n    \"well-behaved\" data) but it fails in others. It is always a good to try\n    different bin sizes to be sure that you are not missing something important.\n    This function allows you to specify bins in several different ways, such as\n    by setting the total number of bins to use, the width of each bin, or the\n    specific locations where the bins should break.\n    \n    Examples\n    --------\n    \n    .. include:: ../docstrings/histplot.rst\n\n\n\n\nplt.figure(figsize=(10,8))\nsns.histplot(df2['Grade'], bins=10, kde = True)\n\n\n\n\n\n\n\n\nbins: The number of bars in the histogram. More bins can make the data distribution more detailed, but too many may cause the chart to be difficult to understand; fewer bins may not be able to show the data distribution accurately. kde: (Kernel Density Estimate Line) a density curve will be added to the histogram, which is generated by kernel density estimation and can help understand the shape of the data distribution\n\n\n9.5.3 Scatter plot with Regression line\nI used an example with less data to be able to show it. We can see that the height and weight of the students are directly proportional.\n\ndf4 = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Fiona', 'George', 'Hannah', 'Ian', 'Julia'],\n    'Height': [160, 172, 158, 165, 170, 162, 175, 168, 180, 155],\n    'Weight': [55, 72, 60, 68, 62, 56, 80, 65, 75, 50]})\n\nplt.figure(figsize = (10, 8))\nsns.regplot(x='Weight', y='Height', data=df4)\n\n\n\n\n\n\n\n\n\n\n9.5.4 Categorical Data\n\n9.5.4.1 barplot\n\nnp.random.seed(0) \ngenders = np.random.choice(['Male', 'Female'], size=500)\nclasses = np.random.choice(['A', 'B', 'C', 'D'], size=500)\ngrades = np.random.choice(['Excellent', 'Good', 'Average', 'Poor'], size=500)\ndf4 = pd.DataFrame({'Gender': genders, 'Class': classes, 'Grades': grades})\n\n\nsns.catplot(x='Class', hue='Gender', col='Grades', kind='count', data=df4, height=5, col_wrap=2)\nplt.show()\n\n\n\n\n\n\n\n\n\nx='Class': This sets the x-axis to represent different classes, so each class will have its own set of bars in the plot.\nhue='Gender': This parameter adds a color coding (hue) based on the ‘Gender’ column\ncol='Grades': This creates separate subplots (columns) for each unique value in the ‘Grades’ column (e.g., Excellent, Good, Average, Poor), effectively grouping the data by grades.\ncol_wrap=2: Limits the number of these subplots to 2 per row. If there are more than 2 unique grades, additional rows will be created to accommodate all the subplots.\nkind='count': Specifies the kind of plot to draw. In this case, 'count'means it will count the occurrences of each category combination and display this as bars in the plot.\nheight=5: Sets the height of each subplot to 5 inches.\n\n\n\n9.5.4.2 Box Plot\n\n# This chunk does not work\nsns.catplot(x='Gender', y='Grades', col='Class', data=df4, kind='box', height = 10, col_wrap=2)\n\n\n\n\n\n\n\n\n\nx='Gender': x-axis variable\ny='Grades': y-axis variable, which in this case is ‘Grades’. Since ‘Grades’ is a categorical variable with values like ‘Excellent’, ‘Good’, ‘Average’, ‘Poor’\ncol='Class': Creates separate subplots for each unique value in the ‘Class’ column, effectively grouping the data by class.\n\n\n\n9.5.4.3 Categorical Data Help\n\nhelp(sns.catplot)\n\nHelp on function catplot in module seaborn.categorical:\n\ncatplot(data=None, *, x=None, y=None, hue=None, row=None, col=None, kind='strip', estimator='mean', errorbar=('ci', 95), n_boot=1000, seed=None, units=None, weights=None, order=None, hue_order=None, row_order=None, col_order=None, col_wrap=None, height=5, aspect=1, log_scale=None, native_scale=False, formatter=None, orient=None, color=None, palette=None, hue_norm=None, legend='auto', legend_out=True, sharex=True, sharey=True, margin_titles=False, facet_kws=None, ci=&lt;deprecated&gt;, **kwargs)\n    Figure-level interface for drawing categorical plots onto a FacetGrid.\n    \n    This function provides access to several axes-level functions that\n    show the relationship between a numerical and one or more categorical\n    variables using one of several visual representations. The `kind`\n    parameter selects the underlying axes-level function to use.\n    \n    Categorical scatterplots:\n    \n    - :func:`stripplot` (with `kind=\"strip\"`; the default)\n    - :func:`swarmplot` (with `kind=\"swarm\"`)\n    \n    Categorical distribution plots:\n    \n    - :func:`boxplot` (with `kind=\"box\"`)\n    - :func:`violinplot` (with `kind=\"violin\"`)\n    - :func:`boxenplot` (with `kind=\"boxen\"`)\n    \n    Categorical estimate plots:\n    \n    - :func:`pointplot` (with `kind=\"point\"`)\n    - :func:`barplot` (with `kind=\"bar\"`)\n    - :func:`countplot` (with `kind=\"count\"`)\n    \n    Extra keyword arguments are passed to the underlying function, so you\n    should refer to the documentation for each to see kind-specific options.\n    \n    See the :ref:`tutorial &lt;categorical_tutorial&gt;` for more information.\n    \n    .. note::\n        By default, this function treats one of the variables as categorical\n        and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n        As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n    \n    \n    After plotting, the :class:`FacetGrid` with the plot is returned and can\n    be used directly to tweak supporting plot details or add other layers.\n    \n    Parameters\n    ----------\n    data : DataFrame, Series, dict, array, or list of arrays\n        Dataset for plotting. If `x` and `y` are absent, this is\n        interpreted as wide-form. Otherwise it is expected to be long-form.    \n    x, y, hue : names of variables in `data` or vector data\n        Inputs for plotting long-form data. See examples for interpretation.    \n    row, col : names of variables in `data` or vector data\n        Categorical variables that will determine the faceting of the grid.\n    kind : str\n        The kind of plot to draw, corresponds to the name of a categorical\n        axes-level plotting function. Options are: \"strip\", \"swarm\", \"box\", \"violin\",\n        \"boxen\", \"point\", \"bar\", or \"count\".\n    estimator : string or callable that maps vector -&gt; scalar\n        Statistical function to estimate within each categorical bin.\n    errorbar : string, (string, number) tuple, callable or None\n        Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n        with a method name and a level parameter, or a function that maps from a\n        vector to a (min, max) interval, or None to hide errorbar. See the\n        :doc:`errorbar tutorial &lt;/tutorial/error_bars&gt;` for more information.\n    \n        .. versionadded:: v0.12.0\n    n_boot : int\n        Number of bootstrap samples used to compute confidence intervals.\n    seed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n        Seed or random number generator for reproducible bootstrapping.\n    units : name of variable in `data` or vector data\n        Identifier of sampling units; used by the errorbar function to\n        perform a multilevel bootstrap and account for repeated measures\n    weights : name of variable in `data` or vector data\n        Data values or column used to compute weighted statistics.\n        Note that the use of weights may limit other statistical options.\n    \n        .. versionadded:: v0.13.1    \n    order, hue_order : lists of strings\n        Order to plot the categorical levels in; otherwise the levels are\n        inferred from the data objects.    \n    row_order, col_order : lists of strings\n        Order to organize the rows and/or columns of the grid in; otherwise the\n        orders are inferred from the data objects.\n    col_wrap : int\n        \"Wrap\" the column variable at this width, so that the column facets\n        span multiple rows. Incompatible with a ``row`` facet.    \n    height : scalar\n        Height (in inches) of each facet. See also: ``aspect``.    \n    aspect : scalar\n        Aspect ratio of each facet, so that ``aspect * height`` gives the width\n        of each facet in inches.    \n    native_scale : bool\n        When True, numeric or datetime values on the categorical axis will maintain\n        their original scaling rather than being converted to fixed indices.\n    \n        .. versionadded:: v0.13.0    \n    formatter : callable\n        Function for converting categorical data into strings. Affects both grouping\n        and tick labels.\n    \n        .. versionadded:: v0.13.0    \n    orient : \"v\" | \"h\" | \"x\" | \"y\"\n        Orientation of the plot (vertical or horizontal). This is usually\n        inferred based on the type of the input variables, but it can be used\n        to resolve ambiguity when both `x` and `y` are numeric or when\n        plotting wide-form data.\n    \n        .. versionchanged:: v0.13.0\n            Added 'x'/'y' as options, equivalent to 'v'/'h'.    \n    color : matplotlib color\n        Single color for the elements in the plot.    \n    palette : palette name, list, or dict\n        Colors to use for the different levels of the ``hue`` variable. Should\n        be something that can be interpreted by :func:`color_palette`, or a\n        dictionary mapping hue levels to matplotlib colors.    \n    hue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n        Normalization in data units for colormap applied to the `hue`\n        variable when it is numeric. Not relevant if `hue` is categorical.\n    \n        .. versionadded:: v0.12.0    \n    legend : \"auto\", \"brief\", \"full\", or False\n        How to draw the legend. If \"brief\", numeric `hue` and `size`\n        variables will be represented with a sample of evenly spaced values.\n        If \"full\", every group will get an entry in the legend. If \"auto\",\n        choose between brief or full representation based on number of levels.\n        If `False`, no legend data is added and no legend is drawn.\n    \n        .. versionadded:: v0.13.0    \n    legend_out : bool\n        If ``True``, the figure size will be extended, and the legend will be\n        drawn outside the plot on the center right.    \n    share{x,y} : bool, 'col', or 'row' optional\n        If true, the facets will share y axes across columns and/or x axes\n        across rows.    \n    margin_titles : bool\n        If ``True``, the titles for the row variable are drawn to the right of\n        the last column. This option is experimental and may not work in all\n        cases.    \n    facet_kws : dict\n        Dictionary of other keyword arguments to pass to :class:`FacetGrid`.\n    kwargs : key, value pairings\n        Other keyword arguments are passed through to the underlying plotting\n        function.\n    \n    Returns\n    -------\n    :class:`FacetGrid`\n        Returns the :class:`FacetGrid` object with the plot on it for further\n        tweaking.\n    \n    Examples\n    --------\n    .. include:: ../docstrings/catplot.rst",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#conclusion",
    "href": "visualization.html#conclusion",
    "title": "8  Visualization",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nMatplotlib is the foundation for making plots in Python. pandas uses Matplotlib for its plotting features but is mainly for handling data. Seaborn makes Matplotlib prettier and easier to use, especially with pandas data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#citation",
    "href": "visualization.html#citation",
    "title": "8  Visualization",
    "section": "9.7 Citation",
    "text": "9.7 Citation\n\nhttps://matplotlib.org/stable/users/project/history.html\nhttps://matplotlib.org/stable/gallery/lines_bars_and_markers/simple_plot.html\nhttps://www.simplilearn.com/tutorials/python-tutorial/matplotlib\nhttps://www.w3schools.com/python/pandas/pandas_plotting.asp\nhttps://github.com/mwaskom/seaborn/tree/master/seaborn\nhttps://seaborn.pydata.org/installing.html\nhttps://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#grammar-of-graphics-with-plotnine",
    "href": "visualization.html#grammar-of-graphics-with-plotnine",
    "title": "8  Visualization",
    "section": "9.8 Grammar of Graphics with Plotnine",
    "text": "9.8 Grammar of Graphics with Plotnine\nThis section was written by Olivia Massad.\n\n9.8.1 Introduction\nHello everyone! My name is Olivia Massad and I am a junior Statistical Data Science Major. I am very interested in sports statistics and analytics, especially involving football, and am very excited to learn more about coding and data science in this class. Today I will be talking about grammar of graphics for python, using Plotnine. This is a new topic for me so I am very excited to show you all what we can do with it.\n\n\n9.8.2 What is Grammar of Graphics?\nSimilarly to how languages have grammar in order to structure language and create a standard for how sentences and words should be arranged, grammar of graphics provides the framework for a consistent way to structure and create statistical visualizations. This framework helps us to create graphs and visualizations which can be widely understood due to the consistent structure. The major components of grammar of graphics are:\n\nData: our datasets and the what components you want to visualize.\nAesthetics: axes, position of data points, color, shape, size.\nScale: scale values or use specific scales depending on multiple values and ranges.\nGeometric objects: how data points are depicted, whether they’re points, lines, bars, etc.\nStatistics: statistical measures of the data included in the graphic, including mean, spread, confidence intervals, etc.\nFacets: subplots for specific data dimensions.\nCoordinate system: cartesian or polar.\n\n\n\n9.8.3 What can you do with Plotnine?\nPlotnine is a program which implements grammar of graphics in order to create data visualizations and graphs using python. It is based on ggplot2 and allows for many variations within graphs. Some examples of things we can create with plotnine are:\n\nBar Charts\nHistograms\nBox Plots\nScatter Plots\nLine Charts\nTime Series\nDensity Plots\netc.\n\n\n\n9.8.4 Using Plotnine\nIn order to use plotnine we first need to install the package using our command line.\nWith conda: “conda install -c conda-forge plotnine”\nWith pip: “pip install plotnine pip install plotnine[all]”\nNow that plotnine is installed, we must call the it in python.\n\nfrom plotnine import *\nfrom plotnine.data import *\n\nNow that plotnine is installed and imported, we can begin to make graphs and plots. Below are different examples of visualizations we can make using plotnine and the personalizations we can add to them. For these graphics I used the rodent sighting data from the NYC open data 311 requests. We also will need pandas and numpy for some of these graphs so we need to import those as well. Additionally, because the data set is so large, we will only be lookng at the first 500 complaints.\n\nfrom plotnine import *\nfrom plotnine.data import *\nimport pandas as pd \nimport numpy as np \nimport os\nfolder = 'data'\nfile = 'rodent_2022-2023.feather'\npath = os.path.join(folder, file)\ndata = pd.read_feather(path)\ndata_used = data.head(500)\n\n\n9.8.4.1 Bar Chart\nOne common type of visualization we can create with plotnine is a bar chart. For this graph we will look at the data for the descriptors of each complaint.\n\n(ggplot(data_used, aes(x = 'descriptor')) \n    + geom_bar())\n\n\n\n\n\n\n\n\nWhile this code provides us with a nice simple chart, because we are using plotnine, we can make some major improvements to the visualization to make it easier to read and more appealing. Some simple things we can do are:\n\nAdd a title.\nColor code the bars.\nChange the orientation of the graph.\nAdd titles to the axes.\n\n\n(ggplot(data_used, aes(x = 'descriptor', fill = 'descriptor')) \n        # Color code the bars.\n    + geom_bar() # Bar Chart\n    + ggtitle('Descriptor Counts') # Add a title.\n    + coord_flip() # Change the orientation of the graph.\n    + xlab(\"Descriptor\") # Add title to x axis.\n    + ylab(\"Number of Complaints\") # Add titles to y axis.\n)\n\n\n\n\n\n\n\n\nSome more complex changes we can make to our graph are:\n\nChange the orientation of the words on the axes to make them easier to read.\nAdd color coded descriptors to each bar.\n\n\n(ggplot(data_used, aes(x = 'descriptor', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_bar() # Bar Chart\n    + ggtitle('Descriptor Counts') # Add a title.\n    + xlab(\"Descriptor\") # Add title to x axis.\n    + ylab(\"Number of Complaints\") # Add titles to y axis.\n    + theme(axis_text_x=element_text(angle=45))\n     # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n9.8.4.2 Scatter Plot\nAnother common visualization we can create is a scatterplot. When looking at the data from the 311 requests, we can see that there are many data points for locations of these complaints. A scatter plot would be a great way to see the location of the complaints by graphing the longitudes and latitudes. In order to better see the points, for this graph we will only use the first 200 complaints.\n\ndata_scatter = data.tail(200)\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude')) \n    + geom_point())\n\n\n\n\n\n\n\n\nSimilarly to the original code for the bar chart, this code provides a very simple scatter plot. Plotnine allows us to add many specializations to the scatterplot in order to differentiate the points from each other. We can:\n\nAdd color to the points.\nDifferentiate using point size.\nDifferentiate using point shape.\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n       color = 'location_type')) # Add color to the points.\n    + geom_point())\n\n\n\n\n\n\n\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n    size = 'descriptor', # Differentiate using point size.\n    shape = 'borough')) # Differentiate using point shape.\n    + geom_point())\n\n/usr/local/lib/python3.11/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nWe can see that due to the close data points, filtering the data using size and shape can become a little congested. One thing we can do to fix this while still viewing the same data is through the use of “facet_grid”.\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n    shape = 'borough')) # Differentiate using point shape.\n    + geom_point()\n    + facet_grid('descriptor ~ .') # Create multiple plots.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude'))\n    + geom_point()\n    + facet_grid('descriptor ~ borough') \n        # Create multiple plots with 2 conditions.\n    + theme(strip_text_y = element_text(angle = 0), # change facet text angle\n        axis_text_x=element_text(angle=45)) # change x axis text angle\n)\n\n\n\n\n\n\n\n\n\n\n9.8.4.3 Histogram\nThe last common graph we will cover using plotnine is a histogram. Here we will use the created date data as a continuous variable. Using plotnine we are able to make many of the same personalizations we were able to do with bar charts.\n\ndata_used['created_date']=pd.to_datetime(\n  data_used['created_date'],\n  format = \"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n(ggplot(data_used, aes(x='created_date'))\n    + geom_histogram())\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_57881/966048317.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/usr/local/lib/python3.11/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 10'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\n\n\nNow that we have a simple histogram with our data we can add specializations, inclduing:\n\nChange width of bins.\nChange oreintation of graph.\nAdd color coded descriptors.\nChange outline color.\nChange the orientation of the words on the axes to make them easier to read.\n\n\n(ggplot(data_used, aes(x='created_date', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_histogram(binwidth=1,  # Change width of bins\n      color = 'black') # Change outline color.\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_used, aes(x='created_date', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_histogram(binwidth=1,  # Change width of bins\n      colour = 'black') # Change outline color.\n    + coord_flip() # Change oreintation of graph.\n)\n\n\n\n\n\n\n\n\nWhile we’re able to color code the histogram to show other descriptors of the data, another way we can do this with plotnine is through the use of multiple graphs. Using “facet_wrap” we can create a multi facet graph with the same data.\n\n(ggplot(data_used, aes(x='created_date')) \n    + geom_histogram(binwidth=1) # Change width of bins\n    + facet_wrap('borough') # Create multiple graphs.\n    + theme(axis_text_x=element_text(angle=45)) \n    # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n9.8.4.4 Density Plot\nThe last visualization we’re going to look at is density plots. While less common than the graphs previously discussed, density plots show the distribution of a specific variable.\n\n(ggplot(data_used, aes(x='created_date'))\n    + geom_density())\n\n\n\n\n\n\n\n\nAbove we can see a very simple density graph with very little description. Using plotnine we are able to:\n\nAdd color coded descriptors.\nScale groups by relative size.\nChange the orientation of the words on the axes to make them easier to read.\n\n\n(ggplot(data_used, aes(x='created_date', color = 'descriptor')) \n        #Add color coded descriptors.\n    + geom_density()\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_used, aes(x='created_date', color = 'descriptor')) \n        #Add color coded descriptors.\n    + geom_density(aes(y=after_stat('count'))) \n        # Scale groups by relative size.\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n\n9.8.5 Resources\n\nhttps://plotnine.readthedocs.io/en/v0.12.4/gallery.html\n\n\n\n9.8.6 References\n\n“Plotnine.Geoms.Geom_bar¶.” Plotnine.Geoms.Geom_bar - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/stable/generated/ plotnine.geoms.geom_bar.html. Accessed 13 Feb. 2024.\n“Plotnine.Geoms.Geom_density¶.” Plotnine.Geoms.Geom_density - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_density.html. Accessed 17 Feb. 2024.\n“Plotnine.Geoms.Geom_histogram¶.” Plotnine.Geoms.Geom_histogram - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_histogram.html#plotnine. geoms.geom_histogram. Accessed 17 Feb. 2024.\n“Plotnine.Geoms.Geom_point¶.” Plotnine.Geoms.Geom_point - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_point.html. Accessed 16 Feb. 2024.\n“Plotnine.” PyPI, pypi.org/project/plotnine/. Accessed 13 Feb. 2024.\nSarkar, Dipanjan (DJ). “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-Dimensional…” Medium, Towards Data Science, 13 Sept. 2018, towardsdatascience.com/a-comprehensive-guide-to-the- grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "statsmod.html",
    "href": "statsmod.html",
    "title": "9  Statistical Models",
    "section": "",
    "text": "9.1 Introduction\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statsmod.html#introduction",
    "href": "statsmod.html#introduction",
    "title": "9  Statistical Models",
    "section": "",
    "text": "9.1.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n9.1.2 Features of statsmodels\nPackage statsmodels offers a comprehensive range of statistical models and tests, making it a powerful tool for a wide array of data analysis tasks:\n\nLinear Regression Models: Essential for predicting quantitative responses, these models form the backbone of many statistical analysis operations.\nGeneralized Linear Models (GLM): Expanding upon linear models, GLMs allow for response variables that have error distribution models other than a normal distribution, catering to a broader set of data characteristics.\nTime Series Analysis: This includes models like ARIMA for analyzing and forecasting time-series data, as well as more complex state space models and seasonal decompositions.\nNonparametric Methods: For data that does not fit a standard distribution, statsmodels provides tools like kernel density estimation and smoothing techniques.\nStatistical Tests: A suite of hypothesis testing tools allows users to rigorously evaluate their models and assumptions, including diagnostics for model evaluation.\n\nIntegrating statsmodels into your data science workflow enriches your analytical capabilities, allowing for both exploratory data analysis and complex statistical modeling.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statsmod.html#generalized-linear-models",
    "href": "statsmod.html#generalized-linear-models",
    "title": "9  Statistical Models",
    "section": "9.2 Generalized Linear Models",
    "text": "9.2 Generalized Linear Models\nGeneralized Linear Models (GLM) extend the classical linear regression to accommodate response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGeneralized Linear Models (GLM) adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function (\\(g(\\mu) = \\mu\\)) is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity (\\(g(\\mu) = \\mu\\))\nDistribution: Normal\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit (\\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\))\nDistribution: Binomial\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log (\\(g(\\mu) = \\log(\\mu)\\))\nDistribution: Poisson\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse (\\(g(\\mu) = \\frac{1}{\\mu}\\))\nDistribution: Gamma\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution.\n\n9.2.1 Examples",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1 Introduction\nSupervised and unsupervised learning represent two core approaches in the field of machine learning, each with distinct methodologies, applications, and goals. Understanding the differences and applicabilities of these learning paradigms is fundamental for anyone venturing into data science and machine learning.\nThe key difference between supervised and unsupervised learning lies in the presence or absence of labeled output data. Supervised learning depends on known outputs to train the model, making it suitable for predictive tasks where the relationship between the input and output is clear. Unsupervised learning, however, thrives on discovering the intrinsic structure of data, making it ideal for exploratory analysis and understanding complex data dynamics without predefined labels.\nBoth supervised and unsupervised learning have their place in the machine learning ecosystem, often complementing each other in comprehensive data analysis and modeling projects. While supervised learning allows for precise predictions and classifications, unsupervised learning offers deep insights and uncovers underlying patterns that might not be immediately apparent.\nThe main tasks in supervised learning can broadly be categorized into two types: classification and regression. Each task utilizes algorithms to interpret the input data and make predictions or decisions based on that data.\nBoth classification and regression are foundational to supervised learning, addressing different types of predictive modeling problems. Classification is used when the output is a category, while regression is used when the output is a numeric value. The choice between classification and regression depends on the nature of the target variable you are trying to predict. Supervised learning algorithms learn from labeled data, refining their models to minimize error and improve prediction accuracy on new, unseen data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#introduction",
    "href": "supervised.html#introduction",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "Supervised Learning Supervised learning is characterized by its use of labeled datasets to train algorithms. In this paradigm, the model is trained on a pre-defined set of training examples, which include an input and the corresponding output. The goal of supervised learning is to learn a mapping from inputs to outputs, enabling the model to make predictions or decisions based on new, unseen data. This approach is widely used in applications such as spam detection, image recognition, and predicting consumer behavior. Supervised learning is further divided into two main categories: regression, where the output is continuous, and classification, where the output is categorical.\nUnsupervised Learning In contrast, unsupervised learning involves working with datasets without labeled responses. The aim here is to uncover hidden patterns, correlations, or structures from input data without the guidance of an explicit output variable. Unsupervised learning algorithms are adept at clustering, dimensionality reduction, and association tasks. They are invaluable in exploratory data analysis, customer segmentation, and anomaly detection, where the structure of the data is unknown, and the goal is to derive insights directly from the data itself.\n\n\n\n\n\nClassification Classification tasks involve categorizing data into predefined classes or groups. In these tasks, the output variable is categorical, such as “spam” or “not spam” in email filtering, or “malignant” or “benign” in tumor diagnosis. The aim is to accurately assign new, unseen instances to one of the categories based on the learning from the training dataset. Classification can be binary, involving two classes, or multiclass, involving more than two classes. Common algorithms used for classification include Logistic Regression, Decision Trees, Support Vector Machines, and Neural Networks.\nRegression Regression tasks predict a continuous quantity. Unlike classification, where the outcomes are discrete labels, regression models predict a numeric value. Examples of regression tasks include predicting the price of a house based on its features, forecasting stock prices, or estimating the age of a person from a photograph. The goal is to find the relationship or correlation between the input features and the continuous output variable. Linear regression is the most basic form of regression, but there are more complex models like Polynomial Regression, Ridge Regression, Lasso Regression, and Regression Trees.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees",
    "href": "supervised.html#decision-trees",
    "title": "10  Supervised Learning",
    "section": "10.2 Decision Trees",
    "text": "10.2 Decision Trees",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "11  Exercises",
    "section": "",
    "text": "Git basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix retult from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\nThe NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.\n\nNYC Rodents Rats in NYC are widespread, as they are in many densely populated areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. In the 311 Service Request Data, there is one complain type Rodent. Extract all the requests with complain type Rodent, created between January 1, 2022 and December 31, 2023. Save them into a csv file named rodent_2022-2023.csv.\n\nAre there any complains that are not closed yet?\nAre there any complains with a closed data before the created date?\nHow many agencies were this complain type reported to?\nSummarize the missingess for each variable.\nSummarize a frequency table for the descriptor variable, and summarize a cross table by year.\nWhich types of ‘DESCRIPTOR’ do you think should be included if our interest is rodent sighting?\nTake a subset of the data with the descriptors you chose and summarize the response time by borough.\n\nNYC rodent sightings data cleaning The data appears to need some cleaning before any further analysis. Some missing values could be filled based on other columns.\n\nChecking all 47 column names suggests that some columns might be redundant. Identify them and demonstrate the redundancy.\nAre zip code and borough always missing together? If geocodes are available, use reverse geocoding to fill the zip code.\nExport the cleaned data in both csv and feather format. Comment on the file sizes.\n\nSQL Practice on NYC rodent sightings The NYC rodent sightings data that we prepared could be stored more efficiently using a database. Let us start from the csv file you exported from the last problem.\n\nCreate a table called rodent from the csv file.\nThe agency and agency_name columns are redundant in the table. Create a table called agency, which contains only these two columns, one agency a row.\nDrop the agency_name name from the rodent table. Justify why we do not need it here.\nComment on the sizes of the table (or exported csv file) of rodent before and after dropping the agency_name column.\nCome up with a scheme for the two tables that allows even more efficient storage of the agency column in the rodent table. _Hint: use an integer to code the agencies.\n\nLogistic Modeling The response time to 311 service requests is a measure of civic service quality. Let us model the response time to 311 requests with complain type Rodent.\n\nCompute the response time in hours. Note that some response will be missing because of unavailable closed date.\nCompute a binary variable over3d, which is one if the response time is greater than 3 days, and zero otherwise. Note that this variable should have no missing values.\nUse the package uszipcode to obtain the zip code level covaraites such as median house income and median home value. Merge these variables to the rodent data.\nSplit the data at random into training (80%) and testing (20%). Build a logistic model to predict over3d on the training data, and validate the performance on the testing data.\nBuild a lasso logistic model to predict over3d, and justify your choice of the tuning parameter. Validate on the testing data.\n\nMidterm Project: Rodents in NYC Rodents in NYC are widespread, as they are in many densely populated areas. As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. Rat sightings in NYC was analyzed by Dr. Michael Walsh in a 2014 PeerJ article. We investigate this problem from a different angle with the NYC Rodent Inspection data, provided by the Department of Health and Mental Hygiene (DOHMH). Download the 2022-2023 data by filtering the INSPECTION_DATE to between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024 and INSPECTION_TYPE is either Initial or Compliance (which should be about 108 MB). Read the meta data information to understand the data.\n\nData cleaning.\n\nThere are two zipcode columns: ZIP_CODE and Zipcodes. Which one represent the zipcode of the inspection site? Comment on the data dictionary.\nSummarize the missing information. Are their missing values that can be filled using other columns? Fill them if yes.\nAre their redundant information in the data? Try storing the data using arrow and comment on the efficiency gain.\nAre there invalid zipcode or borough? Justify and clean them up if yes.\n\nData exploration.\n\nCreate binary variable passing indicating passing or not for the inspection result. Does passing depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.\nAre the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.\nIf we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passting rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.\nPerform a formal hypothesis test to confirm the observations from your visualization.\n\nData analytics.\n\nAggregate the inspections by zip code to create a dataset with five columns. The first three columns are zipcode; n_initial, the count of the initial inspections in that zipcode; and n_initpass, the number of initial inspections with a passing result in that zipcode. The other two variables are n_compliance and n_comppass, the counterpart for compliance inspections.\nAdd a variable to your dataset, n_sighting, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.\nMerge your dataset with the simple zipcode table in package uszipcode by zipcode to obtain demographic and socioeconomic variables at the zipcode level.\nBuild a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.\n\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]