[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-s24.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2023 and Spring 2022 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nOur mid-term project on rodent sightings in New York City was showcased in a virtual session at the NYC Open Data Week 2024 entitled “Landscape of Rodent Sightings in New York City: A Data Science Showcase” on Wednesday, March 20, 2024. The project description is at Section 11  Exercises.\nThe presenters were\n\nVincent Xie: slides; sources.\nPratham Patel: slides; sources.\nIsabelle Perez: slides; sources.\nJoshua Lee: report; sources.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Add yours through a pull request; note the syntax of nested list in Markdown.\n\nChugh, Charitarth\n\nGet better at analyzing data/features\nLearn about more xgboost & gradient boosted trees.\n\nDennison, Jack\n\nLearn how to use Git and GitHub\nBe able to apply my skills in Python and Git to data analytics tasks\n\nElliott, Matt\n\nFaciliate myself into becoming a Data Scientist\nLearn new skills such as Quarto and GitHub\n\nLee, Joshua\n\nImprove model optimization techniques\nlearn how to conduct better feature engineering\nlearn how to perform better model selection and feature selection\nlearn how to deploy ml models and processes to the cloud\n\nMori, Abigail\n\nBecome proficient using Git\nLearn how to properly communiacte statistical evidence and findings\n\nMassad, Olivia\n\nBe able to use Git effectively\nGain knowledge about Data Science and its importance\n\nNguyen, Leon\n\nBecome proficient in utilizing Git and GitHub workflow processes\nDevelop proficiency in Quarto and Python packages\nCreate a data science project start to finish for portfolio work\n\nPatel, Pratham\n\nBecome more proficient and efficient with GitHub and Python\nGet a deeper understanding and appreciate of the Data Science workflow\nUnderstand collaboration and project creation on GitHub\n\nPerez, Isabelle\n\nBecome comfortable working with git and quarto\nLearn data management strategies and the relevant programming skills\n\nPugh, Alex\n\nIncrease my knowledge of Git and Python\nLearn to efficiently clean a data set\n\nQualls, William\n\nBetter understand the Data Science Pipeline\nGain practical knowledge with tools such as Github that aren’t covered in other classes\n\nSchober, Henry\n\nBe more proficient in Git and Python\nDeepen my understanding of Data Science\n\nTaki, William\n\nGet comfortable with Git and Python\nUse the learnings from this class to help with STAT 33494W\n\nWoo, Madison\n\nBe able to comfortably use Git and Python\nLearn about project managment and data science\n\nXie, Vincent\n\nBecome more proficient with Git.\nLearn how to create a proper data science project.\nBe introduced to core concepts in data science.\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\nYankson, Emmanuel\n\nGet better with python\nGet an A in STAT 3255\n\nZhang, Xingye\n\nGet better with computers.\nGet an A in STAT 3255.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\nrandom.seed(4737 + 8852 + 3196 + 2344 + 47) # jointly set by the class on 01/24/2024\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Elliott,Matt A',\n 'Wu,Weijia',\n 'Lek,Victor Khun',\n 'Taki,William Hiroyasu',\n 'Schober,Henry',\n 'Lee,Joshua Jian',\n 'Patel,Pratham Subhas',\n 'Li,Ge',\n 'Zhang,Xingye',\n 'Dennison,Jack Thomas',\n 'Massad,Olivia Grace',\n 'Perez,Isabelle Daenerys Halpine',\n 'Yankson,Emmanuel Opoku',\n 'Li,David',\n 'Mori,Abigail Kanoelani Shim',\n 'Nguyen,Leon Duc',\n 'Pugh,Alex',\n 'Chugh,Charitarth',\n 'Xie,Vincent',\n 'Vijayaraghavendra,Jyothsna',\n 'Qualls,William Wayne',\n 'Woo,Madison Nicole',\n 'Hook,Braedon',\n 'Chowaniec,Amelia Elizabeth']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nDescriptive statistics\nStatistical hypothesis tests\nStatistical modeling\nData visualization\nAccessing census and ACS data\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/07\nMatt Elliott\nData science communication skills\n\n\n02/12\nDr. Haim Bar\nDatabase management\n\n\n02/19\nWillam Taki\nVisualization with matplotlib\n\n\n02/19\nJoshua Lee\nDescriptive Statistics\n\n\n02/07\nWeijia Wu\nVisualizaiton with matplotlib and seaborn\n\n\n02/21\nPratham Patel\nHandling spatial data with geopandas\n\n\n02/21\nOlivia Massad\nGrammar of Graphics plotnine\n\n\n02/26\nXingye Zhang\nData visualizing NYC rodent dataset\n\n\n02/28\nJack Dennison\nGeographic Data Analysis\n\n\n02/28\nIsabelle Perez\nStatistical hypothesis tests scypy.stats\n\n\n03/04\nEmmanuel Yankson\nRandom Forest\n\n\n03/04\nDavid Li\n\n\n\n03/06\nAbigail Mori\nAccessing census and ACS data\n\n\n03/06\nLeon Nguyen\nStatistical Modeling with statsmodels\n\n\n03/25\nAlex Pugh\nTime Series Analysis\n\n\n03/25\nCharitath Chugh\nPyTorch\n\n\n03/27\n\n\n\n\n03/27\nGe Li\nAnimation\n\n\n04/01\nWilliam Qualls\nWeb Scraping\n\n\n04/01\nVincent Xie\nDatabase Operations with SQL\n\n\n04/03\nBraedon Hook\nLong short-term memory (LSTM) network\n\n\n04/03\nMadison Woo\nCalling C/C++ from Python\n\n\n04/08\n\n\n\n\n04/08\n\n\n\n\n04/10\n\n\n\n\n04/10\n\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/15\nMatt Elliott; Weijia Wu; William Taki; Joshua Lee; Pratham Patel\n\n\n04/17\nOlivia Massad; Ge Li; Xingye Zhang; Isabelle Perez\n\n\n04/22\nEmmanual Yankson; Davi Li; Abigail Mori; Leon Nguyen; Alex Pugh\n\n\n04/24\nJack Dennison; Charitath Chugh; Vincent Xie; Madison Woo; Braedon Hook\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nA template for homework is in this repo: hwtemp.qmd.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-2.00466486,  4.73806675, -0.20867504,  0.0212677 ,  9.57756009,\n       -6.95490806,  5.46651728,  5.85710295,  2.94809379,  0.51486778])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.06042213, 0.0789047 , 0.08563356, 0.08824938, 0.01657948,\n       0.00813823, 0.06851133, 0.06265281, 0.09697297, 0.0930928 ])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n8.23 µs ± 325 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.86 µs ± 83.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n85 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 245, 'switch': 382}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4600671104\n4600671104\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4563370656\n4675527344\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Data Import/Export",
    "section": "",
    "text": "5.1 Using the Pandas Package\nThe pandas library simplifies data manipulation and analysis. It’s especially handy for dealing with CSV files.\nimport pandas as pd\n\n# Define the file name\ncsvnm = \"data/rodent_2022-2023.csv\"\n\n# Specify the strings that indicate missing values\n# Q: How would you know these?\nna_values = [\n    \"\",\n    \"0 Unspecified\",\n    \"N/A\",\n    \"na\",\n    \"na na\",\n    \"Unspecified\",\n    \"UNKNOWN\",\n]\n\ndef custom_date_parser(x):\n    return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n\n# Read the CSV file\ndf = pd.read_csv(\n    csvnm,\n    na_values = na_values,\n    parse_dates = ['Created Date', 'Closed Date'], \n    date_parser = custom_date_parser,\n    dtype = {'Latitude': 'float32', 'Longitude': 'float32'},\n)\n\n# Strip leading and trailing whitespace from the column names\ndf.columns = df.columns.str.strip()\ndf.columns = df.columns.str.replace(' ', '_', regex = False).str.lower()\n\n# Drop the 'Location' since it is redundant\n# df.drop(columns=['Location'], inplace=True)\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_27231/754040398.py:22: FutureWarning:\n\nThe argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\nThe pandas package also provides some utility functions for quick summaries about the data frame.\ndf.shape\ndf.describe()\ndf.isnull().sum()\n\nunique_key                            0\ncreated_date                          0\nclosed_date                        2787\nagency                                0\nagency_name                           0\ncomplaint_type                        0\ndescriptor                            0\nlocation_type                         0\nincident_zip                          0\nincident_address                      0\nstreet_name                           0\ncross_street_1                       90\ncross_street_2                       55\nintersection_street_1               104\nintersection_street_2                69\naddress_type                          0\ncity                               1384\nlandmark                           3820\nfacility_type                     82869\nstatus                                2\ndue_date                          82869\nresolution_description             2787\nresolution_action_updated_date     2787\ncommunity_board                       8\nbbl                                2914\nborough                               8\nx_coordinate_(state_plane)          520\ny_coordinate_(state_plane)          520\nopen_data_channel_type                0\npark_facility_name                82869\npark_borough                          8\nvehicle_type                      82869\ntaxi_company_borough              82869\ntaxi_pick_up_location             82869\nbridge_highway_name               82869\nbridge_highway_direction          82869\nroad_ramp                         82869\nbridge_highway_segment            82869\nlatitude                            520\nlongitude                           520\nlocation                            520\nzip_codes                           622\ncommunity_districts                 526\nborough_boundaries                  526\ncity_council_districts              526\npolice_precincts                    526\npolice_precinct                     526\ndtype: int64\nWhat are the unique values of descriptor?\ndf.descriptor.unique()\n\narray(['Rat Sighting', 'Mouse Sighting', 'Condition Attracting Rodents',\n       'Signs of Rodents', 'Rodent Bite - PCS Only'], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#filling-missing-values",
    "href": "import.html#filling-missing-values",
    "title": "5  Data Import/Export",
    "section": "5.2 Filling Missing Values",
    "text": "5.2 Filling Missing Values\nIf geocodes are available but zip code is missing, we can use reverse geocoding to fill the zip code. This process involves querying a geocoding service with latitude and longitude to get the corresponding address details, including the ZIP code. This can be done with package geopy, which needs to be installed first: pip install geopy.\n\nimport pandas as pd\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut, GeocoderServiceError\n\n# Initialize the geocoder\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n\n# Function for reverse geocoding\ndef reverse_geocode(lat, lon):\n    try:\n        location = geolocator.reverse((lat, lon), exactly_one=True)\n        address = location.raw.get('address', {})\n        zip_code = address.get('postcode')\n        return zip_code\n    except (GeocoderTimedOut, GeocoderServiceError):\n        # Handle errors or timeouts\n        return None\n\n# Apply reverse geocoding to fill missing ZIP codes\nfor index, row in df.iterrows():\n    if pd.isnull(row['incident_zip']) and pd.notnull(row['latitude']) and pd.notnull(row['longitude']):\n        df.at[index, 'incident_zip'] = reverse_geocode(row['latitude'], row['longitude'])\n\n# Note: This can be slow for large datasets due to API rate\n# limits and network latency",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#using-appache-arrow-library",
    "href": "import.html#using-appache-arrow-library",
    "title": "5  Data Import/Export",
    "section": "5.3 Using Appache Arrow Library",
    "text": "5.3 Using Appache Arrow Library\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R.\n\ndf.to_feather('data/rodent_2022-2023.feather')\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/rodent_2022-2023.feather\")\ndff.shape\n\n(82869, 47)\n\n\nBenefits of Using Feather:\n\nEfficiency: Feather is designed to support fast reading and writing of data frames, making it ideal for analytical workflows that need to exchange large datasets between Python and R.\nCompatibility: Maintains data type integrity across Python and R, ensuring that numbers, strings, and dates/times are correctly handled and preserved.\nSimplicity: The API for reading and writing Feather files is straightforward, making it accessible to users with varying levels of programming expertise.\n\nBy using Feather format for data storage, you leverage a modern approach optimized for speed and compatibility, significantly enhancing the performance of data-intensive applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#accessing-the-census-data-with-uszipcode",
    "href": "import.html#accessing-the-census-data-with-uszipcode",
    "title": "5  Data Import/Export",
    "section": "5.4 Accessing the Census Data with uszipcode",
    "text": "5.4 Accessing the Census Data with uszipcode\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the incident_zip column is clean and consistent.\n\nprint(df['incident_zip'].isnull().sum())\n# Standardize to 5-digit codes, if necessary\ndf['incident_zip'] = df['incident_zip'].astype(str).str.zfill(5) \n\n0\n\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment by pip install uszipcode.\nNow let’s work on the rodent sightings data.\nWe will first clean the incident_zip column to ensure it only contains valid ZIP codes. Then, we will use a vectorized approach to fetch the required data for each unique ZIP code and merge this information back into the original DataFrame.\n\n# Remove rows where 'incident_zip' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['incident_zip']).copy()\nvalid_zip_df['incident_zip'] = valid_zip_df['incident_zip'].astype(str).str.zfill(5)\nunique_zips = valid_zip_df['incident_zip'].unique()\n\nSince uszipcode doesn’t inherently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple times.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'incident_zip'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"incident_zip\")\n\nmerged_df.columns\n\n/Users/junyan/work/teaching/ids-s24/ids-s24/.myvenv/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning:\n\nUsing slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n\n\n\nIndex(['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',\n       'complaint_type', 'descriptor', 'location_type', 'incident_zip',\n       'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n       'intersection_street_1', 'intersection_street_2', 'address_type',\n       'city', 'landmark', 'facility_type', 'status', 'due_date',\n       'resolution_description', 'resolution_action_updated_date',\n       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',\n       'y_coordinate_(state_plane)', 'open_data_channel_type',\n       'park_facility_name', 'park_borough', 'vehicle_type',\n       'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name',\n       'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment',\n       'latitude', 'longitude', 'location', 'zip_codes', 'community_districts',\n       'borough_boundaries', 'city_council_districts', 'police_precincts',\n       'police_precinct', 'median_home_value', 'median_household_income'],\n      dtype='object')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#accessing-acs-data",
    "href": "import.html#accessing-acs-data",
    "title": "5  Data Import/Export",
    "section": "5.5 Accessing ACS Data",
    "text": "5.5 Accessing ACS Data\nThis section was written by Abigail Mori.\n\n5.5.1 Introduction\nHi! My name is Abigail Mori and I am a current senior, graduating this May. This past summer I worked with the UConn’s Center for Voting Technology Research. We used ACS data to analyze ease of voting within Connecticut. Though, I only used sql and never had to access this data through python. I am excited to show what I’ve learned over the past couple weeks.\n\n\n5.5.2 Installation\nIn order to properly access ACS data we will need make sure we have installed cenpy and geodatasets. Cenpy is a package that “automatically discovers US Census Bureau API endpoints and exposes them to Python in a consistent fashion.” Geodatasets “contains an API on top of a JSON with metadata of externally hosted datasets containing geospatial information useful for illustrative and educational purposes.” Both of these modules are dependent on pandas.\n\n\n5.5.3 Accessing ACS Data\nThere are many different kinds of census data, one of which is ACS (American Community Survery) data. The ACS data is made up of a “wide range of social, economic, demographic, and housing charateristics.” There are multiple ways to access this data using Python. I will examine one way using cenpy.\nFor much of work I referenced a cenpy API reference page (https://cenpy-devs.github.io/cenpy/api.html)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport cenpy as cen\nimport geodatasets\n\n\n# Set a variable to all tables in the The American Community Survey from\n#  the Census Bureau\nacs = cen.products.ACS()\n# Filter through acs tables based on specific variables\nprint(acs.filter_tables('RACE', by='description'))\n# Once you select your desired table you can add more specific parameters, \n# in this case I chose to do so by state\nconnecticut = cen.products.ACS(2019).from_state('Connecticut',\n variables= 'B02001')\nprint(connecticut.head())\nnevada = cen.products.ACS(2019).from_state('Nevada', \nvariables= 'B02001')\nprint(nevada.head())\n\n                                                  description  \\\ntable_name                                                      \nB02001                                                   RACE   \nB02008      WHITE ALONE OR IN COMBINATION WITH ONE OR MORE...   \nB02009      BLACK OR AFRICAN AMERICAN ALONE OR IN COMBINAT...   \nB02010      AMERICAN INDIAN AND ALASKA NATIVE ALONE OR IN ...   \nB02011      ASIAN ALONE OR IN COMBINATION WITH ONE OR MORE...   \nB02012      NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER ALO...   \nB02013      SOME OTHER RACE ALONE OR IN COMBINATION WITH O...   \nB03002                      HISPANIC OR LATINO ORIGIN BY RACE   \nB25006                                    RACE OF HOUSEHOLDER   \nB98013      TOTAL POPULATION COVERAGE RATE BY WEIGHTING RA...   \nB99021                                     ALLOCATION OF RACE   \nC02003                                          DETAILED RACE   \n\n                                                      columns  \ntable_name                                                     \nB02001      [B02001_001E, B02001_002E, B02001_003E, B02001...  \nB02008                                          [B02008_001E]  \nB02009                                          [B02009_001E]  \nB02010                                          [B02010_001E]  \nB02011                                          [B02011_001E]  \nB02012                                          [B02012_001E]  \nB02013                                          [B02013_001E]  \nB03002      [B03002_001E, B03002_002E, B03002_003E, B03002...  \nB25006      [B25006_001E, B25006_002E, B25006_003E, B25006...  \nB98013      [B98013_001E, B98013_002E, B98013_003E, B98013...  \nB99021                [B99021_001E, B99021_002E, B99021_003E]  \nC02003      [C02003_001E, C02003_002E, C02003_003E, C02003...  \n         GEOID                                           geometry  \\\n0  09001010400  POLYGON ((-8201121.400 5017141.610, -8201100.1...   \n1  09001010600  POLYGON ((-8196750.330 5017084.050, -8196748.9...   \n2  09001010300  POLYGON ((-8200317.120 5018093.350, -8200295.0...   \n3  09001011300  POLYGON ((-8199823.530 5012413.050, -8199813.0...   \n4  09001020500  POLYGON ((-8193009.330 5026964.430, -8192869.1...   \n\n   B02001_001E  B02001_002E  B02001_003E  B02001_004E  B02001_005E  \\\n0       5799.0       4698.0        531.0         17.0        346.0   \n1       2062.0       1518.0        282.0          0.0        145.0   \n2       3780.0       3305.0        123.0          0.0        247.0   \n3       3416.0       2530.0         65.0          8.0        154.0   \n4       4886.0       4236.0        232.0        117.0        114.0   \n\n   B02001_006E  B02001_007E  B02001_008E  B02001_009E  B02001_010E  \\\n0          0.0        143.0         64.0          0.0         64.0   \n1         11.0         33.0         73.0          7.0         66.0   \n2          0.0         10.0         95.0         16.0         79.0   \n3         20.0        408.0        231.0         67.0        164.0   \n4         15.0          0.0        172.0          0.0        172.0   \n\n                                              NAME state county   tract  \n0  Census Tract 104, Fairfield County, Connecticut    09    001  010400  \n1  Census Tract 106, Fairfield County, Connecticut    09    001  010600  \n2  Census Tract 103, Fairfield County, Connecticut    09    001  010300  \n3  Census Tract 113, Fairfield County, Connecticut    09    001  011300  \n4  Census Tract 205, Fairfield County, Connecticut    09    001  020500  \n         GEOID                                           geometry  \\\n0  32031002613  POLYGON ((-13358452.220 4827161.350, -13358451...   \n1  32031003308  POLYGON ((-13359059.470 4760834.720, -13357433...   \n2  32031980200  POLYGON ((-13358521.900 4802740.580, -13358516...   \n3  32031002407  POLYGON ((-13346382.740 4800296.200, -13346335...   \n4  32031001105  POLYGON ((-13343276.140 4791951.280, -13343275...   \n\n   B02001_001E  B02001_002E  B02001_003E  B02001_004E  B02001_005E  \\\n0       5830.0       5387.0        140.0         35.0         74.0   \n1       2155.0       1976.0          0.0         35.0         74.0   \n2          0.0          0.0          0.0          0.0          0.0   \n3       4151.0       3025.0         87.0        114.0        671.0   \n4       3819.0       3434.0         67.0          0.0        233.0   \n\n   B02001_006E  B02001_007E  B02001_008E  B02001_009E  B02001_010E  \\\n0         25.0         61.0        108.0          0.0        108.0   \n1          9.0          4.0         57.0         12.0         45.0   \n2          0.0          0.0          0.0          0.0          0.0   \n3         29.0         77.0        148.0         55.0         93.0   \n4         14.0         16.0         55.0          0.0         55.0   \n\n                                        NAME state county   tract  \n0  Census Tract 26.13, Washoe County, Nevada    32    031  002613  \n1  Census Tract 33.08, Washoe County, Nevada    32    031  003308  \n2   Census Tract 9802, Washoe County, Nevada    32    031  980200  \n3  Census Tract 24.07, Washoe County, Nevada    32    031  002407  \n4  Census Tract 11.05, Washoe County, Nevada    32    031  001105  \n\n\nEach columns meanings can be found here: https://api.census.gov/data/2019/acs/acs1/variables.html and https://www.census.gov/programs-surveys/acs/technical-documentation /table-shells.2019.html#list-tab-79594641\n\n\n\n\n\n\n\nB02001_001E:\nTotal\n\n\nB02001_002E\nWhite alone\n\n\nB02001_003E\nBlack or African American alone\n\n\nB02001_004E\nAmerican Indian and Alaska Native alone\n\n\nB02001_005E\nAsian alone\n\n\nB02001_006E\nNative Hawaiian and Other Pacific Islander alone\n\n\nB02001_007E\nSome other race alone\n\n\nB02001_008E\nTwo or more races\n\n\nB02001_009E\nTwo races including Some other race\n\n\nB02001_010E\nTwo races excluding Some other race, and three or more races\n\n\n\n\n\n5.5.4 Visualizing\nCenpy returns users with datasets based on specifications, like RACE. Some of the columns include: geiod, geometry, name, state, county, and tract. The geometry is made up of Polygons which then can be used to make a map with matplotlib Since, cenpy provides a table of type: geopandas.geodataframe.GeoDataFrame. GeoPandas provides a high-level interface to the matplotlib library for making maps.\nBelow, I show maps of Connecticut and Nevada which display the populations of Native Hawaiians and other Pacific Islanders by census tract.\n\nimport matplotlib.pyplot as plt\nimport geopandas\nimport pandas as pd\n\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, \nfigsize=(20, 16))\n\n#B02001_006E: total number of Native Hawaiians and other Pacific Islanders\nax1 = connecticut.plot('B02001_006E', ax = ax1, cmap = 'viridis', legend = True)\nax2 = nevada.plot('B02001_006E', ax = ax2, cmap = 'viridis', legend = True)\n\n\n\n\n\n\n\n\n\n\n5.5.5 Further Analysis\nACS data can allow individuals to see discrepancies in voting accessability. For instance, we can look at correlation between a poverty level and education in a specific state.\nWe can access this data doing the same search method as above.\nLike before we filter through the tables to access our desired data. In this instance I chose to select data concerning poverty and education in Connecticut.\n\nprint(acs.filter_tables('POVERTY', by='description'))\n# B17001: Poverty status in the past 12 months \npoverty_ct = cen.products.ACS(2019).from_state('Connecticut', \nvariables = 'B17001')\npoverty_ct.head()\n\n                                                  description  \\\ntable_name                                                      \nB05010      RATIO OF INCOME TO POVERTY LEVEL IN THE PAST 1...   \nB06012      PLACE OF BIRTH BY POVERTY STATUS IN THE PAST 1...   \nB07012      GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY POVE...   \nB07412      GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY POVE...   \nB08122      MEANS OF TRANSPORTATION TO WORK BY POVERTY STA...   \nB08522      MEANS OF TRANSPORTATION TO WORK BY POVERTY STA...   \nB10059      POVERTY STATUS IN THE PAST 12 MONTHS OF GRANDP...   \nB13010      WOMEN 15 TO 50 YEARS WHO HAD A BIRTH IN THE PA...   \nB14006      POVERTY STATUS IN THE PAST 12 MONTHS BY SCHOOL...   \nB16009      POVERTY STATUS IN THE PAST 12 MONTHS BY AGE BY...   \nB17001      POVERTY STATUS IN THE PAST 12 MONTHS BY SEX BY...   \nB17003      POVERTY STATUS IN THE PAST 12 MONTHS OF INDIVI...   \nB17004      POVERTY STATUS IN THE PAST 12 MONTHS OF INDIVI...   \nB17005      POVERTY STATUS IN THE PAST 12 MONTHS OF INDIVI...   \nB17006      POVERTY STATUS IN THE PAST 12 MONTHS OF RELATE...   \nB17007      POVERTY STATUS IN THE PAST 12 MONTHS OF UNRELA...   \nB17009      POVERTY STATUS BY WORK EXPERIENCE OF UNRELATED...   \nB17010      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17012      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17013      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17014      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17015      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17016      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17017      POVERTY STATUS IN THE PAST 12 MONTHS BY HOUSEH...   \nB17018      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17019      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17020            POVERTY STATUS IN THE PAST 12 MONTHS BY AGE   \nB17021      POVERTY STATUS OF INDIVIDUALS IN THE PAST 12 M...   \nB17022      RATIO OF INCOME TO POVERTY LEVEL IN THE PAST 1...   \nB17023      POVERTY STATUS IN THE PAST 12 MONTHS OF FAMILI...   \nB17024      AGE BY RATIO OF INCOME TO POVERTY LEVEL IN THE...   \nB17025       POVERTY STATUS IN THE PAST 12 MONTHS BY NATIVITY   \nB17026      RATIO OF INCOME TO POVERTY LEVEL OF FAMILIES I...   \nB22003      RECEIPT OF FOOD STAMPS/SNAP IN THE PAST 12 MON...   \nB23024      POVERTY STATUS IN THE PAST 12 MONTHS BY DISABI...   \nB29003       CITIZEN, VOTING-AGE POPULATION BY POVERTY STATUS   \nB99171      ALLOCATION OF POVERTY STATUS IN THE PAST 12 MO...   \nB99172      ALLOCATION OF POVERTY STATUS IN THE PAST 12 MO...   \nC17002      RATIO OF INCOME TO POVERTY LEVEL IN THE PAST 1...   \nC18130             AGE BY DISABILITY STATUS BY POVERTY STATUS   \nC18131      RATIO OF INCOME TO POVERTY LEVEL IN THE PAST 1...   \nC21007      AGE BY VETERAN STATUS BY POVERTY STATUS IN THE...   \nC27016      HEALTH INSURANCE COVERAGE STATUS BY RATIO OF I...   \nC27017      PRIVATE HEALTH INSURANCE BY RATIO OF INCOME TO...   \nC27018      PUBLIC HEALTH INSURANCE BY RATIO OF INCOME TO ...   \n\n                                                      columns  \ntable_name                                                     \nB05010      [B05010_001E, B05010_002E, B05010_003E, B05010...  \nB06012      [B06012_001E, B06012_002E, B06012_003E, B06012...  \nB07012      [B07012_001E, B07012_002E, B07012_003E, B07012...  \nB07412      [B07412_001E, B07412_002E, B07412_003E, B07412...  \nB08122      [B08122_001E, B08122_002E, B08122_003E, B08122...  \nB08522      [B08522_001E, B08522_002E, B08522_003E, B08522...  \nB10059      [B10059_001E, B10059_002E, B10059_003E, B10059...  \nB13010      [B13010_001E, B13010_002E, B13010_003E, B13010...  \nB14006      [B14006_001E, B14006_002E, B14006_003E, B14006...  \nB16009      [B16009_001E, B16009_002E, B16009_003E, B16009...  \nB17001      [B17001_001E, B17001_002E, B17001_003E, B17001...  \nB17003      [B17003_001E, B17003_002E, B17003_003E, B17003...  \nB17004      [B17004_001E, B17004_002E, B17004_003E, B17004...  \nB17005      [B17005_001E, B17005_002E, B17005_003E, B17005...  \nB17006      [B17006_001E, B17006_002E, B17006_003E, B17006...  \nB17007      [B17007_001E, B17007_002E, B17007_003E, B17007...  \nB17009      [B17009_001E, B17009_002E, B17009_003E, B17009...  \nB17010      [B17010_001E, B17010_002E, B17010_003E, B17010...  \nB17012      [B17012_001E, B17012_002E, B17012_003E, B17012...  \nB17013      [B17013_001E, B17013_002E, B17013_003E, B17013...  \nB17014      [B17014_001E, B17014_002E, B17014_003E, B17014...  \nB17015      [B17015_001E, B17015_002E, B17015_003E, B17015...  \nB17016      [B17016_001E, B17016_002E, B17016_003E, B17016...  \nB17017      [B17017_001E, B17017_002E, B17017_003E, B17017...  \nB17018      [B17018_001E, B17018_002E, B17018_003E, B17018...  \nB17019      [B17019_001E, B17019_002E, B17019_003E, B17019...  \nB17020      [B17020_001E, B17020_002E, B17020_003E, B17020...  \nB17021      [B17021_001E, B17021_002E, B17021_003E, B17021...  \nB17022      [B17022_001E, B17022_002E, B17022_003E, B17022...  \nB17023      [B17023_001E, B17023_002E, B17023_003E, B17023...  \nB17024      [B17024_001E, B17024_002E, B17024_003E, B17024...  \nB17025      [B17025_001E, B17025_002E, B17025_003E, B17025...  \nB17026      [B17026_001E, B17026_002E, B17026_003E, B17026...  \nB22003      [B22003_001E, B22003_002E, B22003_003E, B22003...  \nB23024      [B23024_001E, B23024_002E, B23024_003E, B23024...  \nB29003                [B29003_001E, B29003_002E, B29003_003E]  \nB99171      [B99171_001E, B99171_002E, B99171_003E, B99171...  \nB99172      [B99172_001E, B99172_002E, B99172_003E, B99172...  \nC17002      [C17002_001E, C17002_002E, C17002_003E, C17002...  \nC18130      [C18130_001E, C18130_002E, C18130_003E, C18130...  \nC18131      [C18131_001E, C18131_002E, C18131_003E, C18131...  \nC21007      [C21007_001E, C21007_002E, C21007_003E, C21007...  \nC27016      [C27016_001E, C27016_002E, C27016_003E, C27016...  \nC27017      [C27017_001E, C27017_002E, C27017_003E, C27017...  \nC27018      [C27018_001E, C27018_002E, C27018_003E, C27018...  \n\n\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nB17001A_001E\nB17001A_002E\nB17001A_003E\nB17001A_004E\nB17001A_005E\nB17001A_006E\nB17001A_007E\nB17001A_008E\n...\nB17001_051E\nB17001_052E\nB17001_053E\nB17001_054E\nB17001_055E\nB17001_056E\nB17001_057E\nB17001_058E\nB17001_059E\nNAME\n\n\n\n\n0\n09001010400\nPOLYGON ((-8201121.400 5017141.610, -8201100.1...\n4698.0\n399.0\n232.0\n0.0\n0.0\n17.0\n0.0\n15.0\n...\n0.0\n88.0\n252.0\n511.0\n282.0\n443.0\n358.0\n343.0\n273.0\nCensus Tract 104, Fairfield County, Connecticut\n\n\n1\n09001010600\nPOLYGON ((-8196750.330 5017084.050, -8196748.9...\n1513.0\n151.0\n62.0\n10.0\n10.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n36.0\n135.0\n130.0\n171.0\n147.0\n172.0\n118.0\nCensus Tract 106, Fairfield County, Connecticut\n\n\n2\n09001010300\nPOLYGON ((-8200317.120 5018093.350, -8200295.0...\n3300.0\n152.0\n62.0\n0.0\n0.0\n21.0\n0.0\n0.0\n...\n0.0\n11.0\n108.0\n91.0\n254.0\n246.0\n367.0\n167.0\n185.0\nCensus Tract 103, Fairfield County, Connecticut\n\n\n3\n09001011300\nPOLYGON ((-8199823.530 5012413.050, -8199813.0...\n2530.0\n262.0\n139.0\n0.0\n15.0\n41.0\n28.0\n0.0\n...\n8.0\n27.0\n91.0\n346.0\n202.0\n261.0\n226.0\n56.0\n100.0\nCensus Tract 113, Fairfield County, Connecticut\n\n\n4\n09001020500\nPOLYGON ((-8193009.330 5026964.430, -8192869.1...\n4094.0\n81.0\n48.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n45.0\n22.0\n186.0\n110.0\n267.0\n318.0\n364.0\n229.0\n376.0\nCensus Tract 205, Fairfield County, Connecticut\n\n\n\n\n5 rows × 596 columns\n\n\n\n\n\nprint(acs.filter_tables('HOUSING', by = 'description'))\n# Total Population in occupied housing\nhousing_ct = cen.products.ACS(2019).from_state('Connecticut', \nvariables = 'B25008')\nhousing_ct.head()\n\n                                                  description  \\\ntable_name                                                      \nB25001                                          HOUSING UNITS   \nB25008      TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...   \nB25010      AVERAGE HOUSEHOLD SIZE OF OCCUPIED HOUSING UNI...   \nB25026      TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...   \nB25033      TOTAL POPULATION IN OCCUPIED HOUSING UNITS BY ...   \nB25047              PLUMBING FACILITIES FOR ALL HOUSING UNITS   \nB25048         PLUMBING FACILITIES FOR OCCUPIED HOUSING UNITS   \nB25051               KITCHEN FACILITIES FOR ALL HOUSING UNITS   \nB25052          KITCHEN FACILITIES FOR OCCUPIED HOUSING UNITS   \nB25101      MORTGAGE STATUS BY MONTHLY HOUSING COSTS AS A ...   \nB25104                                  MONTHLY HOUSING COSTS   \nB25105                 MEDIAN MONTHLY HOUSING COSTS (DOLLARS)   \nB25106      TENURE BY HOUSING COSTS AS A PERCENTAGE OF HOU...   \nB98001                         UNWEIGHTED HOUSING UNIT SAMPLE   \nB98011                             HOUSING UNIT COVERAGE RATE   \nB98021      HOUSING UNIT RESPONSE AND NONRESPONSE RATES WI...   \nB98032      OVERALL HOUSING UNIT CHARACTERISTIC ALLOCATION...   \nB992523     ALLOCATION OF SELECTED MONTHLY OWNER COSTS FOR...   \n\n                                                      columns  \ntable_name                                                     \nB25001                                          [B25001_001E]  \nB25008                [B25008_001E, B25008_002E, B25008_003E]  \nB25010                [B25010_001E, B25010_002E, B25010_003E]  \nB25026      [B25026_001E, B25026_002E, B25026_003E, B25026...  \nB25033      [B25033_001E, B25033_002E, B25033_003E, B25033...  \nB25047                [B25047_001E, B25047_002E, B25047_003E]  \nB25048                [B25048_001E, B25048_002E, B25048_003E]  \nB25051                [B25051_001E, B25051_002E, B25051_003E]  \nB25052                [B25052_001E, B25052_002E, B25052_003E]  \nB25101      [B25101_001E, B25101_002E, B25101_003E, B25101...  \nB25104      [B25104_001E, B25104_002E, B25104_003E, B25104...  \nB25105                                          [B25105_001E]  \nB25106      [B25106_001E, B25106_002E, B25106_003E, B25106...  \nB98001                             [B98001_001E, B98001_002E]  \nB98011                                          [B98011_001E]  \nB98021      [B98021_001E, B98021_002E, B98021_003E, B98021...  \nB98032                                          [B98032_001E]  \nB992523            [B992523_001E, B992523_002E, B992523_003E]  \n\n\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nB25008_001E\nB25008_002E\nB25008_003E\nNAME\nstate\ncounty\ntract\n\n\n\n\n0\n09001010400\nPOLYGON ((-8201121.400 5017141.610, -8201100.1...\n5777.0\n3647.0\n2130.0\nCensus Tract 104, Fairfield County, Connecticut\n09\n001\n010400\n\n\n1\n09001010600\nPOLYGON ((-8196750.330 5017084.050, -8196748.9...\n2052.0\n858.0\n1194.0\nCensus Tract 106, Fairfield County, Connecticut\n09\n001\n010600\n\n\n2\n09001010300\nPOLYGON ((-8200317.120 5018093.350, -8200295.0...\n3717.0\n2791.0\n926.0\nCensus Tract 103, Fairfield County, Connecticut\n09\n001\n010300\n\n\n3\n09001011300\nPOLYGON ((-8199823.530 5012413.050, -8199813.0...\n3411.0\n1500.0\n1911.0\nCensus Tract 113, Fairfield County, Connecticut\n09\n001\n011300\n\n\n4\n09001020500\nPOLYGON ((-8193009.330 5026964.430, -8192869.1...\n4746.0\n4470.0\n276.0\nCensus Tract 205, Fairfield County, Connecticut\n09\n001\n020500\n\n\n\n\n\n\n\n\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 16))\n\nax1 = poverty_ct.plot('B17001A_001E', ax = ax1, cmap = 'viridis', legend = True)\nax2 = housing_ct.plot('B25008_001E', ax = ax2, cmap = 'viridis', legend = True)\n\n\n\n\n\n\n\n\nBased on this we can visualize the numbers of individuals who are in poverty and individuals who have housing by census tract.\nA further step is to run statistical analysis to see if there is any dependence and correlation between poverty and housing attainment within Connecituct.\n\n\n5.5.6 References\nhttps://cenpy-devs.github.io/cenpy/api.html https://pypi.org/project/CensusData/ https://pypi.org/project/cenpy/0.9.1/ https://geopandas.org/en/stable/getting_started/introduction.html https://data.census.gov/table/ACSDT1Y2022.B02001?t=Race+and+Ethnicity https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2019.html#list-tab-79594641",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#database-operations-with-sql",
    "href": "import.html#database-operations-with-sql",
    "title": "5  Data Import/Export",
    "section": "5.6 Database Operations with SQL",
    "text": "5.6 Database Operations with SQL\n\n5.6.1 Introduction\nToday, I will be presenting on Database Operations using SQL and I will demonstrate it by using sqlite3 in Python and I will use our cleaned rodent data set. This presentation will briefly go over what Professor Haim Bar talked about during his talk and we will dive into some more advanced SQL commands.\n\n\n5.6.2 What is SQL?\nStructured Query Language (SQL) is a standard language for accessing and manipulating databases. It is used within relational database management systems such as MS SQL Server, MySQL, Oracle, etc… to perform CRUD operations.\n\n5.6.2.1 CRUD\n\nCreate - inserting new records/values\n\nINSERT INTO table_name VALUES (field value 1, field value 2, …)\n\nRead - searching for records\n\nSELECT field 1, field2, … FROM table_name [WHERE condition]\n\nUpdate - modifying existing records\n\nUPDATE table_name SET field1=value1, field2=value2, … [WHERE condition]\n\nDelete - removing existing records\n\nDELETE FROM table_name [WHERE condition]\n\n\n5.6.2.2 RDBMS vs NoSQL\nRDBMS\n\nBasis for SQL\nModels relationships in “Tables”\nRows and Columns (Record/Fields)\nMS Access, SQL Server, MySQL\nTwo tables can be linked if they have a matching field\n\nNoSQL\n\n“Not only” SQL or Non-SQL\nGreat with large data sets\nKey / Value Pairs\nDoes not always use SQL\nAWS DynamoDB\n\n\n\n\n5.6.3 Recap and Overview of some Basics\n\nKey Notes from Professor Bar\n\n\n5.6.3.1 Creating Tables\nCREATE TABLE contacts(\n    contact_id INTEGER PRIMARY KEY,\n    first_name TEXT NOT NULL,\n    last_name TEXT NOT NULL,\n    email TEXT NOT NULL UNIQUE,\n    phone TEXT NOT NULL UNIQUE\n);\n\nCreates a table named contacts.\nFive fields: contact_id, first_name, last_name, email, phone\nPRIMARY KEY: uniquely identifies each record in the table.\nSome columns will store character data while others will store integer data.\nNOT NULL: Columns cannot be empty.\nUNIQUE: All values in this specific column have to be different from each other.\n\nCREATE TABLE contact_groups (\n    contact_id INTEGER PRIMARY KEY,\n    group_id INTEGER PRIMARY KEY,\n    PRIMARY KEY(contact_id, group_id)\n    FOREIGN KEY(contact_id)\n        REFERENCES contacts (contact_id)\n            ON DELETE CASCADE\n            ON UPDATE NO ACTION,\n    FOREIGN KEY(group_id)\n        REFERENCES groups (group_Id)\n            ON DELETE CASCADE\n            ON UPDATE NO ACTION\n);\n\nPRIMARY KEY() declares which variables are primary keys. We should remove the first two Primary Key declarations to have proper syntax.\nFOREIGN KEY() declares the variable as a foreign key\n\nA constraint that is used to prevent actions that would destroy links between tables.\nA field in one table that refers to a different PRIMARY KEY in another table.\nTable with a foreign key is a child table.\nPrevents invalid data from being inserted into the foreign key column because it has to be a value contained in the parent table.\n\nON DELETE CASECADE: If a record in contacts is deleted, any corresponding records in contact_groups will also be deleted.\nON UPDATE NO ACTION: If a contact_id in contacts is updated, no action will be taken on contact_groups.\n\nCan be replaced with RESTRICT.\n\n\n\n\n5.6.3.2 Inserting and Searching\n\nMultiple ways to insert values into a specific table.\n\nINSERT INTO artists (name) VALUES('Bud Powell')\nINSERT INTO artists (name)\nVALUES\n    ('Buddy Rich')\n    ('Example 2')\n\nUpdating multiple or all rows.\n\nSELECT employeeid, firstname, lastname, title, email FROM employees;\nUPDATE employees SET lastname = 'smith' WHERE employeeid = 3;\n\n# Multiple\nUPDATE employees SET city = 'Toronto', state='ON', \npostalcode= 'M5p2n7' WHERE employeeid=4;\n\n# All\nUPDATE employees SET email = LOWER(firstname || \".\" || lastname || '@gmail.com';\n\n\n5.6.3.3 Deleting\n\nDeleting Tables/Databases\n\nDROP TABLE addresses;\nDROP DATABASE databasename;\nNote: Dropping a database will result in loss of information.\n\nDeleting entries given a condition\n\nDELETE FROM table_name WHERE condition;\n\n\n\n5.6.4 Utilizing sqlite3 in Python with Rodent Data Set\nConnecting to SQLite3 Using Python\n\nimport pandas as pd\nimport sqlite3\n\n# Connects to Database\nconn = sqlite3.connect(\"presentation.db\")\n\n# Using our cleaned rodent data\ndata = pd.read_feather(\"data/rodent_2022-2023.feather\")\n\n\n# Creates Rodent table\ndata.to_sql(\n            'rodent', # Name of SQL table\n            conn, # sqlite3 connection\n            if_exists = 'replace',\n            index = False\n)\n\n# Cursor, Instance that allows you to invoke methods to execute SQL statements.\ncursor = conn.cursor()\n\n# Executing example statement to show functionality\ncursor.execute(\"\"\"\n                SELECT *\n                FROM rodent\n                LIMIT 5\n               \"\"\")\nrows = cursor.fetchall()\n\n# Column names\ncolumns = [description[0] for description in cursor.description]\n\n# Displaying the output in a table format\npd.DataFrame(rows, columns=columns)\n\n\n\n\n\n\n\n\n\nunique_key\ncreated_date\nclosed_date\nagency\nagency_name\ncomplaint_type\ndescriptor\nlocation_type\nincident_zip\nincident_address\n...\nbridge_highway_segment\nlatitude\nlongitude\nlocation\nzip_codes\ncommunity_districts\nborough_boundaries\ncity_council_districts\npolice_precincts\npolice_precinct\n\n\n\n\n0\n59893776\n2023-12-31 23:05:41\n2023-12-31 23:05:41\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n3+ Family Apt. Building\n11216\n265 PUTNAM AVENUE\n...\nNone\n40.683857\n-73.951645\n(40.683855196486164, -73.95164557951071)\n17618.0\n69.0\n2.0\n49.0\n51.0\n51.0\n\n\n1\n59887523\n2023-12-31 22:19:22\n2024-01-03 08:47:02\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\nCommercial Building\n10028\n1538 THIRD AVENUE\n...\nNone\n40.779243\n-73.953690\n(40.77924175816874, -73.95368859796383)\n10099.0\n23.0\n4.0\n1.0\n11.0\n11.0\n\n\n2\n59891998\n2023-12-31 22:03:12\n2023-12-31 22:03:12\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n3+ Family Apt. Building\n10458\n2489 TIEBOUT AVENUE\n...\nNone\n40.861694\n-73.894989\n(40.861693023118924, -73.89499228560491)\n10936.0\n6.0\n5.0\n22.0\n29.0\n29.0\n\n\n3\n59887520\n2023-12-31 21:13:02\n2024-01-03 09:33:43\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nMouse Sighting\n3+ Family Apt. Building\n11206\n116 JEFFERSON STREET\n...\nNone\n40.699741\n-73.930733\n(40.69974221739347, -73.93073474327662)\n17213.0\n42.0\n2.0\n30.0\n53.0\n53.0\n\n\n4\n59889297\n2023-12-31 20:50:10\nNone\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n1-2 Family Dwelling\n11206\n114 ELLERY STREET\n...\nNone\n40.698444\n-73.948578\n(40.69844506428295, -73.94858040356128)\n17213.0\n69.0\n2.0\n49.0\n51.0\n51.0\n\n\n\n\n5 rows × 47 columns\n\n\n\n\n\n\n5.6.5 References\n\nhttps://www.w3schools.com/sql/\nhttps://aws.amazon.com/what-is/sql/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "6  Communication and Ethics",
    "section": "",
    "text": "6.1 Data Science Communication Skills\nThis section was written by Matt Elliott.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communication and Ethics</span>"
    ]
  },
  {
    "objectID": "communication.html#data-science-communication-skills",
    "href": "communication.html#data-science-communication-skills",
    "title": "6  Communication and Ethics",
    "section": "",
    "text": "6.1.1 Introduction\nHi! My name is Matt Elliott and I decided to start this presentation off by introducing myself and what I hope to become moving forward! I am currently a Senior aiming to graduate in Fall 2024 with a Bachelor’s of Arts in Individualized Data Science under the CLAS’ IISP program. My primary advisor is our professor Jun Yan! Moving forward, I hope to learn valuable skill sets and ideas from both this course and the Data Science field. Even using Quarto instead of typical Google Slides is a step for me in creating new skills! The topic I chose to discuss today in class is “Data Science Communication Skills” . I find this to be one of the most crucial topics to discuss about Data Science; since Data Scientists are often the glue that keeps projects, companies, and ideas together.\n\n\n6.1.2 Describing Data Science and its Rise\n\nAccording to IBM, Data Science “combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI), and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data.\nThese insights are then used to “guide decision making” and create “strategic planning”\nAccording to the U.S. Bureau of Labor Statistics, Data Science is\n“projected to grow 35 percent from 2022 to 2032, much faster than the average for all occupations”\nThe median annual pay for Data Scientists in May 2022 was around $103,500, showing its high demand in a monetary light\n\n\n\n\nData Science\n\n\n\n\n6.1.3 Why does Communication Matter?\n\nCommunication matters in any professional setting in our lives, and especially in a field that can be extremely confusing and perplexing to those who are viewing it from an outside perspective.\nThe Data Incubator states that “You may work alongside data analysts or other scientists as part of a team, especially when handling large datasets or working on big projects. Beyond this, you may also frequently work with other teams of professionals who don’t work with data. Thus, it’s essential to be an excellent communicator to work with others effectively”. This is an important idea as being able to be a cooperative person will create partnerships that flow in the correct manner.\nData Scientists often present insights to other partners in order to facilitate goals and achievements in a professional setting.\nKaren Church for Medium writes that “Communication enables data scientists to gather all the necessary information, clarify needs and expectations of stakeholders, and align their work with broader business goals.”\n\n\n\n\nCommunication\n\n\n\n\n6.1.4 Inherent Communication Skills\n\nUsing the right communication methods\nFriendliness\nConfidence\nVolume and tone\nEmpathy\nRespect\nCues\n\n\n\n\nInherent Communication\n\n\n\n\n6.1.5 Identify your audience\n\nThe Data Incubator states that “Transferring knowledge across departments is crucial, so it’s vital to share insights and analyses in simple, clear terms that don’t overwhelm individuals with jargon or technical details.”\nIdentifying your audience and speaking their language is an important step, as it can vary to low familiarity to full comprehension of the topic\nIn a 2018 HBR.org article, Hugo Bowne-Anderson interviewed 35 data scientists on his podcast and found that their main issues were: “lack of management/financial support,” “lack of clear questions to answer,” “results not used by decision makers,” and “explaining data science to others.” from Harvard Business Review\nKnowing your audience in this situation can cover your back on how Data Scientists are treated in the field, communicating creates cooperation that can lead to the avoidance of these issues found.\n\n\n\n\nAudience\n\n\n\n\n6.1.6 Data Applictions\n\nData scientists have their hands full as many different fields and professions can use data analytics and information to facilitate their operations\nThese fields include:\n\nHealthcare\nMedia/Entertainment\nRetail\nTelecommunication\nAutomotive\nDigital Marketing\nCyber security\n\nData science communication skills vary in these fields, as some may prefer verbal information or visual information.\n\n\n\n\nApplications\n\n\n\n\n6.1.7 Storytelling\n\nStorytelling in the context of Data Science gives the audience a shared goal within understanding the topics and information given.\nThe goals of promoting improved customer service, innovation, or operation optimization need to be conveyed in a manner that is direct and professional.\nAccording to Sonali Verghese for Medium, here are the possibilities of telling a story within the Data Science field:\n\nExplain how you arrived at a particular conclusion\nJustify rationally why you approached a problem in a specific manner\nConvey interesting insights in a way that gets people to think or act differently\nPersuade your audience that your results are conclusive and can be turned into something actionable\nExpress why your findings are valuable and how they fit into the overall picture\n\nThis is an inspiring quotation that I found while researching this presentation. Valentin Mucke for Medium: “Data science is about humans. Data scientists must remember that, and not just when presenting to people with a non-technical background. It’s important to find common ground with everyone you work with to build trust and move forward effectively.\n\n\n\n\nStorytelling\n\n\n\n\n6.1.8 Data Visualization\n\nThe idea of expressing data through visualization has been a vital step for Data Scientists in the field\nExamples of Data Visualization for Data Scientists are:\n\nMany of these forms of visualization can be combined with other learned skill sets that will be mentioned in the next topic\n\n\n\n\n\nVisuals\n\n\n\n\n6.1.9 Usable Skill Sets for Data Communication\n\nCoding languages: Python, Structured Query Language, R, Visual Basic for Applications, Julia\nStatistical programming: the process of using computer programming languages to analyze and manipulate data for statistical purposes\nStatistics and probability: help predict the likelihood of future events and understand patterns in data\nMachine learning/Artificial intelligence: automates the data analysis process and makes predictions in real-time without human involvement, leading to further building and training of a data model to make real-time predictions\nStatistical visualization: the graphical representation of information and data that uses visual elements like charts, graphs, and maps, and tools to provide an accessible ways to see and understand trends, outliers, and patterns in data\nData management: process of collecting, storing, organizing and maintaining data to ensure that it is accurate and accessible to those who need it reliably throughout the data science project lifecycle\n\n\n\n\nskillsets\n\n\n\n\n6.1.10 Gather questions and Feedback\n\nAccording to the Data Incubator, a step to take before finalizing the project or an end of a report is to “consider soliciting direct feedback from your audience. It doesn’t matter if you have to prompt them to ask you questions or if they’re impatient to put your knowledge to the test—this form of interaction can help you improve your communication skills and establish a successful career as a data scientist.”\nBeing able to interact with your audience gives them a better understanding of the topic at hand, and can help avoid ambiguity that would occur if communication was not present\n\n\n\n\nFeedback\n\n\n\n\n6.1.11 Sources\nhttps://towardsdatascience.com/tell-stories-with-data-communication-in-data-science-5266f7671d7\nhttps://hbr.org/2019/01/data-science-and-the-art-of-persuasion\nhttps://towardsdatascience.com/communicating-as-a-data-scientist-why-it-matters-and-how-to-do-it-well-f1c34d28c7c4\nhttps://www.thedataincubator.com/blog/2022/10/13/improve-your-data-science-communication/\nhttps://emeritus.org/in/learn/why-communication-skills-are-important-for-a-data-analyst/\nhttps://medium.com/intercom-rad/the-most-underrated-skill-in-data-science-communication-7ed2fab82801\nhttps://www.ibm.com/topics/data-science\nhttps://www.bls.gov/ooh/math/data-scientists.htm\nhttps://medium.com/analytics-vidhya/introduction-to-data-science-28deb32878e7\nhttps://blog.jostle.me/customerresources/3-actionable-communication-tips\nhttps://www.breathehr.com/en-gb/blog/topic/employee-performance/effective-communication-is-key-to-your-business-success\nhttps://ideas.ted.com/before-your-next-presentation-or-speech-heres-the-first-thing-you-must-think-about/\nhttps://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/12/Data-Science-Applications-Edureka.jpg  https://lectera.com/info/storage/img/20210805/fa586bb6c04bf0989d70_808xFull.jpg\nhttps://thenewstack.io/7-best-practices-for-data-visualization/\nhttps://www.learningtree.com/blog/the-6-major-skill-areas-of-data-science/\nhttps://www.poynter.org/reporting-editing/2019/cohort4/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communication and Ethics</span>"
    ]
  },
  {
    "objectID": "communication.html#ethical-considerations-for-data-scientists",
    "href": "communication.html#ethical-considerations-for-data-scientists",
    "title": "6  Communication and Ethics",
    "section": "6.2 Ethical Considerations for Data Scientists",
    "text": "6.2 Ethical Considerations for Data Scientists\nThe field of data science, with its vast capabilities for societal impact, necessitates a robust ethical framework. Below, we outline key ethical principles that should guide data scientists in their work, referencing foundational works that have contributed to the ongoing discourse on ethics in data science.\n\n6.2.1 Privacy and Anonymity\nPrivacy refers to the right of individuals to control information about themselves and decides who can access it. Anonymity is closely related, allowing individuals to act or communicate without revealing their identities. In data science, respecting privacy means ensuring that personal data is used in a way that is consistent with the expectations of the individuals it pertains to and adheres to applicable laws and ethical guidelines. Anonymity protects individuals from potential harm that could arise from the disclosure of their identity alongside their data. Techniques like data anonymization are employed to protect privacy and anonymity, stripping datasets of personally identifiable information to prevent the tracing of data back to an individual.\nEnsuring the privacy and anonymity of data subjects is paramount in data science. O’Neil (2016) and Noble (2018) discuss the challenges and implications of privacy in the age of big data, highlighting the necessity for data scientists to employ techniques such as anonymization and secure data handling to protect individuals’ identities and personal information.\n\n\n6.2.2 Bias and Fairness\nBias in data science refers to systematic errors that favor certain outcomes over others, which can stem from the data collection process, algorithmic design, or model interpretation stages. Fairness is the principle that seeks to ensure equitable treatment and outcomes for all individuals, particularly across different demographic groups. Addressing bias and ensuring fairness involve critically evaluating and adjusting datasets and algorithms to prevent discrimination against any individual or group. This can include diversifying training data, employing statistical methods to identify and correct for biases, and designing algorithms that account for fairness metrics.\nThe presence of bias in data and algorithms represents a significant ethical challenge, potentially leading to discrimination and unfair treatment. Kearns and Roth (2019) explore mechanisms for detecting and mitigating bias to ensure fairness in algorithmic decision-making. Data scientists must critically examine both the data and the algorithms they use to prevent perpetuating or amplifying biases.\n\n\n6.2.3 Transparency and Accountability\nTransparency in data science involves openness about the methodologies, data sources, and algorithms used in developing models, allowing others to understand and evaluate the decision-making process. Accountability means that data scientists and their organizations take responsibility for the outcomes of their data-driven decisions, including addressing any negative impacts. Achieving transparency and accountability requires thorough documentation, sharing of methodologies and data (where possible), and the creation of mechanisms for auditing and challenging algorithmic decisions.\nTransparency in the development and deployment of data science models, along with accountability for their outcomes, is critical. O’Neil (2016) argues for the necessity of making data science processes transparent and accountable, particularly when models influence significant decisions affecting individuals’ lives.\n\n\n6.2.4 Data Integrity and Quality\nData integrity refers to the accuracy, consistency, and reliability of data throughout its lifecycle. Quality of data means that the data is fit for its intended uses in operations, decision-making, and planning. In data science, ensuring data integrity and quality is critical for building models that accurately represent the world and make reliable predictions. This involves rigorous data collection, cleaning, and validation processes to ensure that data is not corrupted, accurately reflects the phenomena it is supposed to represent, and is used appropriately in models.\nMaintaining the integrity and quality of data is fundamental to ethical data science practice. Saltz and Dewar (2019) emphasize the importance of ensuring that data used and produced by data scientists are accurate, valid, and reliable, supporting the credibility of data science findings and applications.\n\n\n6.2.5 Respect for Intellectual Property\nRespecting intellectual property in data science means acknowledging and adhering to the legal and moral rights of creators and owners of data, algorithms, software, and other resources used in data science projects. This includes proper attribution of sources, complying with licensing agreements, and not using copyrighted materials without permission. Ethical practice requires data scientists to be aware of the origins of the data and tools they use, to ensure that their use respects the rights of the creators and is in line with any terms and conditions of use.\nData scientists must respect intellectual property rights, properly attributing data sources and adhering to the terms of use for data and software. The ethical use of data and software tools, acknowledging creators and complying with licensing, is essential for fostering a culture of integrity within the field (Saltz and Dewar 2019).\n\n\n\n\nKearns, Michael, and Aaron Roth. 2019. The Ethical Algorithm: The Science of Socially Aware Algorithm Design. Oxford University Press.\n\n\nNoble, Safiya Umoja. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown.\n\n\nSaltz, Jeffrey S., and Neil Dewar. 2019. “Data Science Ethical Considerations: A Systematic Literature Review and Proposed Project Framework.” Ethics and Information Technology 21 (3): 197–208.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communication and Ethics</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "7.1 Descriptive Statistics\nWhen you first begin working with a new dataset, it is important to develop an understanding of the data’s overall behavior. This is important for both understanding numerical and categorical data.\nFor numeric data, we can develop this understanding through the use of descriptive statistics. The goal of descriptive statistics is to understand three primary elements of a given variable [2]:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#descriptive-statistics",
    "href": "eda.html#descriptive-statistics",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "Presented by Joshua Lee\n\n\n\n\ndistribution\ncentral tendency\nvariability\n\n\n7.1.1 Variable Distributions\nEvery random variable is given by a probability distribution, which is “a mathematical function that describes the probability of different possible values of a variable” [3].\nThere are a few common types of distributions which appear frequently in real-world data [3]:\n\nUniform:\nPoisson:\nBinomial:\nNormal and Standard Normal:\nGamma:\nChi-squared:\nExponential\nBeta\nT-distribution\nF-distribution\n\nUnderstanding the distribution of different variables in a given dataset can inform how we may decide to transform that data. For example, in the context of the rodent data, we are interested in the patterns which are associated with “rodent” complaints which occur.\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\ndata = pd.read_feather(\"data/rodent_2022-2023.feather\")\n\nNow that we have read in the data, we can examine the distributions of several important variables. Namely, let us examine a numerical variable which is associated with rodent sightings:\n\ndata.head(2).T\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nunique_key\n59893776\n59887523\n\n\ncreated_date\n2023-12-31 23:05:41\n2023-12-31 22:19:22\n\n\nclosed_date\n2023-12-31 23:05:41\n2024-01-03 08:47:02\n\n\nagency\nDOHMH\nDOHMH\n\n\nagency_name\nDepartment of Health and Mental Hygiene\nDepartment of Health and Mental Hygiene\n\n\ncomplaint_type\nRodent\nRodent\n\n\ndescriptor\nRat Sighting\nRat Sighting\n\n\nlocation_type\n3+ Family Apt. Building\nCommercial Building\n\n\nincident_zip\n11216\n10028\n\n\nincident_address\n265 PUTNAM AVENUE\n1538 THIRD AVENUE\n\n\nstreet_name\nPUTNAM AVENUE\nTHIRD AVENUE\n\n\ncross_street_1\nBEDFORD AVENUE\nEAST 86 STREET\n\n\ncross_street_2\nNOSTRAND AVENUE\nEAST 87 STREET\n\n\nintersection_street_1\nBEDFORD AVENUE\nEAST 86 STREET\n\n\nintersection_street_2\nNOSTRAND AVENUE\nEAST 87 STREET\n\n\naddress_type\nADDRESS\nADDRESS\n\n\ncity\nBROOKLYN\nNEW YORK\n\n\nlandmark\nPUTNAM AVENUE\n3 AVENUE\n\n\nfacility_type\nNaN\nNaN\n\n\nstatus\nClosed\nClosed\n\n\ndue_date\nNaN\nNaN\n\n\nresolution_description\nThe Department of Health and Mental Hygiene fo...\nThis service request was closed because the De...\n\n\nresolution_action_updated_date\n12/31/2023 11:05:41 PM\n12/31/2023 10:19:22 PM\n\n\ncommunity_board\n03 BROOKLYN\n08 MANHATTAN\n\n\nbbl\n3018220072.0\n1015157503.0\n\n\nborough\nBROOKLYN\nMANHATTAN\n\n\nx_coordinate_(state_plane)\n997661.0\n997076.0\n\n\ny_coordinate_(state_plane)\n188427.0\n223179.0\n\n\nopen_data_channel_type\nMOBILE\nMOBILE\n\n\npark_facility_name\nNaN\nNaN\n\n\npark_borough\nBROOKLYN\nMANHATTAN\n\n\nvehicle_type\nNaN\nNaN\n\n\ntaxi_company_borough\nNaN\nNaN\n\n\ntaxi_pick_up_location\nNaN\nNaN\n\n\nbridge_highway_name\nNaN\nNaN\n\n\nbridge_highway_direction\nNaN\nNaN\n\n\nroad_ramp\nNaN\nNaN\n\n\nbridge_highway_segment\nNaN\nNaN\n\n\nlatitude\n40.683857\n40.779243\n\n\nlongitude\n-73.951645\n-73.95369\n\n\nlocation\n(40.683855196486164, -73.95164557951071)\n(40.77924175816874, -73.95368859796383)\n\n\nzip_codes\n17618.0\n10099.0\n\n\ncommunity_districts\n69.0\n23.0\n\n\nborough_boundaries\n2.0\n4.0\n\n\ncity_council_districts\n49.0\n1.0\n\n\npolice_precincts\n51.0\n11.0\n\n\npolice_precinct\n51.0\n11.0\n\n\n\n\n\n\n\n\nIn this dataset, the most relevant numerical data to consider is the time between the opening of a rodent complaint and its closing. All of the other relevant variables are either geospatial or categorical:\n\n# convert strings into datetime objects\ndata[\"closed_date\"] =  pd.to_datetime(data[\"closed_date\"],\n                                     format=\"%m/%d/%Y %I:%M:%S %p\")\ndata[\"created_date\"] = pd.to_datetime(data[\"created_date\"],\n                                      format=\"%m/%d/%Y %I:%M:%S %p\")\n\ndata[\"time_dif\"] = data[\"closed_date\"] - data[\"created_date\"]\n\n# set the time delta as the number of hours difference\ndata[\"time_dif\"] = data[\"time_dif\"].dt.total_seconds()/3600\ndata[\"time_dif\"]\n\n0         0.000000\n1        58.461111\n2         0.000000\n3        60.344722\n4              NaN\n           ...    \n82864    51.862222\n82865     0.000000\n82866    57.840278\n82867    58.551944\n82868     0.000000\nName: time_dif, Length: 82869, dtype: float64\n\n\nNow we have a column describing the time difference between when a complaint is opened and closed. We can plot this distribution with plotly to provide a better visual representation of the distribution:\n\nNote, every value in the data is shifted up 1 for plotting purposes. Fitting an exponential distribution with parameter \\(\\lambda=0\\) exactly is not possible to fit precisely due to divide by \\(0\\) errors. Additionally, this plot ignores the location parameter provided by output from stats.expon.fit() since the mean brought up significantly by outliers at the asbolute extremes of the distribution (the higher end).\n\n\nimport plotly.graph_objects as go\nfrom scipy import stats\n\n# add a 1 to avoid weird zero errors\nresponse_dat2 = data[\"time_dif\"].dropna() + 1\n\nhist2 = go.Histogram(x=response_dat2, \n                    nbinsx=2500, \n                    opacity=0.75, \n                    name='response time', \n                    histnorm='probability density')\n\n# Calculate KDE\nscale, loc = stats.expon.fit(response_dat2.values)\nx_range = np.linspace(min(response_dat2), max(response_dat2), 10000)\nfitted_vals = stats.expon.pdf(x_range, loc=0.2, scale=scale)\nfitted_dist = go.Scatter(x=x_range, y=fitted_vals, mode=\"lines\", \n                         name=\"Fitted Exponential Distribution\")\n\n# Create a layout\nlayout = go.Layout(title='Complaint Response Time Histogram and Density',\n                   xaxis=dict(title='Complaint Response Time (hours)', range=[0,100]),\n                   yaxis=dict(title='Density', range=[0,0.2]),\n                   bargap=0.1\n                  )\n\n# Create a figure and add both the histogram and KDE\nfig = go.Figure(data=[hist2, fitted_dist], layout=layout)\n\n# Show the figure\nfig.show()\n\n                                                \n\n\nAs you can see, there is a strong right skew (the majority of observations are concentrated at the lower end of the distribution, but there are a few observations at the extreme right end).\nHere, we use pandas plotting to generate a density estimation curve.\n\nx_range = np.linspace(response_dat2.min(), response_dat2.max(), 1000)\nresponse_dat2.plot.kde(ind=x_range)\n\n\n\n\n\n\n\n\nWe can compare this density curve to plots of the exponential distribution, and see that this variable (complaint response times) closely match an exponential distribution with a very high \\(\\lambda\\) parameter value. Below is a figure displaying a series of exponential distributions for different values of \\(\\lambda\\):\n\nimport matplotlib.pyplot as plt\n\n# Define the lambda parameters\nlambdas = [0.5, 1, 2, 4, 8]\n\n# Define the x range\nx = np.linspace(0, 2*np.pi, 1000)\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Plot the exponential distribution for each lambda\nfor lam in lambdas:\n    y = lam * np.exp(-lam * x)\n    plt.plot(x, y, label=f'λ = {lam}')\n\n# Set the x-axis labels\nplt.xticks([np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], ['π/2', 'π', '3π/2', '2π'])\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.1.2 Central Tendency Measures\nNow that we have examined the distribution of the response time, it is appropriate to investigate the important measures of central tendency for the data.\nThere are three main measures of central tendency which are used:\n\nMean: The average or expected value of a random variable\n\n\\(\\overline{X} = (1/n)\\sum_{i=1}^{n} X_{i}\\) (where \\(X_{i}\\text{s}\\) are independent random samples from the same distribution)\n\nMedian: exact middle value of a random variable [5]\n\nFor even \\(n\\), \\(\\overset{\\sim}{X} = (1/2)[X_{(n/2+1)} + X_{(n/2)}]\\)\nFor odd \\(n\\), \\(\\overset{\\sim}{X} = X_{([n+1]/2)}\\)\n\nMode: the most frequently occurring value of a random variable\n\nFor the given variable (complaint response time), we can find each of the respective statistics using pandas:\n\nNOTE: pandas.Series.mode() returns the most commonly occurring value in the Series, or a Series of the most commonly occurring values if there is a tie between multiple values. It does not calculate multiple modes in the case of a multi-modal distribution. Here, Series.mode() returns \\(0\\) and \\(0.000\\dots\\) so I elected to choose the first element of that series for display.\n\n\ncentral_tendency = pd.Series(\n    {\"Mean\": response_dat2.mean(), \n     \"Median\": response_dat2.median(), \n     \"Mode\": response_dat2.mode().iloc[0]}\n)\ncentral_tendency\n\nMean      53.346003\nMedian     1.000000\nMode       1.000000\ndtype: float64\n\n\nAs you can see, the most commonly occurring value (as is obvious from the density plot) is 0. This means that the time between when a rodent sighting complaint is filed and responded to (or closed) is most likely to be 0. Additionally, it implies that more than half of all data points have a complaint response time of zero since the median is zero as well.\nIt makes sense that the mean is greater than the median in this case since the distribution is exponential and skewed to the right.\n\n\n7.1.3 Variability Measures\nAs with central tendency, there are also several relevant measures of variance [2]. These include:\n\nrange: \\(X_{(n)} - X_{(1)}\\) - the difference between the greatest observed value and the smallest one.\nstandard deviation: \\(S = \\sqrt{(1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}}\\) - the average difference of values from the observed mean of a sample.\nvariance: Square of the standard deviation of a sample \\(S^{2} = (1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}\\)\nInterquartile Range: \\(X_{[3/4]} - X_{[1/4]}\\) where \\(X_{[p]}\\) is the \\(p\\text{th}\\) sample quantile - A measure of the difference between the 1st and third quantiles of a distribution\n\nWe can easily calculate all of these values using pandas in python [6]\n\nquartiles = response_dat2.quantile([0.25, 0.75])\niqr = quartiles[0.75] - quartiles[0.25]\n\nvariability = pd.Series(\n    {\"range\": response_dat2.max() - response_dat2.min(), \n     \"standard deviation\": response_dat2.std(), \n     \"variance\": response_dat2.std()**2, \n     \"IQR\": iqr}\n)\nvariability\n\nrange                  6530.811944\nstandard deviation      183.977103\nvariance              33847.574408\nIQR                      16.492986\ndtype: float64\n\n\nWe can also use the interquartile range as a means to obtain a rudimentary measure of outliers in the data. Specifically, any observations which are a distance of \\(1.5 * IQR\\) beyond the third or first quartiles.\nSeeing as the first quartile is also the minimum in this, case we only need to be concerned with outliers at the higher end of the spectrum. We calculate the upper fence for outliers as follows [5]:\n\\(\\text{upper fence } = X_{[0.75]} + 1.5\\cdot IQR\\)\n\nupper_lim = quartiles[0.75] + 1.5*iqr\n\noutliers = response_dat2[response_dat2 &gt; upper_lim]\noutliers\n\n1          59.461111\n3          61.344722\n5         188.193889\n10         92.917222\n12        206.714722\n            ...     \n82857    1110.238611\n82860      52.264444\n82864      52.862222\n82866      58.840278\n82867      59.551944\nName: time_dif, Length: 15011, dtype: float64\n\n\nGiven the exponential nature of the distribution, it would be interesting to examine the patterns which occur in categorical variables to see if there may be any connections between those variables and the response time. It may also be useful to examine relationships between geospatial data and the response time.\n\n\n7.1.4 Univariate Categorical Descriptive Statistics\nDescriptive statistics for categorical data are primarily aimed at understanding the rates of occurrence for different categorical variables. These include the following measures [7]:\n\nfrequencies: number of occurrences\npercentages / relative frequencies: the percentage of observations which have a given value for a categorical variable\n\nThese sorts of metrics are often best represented by frequency distribution tables, pie-charts, and bar charts:\nFor example, let us examine the categorical variable “Borough” from the rodent data:\n\n# create a frequency distribution table\ncounts = data[\"borough\"].value_counts()\nproportions = counts/len(data)\ncumulative_proportions = proportions.cumsum()\n\nfrequency_table = pd.DataFrame(\n                    {\"Counts\": counts, \n                    \"Proportions\": proportions, \n                    \"Cumulative Proportion\": cumulative_proportions}\n)\nfrequency_table\n\n\n\n\n\n\n\n\n\nCounts\nProportions\nCumulative Proportion\n\n\nborough\n\n\n\n\n\n\n\nBROOKLYN\n30796\n0.371623\n0.371623\n\n\nMANHATTAN\n22641\n0.273214\n0.644837\n\n\nQUEENS\n13978\n0.168676\n0.813513\n\n\nBRONX\n12829\n0.154811\n0.968323\n\n\nSTATEN ISLAND\n2617\n0.031580\n0.999903\n\n\n\n\n\n\n\n\nThis table demonstrates that the most significant proportion of rodent sightings occurred in the borough of Brooklyn. Additionally, it indicates that Manhattan and Brooklyn collectively represent more than half of all rodent sightings which occur, while Staten Island in particular represents a relatively small proportion.\nWe can also use bar chart to represent this data:\n\n# Create a bar chart\nfig = go.Figure(data=[go.Bar(x=counts.index, y=counts.values)])\n\n# Show the figure\nfig.show()\n\n                                                \n\n\nA pie-chart also serves as a good representation of the relative frequencies of categories:\n\nfig = go.Figure(data=[go.Pie(labels=counts.index, values=counts.values, hole=.2)])\n\n# Show the figure\nfig.show()\n\n                                                \n\n\n\n\n7.1.5 Chi-Squared Significance Tests (Contingency Table Testing)\nIn order to determine whether there exists a dependence between several categorical variables, we can use chi-squared contingency table testing. This is also referred to as the chi-squared test of independence [8]. We will examine this topic by investigating the relationship between the borough and the complaint descriptor variables in the rodents data.\nThe first step in conducting a Chi-squared significance test is to construct a contingency table.\n\ncontingency tables are frequency tables of two variables which are presented simultaneously [8].\n\nThis can be accomplished in python by utilizing the pd.crosstab() function\n\n# produce a contingency table for viewing\ncontingency_table_view = pd.crosstab(data[\"borough\"], \n                                     data[\"descriptor\"], \n                                     margins=True)\n\n# produce a contingency table for calculations\ncontingency_table = pd.crosstab(data[\"borough\"], \n                                     data[\"descriptor\"], \n                                     margins=False)\n\ncontingency_table_view\n\n\n\n\n\n\n\n\ndescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\nAll\n\n\nborough\n\n\n\n\n\n\n\n\n\n\nBRONX\n2395\n1381\n7745\n8\n1300\n12829\n\n\nBROOKLYN\n5332\n2075\n20323\n12\n3054\n30796\n\n\nMANHATTAN\n2818\n1758\n14036\n2\n4027\n22641\n\n\nQUEENS\n3105\n1219\n8449\n4\n1201\n13978\n\n\nSTATEN ISLAND\n795\n184\n1391\n0\n247\n2617\n\n\nAll\n14445\n6617\n51944\n26\n9829\n82861\n\n\n\n\n\n\n\n\nNow that we have constructed the contingency table, we are ready to begin conducting the signficance tests (for independence of Borough and Descriptor). This requires that we compute the chi-squared statistic.\nThere are multiple steps to computing the chi-squared statistic for this test, but the general test-statistic is computed as follows:\n\\[\\chi_{rows-1 * cols-1}^{2} = \\sum_{cells} \\frac{(O - E)^{2}}{E}\\]\nHere, \\(E = \\text{row sum} * \\text{col sum}/N\\) stands for the expected value of each cell, and \\(O\\) refers to the observed values. Note that \\(N\\) refers to the total observations (the right and lower-most cell value in the contingency table above)\nFirst, let’s calculate the expected values. This can be accomplished by performing the outer product of row sums and column sums for the contingency table:\n\\[\n\\begin{align}\n\\text{row\\_margins} = \\langle r_{1}, r_{2}, \\dots, r_{n}\\rangle \\\\\n\\text{col\\_margins} = \\langle c_{1}, c_{2}, \\dots, c_{m}\\rangle \\\\\n\\text{row\\_margins} \\otimes \\text{col\\_margins} = \\left[\\begin{array}{cccc}\n    r_{1}c_{1} & r_{1}c_{2} & \\dots & r_{1}c_{m} \\\\\n    r_{2}c_{1} & r_{2}c_{2} & \\dots & r_{2}c_{m} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    r_{n}c_{1} & r_{n}c_{2} & \\dots & r_{n}c_{m}\n\\end{array}\\right]\n\\end{align}\n\\]\nIn python this is calculated as:\n\nrow_margins = contingency_table_view[\"All\"]\ncol_margins = contingency_table_view.T[\"All\"]\ntotal = contingency_table_view[\"All\"][\"All\"]\n\nexpected = np.outer(row_margins, col_margins)/total\npd.DataFrame(expected, columns=contingency_table_view.columns).set_index(\n    contingency_table_view.index\n)\n\n\n\n\n\n\n\n\ndescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\nAll\n\n\nborough\n\n\n\n\n\n\n\n\n\n\nBRONX\n2236.455087\n1024.480672\n8042.258433\n4.025464\n1521.780343\n12829.0\n\n\nBROOKLYN\n5368.607910\n2459.264696\n19305.432278\n9.663123\n3653.031993\n30796.0\n\n\nMANHATTAN\n3946.962322\n1808.033900\n14193.216399\n7.104259\n2685.683120\n22641.0\n\n\nQUEENS\n2436.758065\n1116.235937\n8762.544888\n4.385996\n1658.075114\n13978.0\n\n\nSTATEN ISLAND\n456.216616\n208.984794\n1640.548002\n0.821158\n310.429430\n2617.0\n\n\nAll\n14445.000000\n6617.000000\n51944.000000\n26.000000\n9829.000000\n82861.0\n\n\n\n\n\n\n\n\nThe chi-squared statistic can be calculated directly from the (component-wise) squared difference between the original contingency table and the expected values presented above divided by the total number of observations. However, we can also use the scipy.stats package to perform the contingency test automatically.\nBefore performing this test, let us also examine the relavent hypotheses to this significance test.\n\\[\n\\begin{align}\n& H_{0}: \\text{Rodent complaint type reported and Borough are independent} \\\\\n& H_{1}: H_{0} \\text{ is false.}\n\\end{align}\n\\]\nWe assume a significance level of \\(\\alpha=0.05\\) for this test:\n\nNOTE: the contingency table without row margins is used for calculating the chi-squared test.\n\n\nfrom scipy.stats import chi2_contingency\n\nchi2_val, p, dof, expected = chi2_contingency(contingency_table)\n\npd.Series({\n    \"Chi-Squared Statistic\": chi2_val, \n    \"P-value\": p, \n    \"degrees of freedom\": dof\n})\n\nChi-Squared Statistic    2031.14984\nP-value                     0.00000\ndegrees of freedom         16.00000\ndtype: float64\n\n\nNow we can create a plot to demonstrate the location of the chi-squared statistic with respect to the chi-squared distribution\n\nx = np.arange(0, 45, 0.001)\n# x2 = np.arange(59, 60, 0.001)\n\nplt.plot(x, stats.chi2.pdf(x, df=20), label=\"df: 20\", color=\"red\")\n# plt.fill_between(x, x**4, color=\"red\", alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n&lt;!–\n\nfrom scipy.stats import chi2\n\nmax_chi_val = 59.0\nx_range = np.arange(0, 60, 0.001)\nfig = px.histogram(x=x_range, \n                   y=chi2.pdf(x_range, df=dof), \n                   labels={\"x\":\"Chi-Squared Value\", \n                           \"y\":\"Density\"}, \n                   title=\"Chi-Squared Distribution (df = {})\".format(dof))\n# create a a scatter plot of values from chi2 to chi2 (a single point)\n# and going from 0 to the y value at the critical point - a vertical\n# line\nfig.add_trace(go.Scatter(x=[max_chi_val, max_chi_val],\n                         y=[0,chi2.pdf(max_chi_val, df=dof)], \n              mode=\"lines\", \n              name=\"Critical Value\", \n              line=dict(color=\"red\", dash=\"dash\")))\nfig.update_layout(shapes=[dict(type=\"rect\", \n                               x0=max_chi_val, \n                               x1=20, \n                               y0=0,\n                               y1=chi2.pdf(max_chi_val, df=dof), \n                          fillcolor=\"rgba(0, 100, 80, 0.2)\", \n                          line=dict(width=0))], \n                  annotations=[dict(x=max_chi_val + 0.5, \n                                    y=0.02, \n                                    text=\"Area of Interest\", \n                                    showarrow=False, \n                                    font=dict(size=10, color=\"black\"))])\nfig.show()\n\n                                                \n\n\nAs you can see from the figure, the critical value we obtain (2034) is exceptionally far beyond the bounds of the distribution, that there must be a significant dependence relationship between the borough and the rodent incident type which is reported.\nMoreover, the p-value returned for this test is 0.00000, meaning that there is virtually 0 probability that such observations would be made given that the borough and rodent incident type reported were independent.\n\n\n7.1.6 Sources\n\ntowardsdatascience.com - Exploratory data analysis\nscribbr.com - Descriptive statistics\nscribbr.com - Probability Distributions\nmathisfun.com - Median definition\nstats.libretexts - outliers and sample quantiles\ndatagy.io - calculating IQR in python\ncurtin university - descriptive statistics for categorical data\ndwstockburger.com - hypothesis testing with contingency tables\naskpython.com - chi-squared testing in python\nsphweb - Hypotheses for chi-squared tests",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#hypothesis-testing-with-scipy.stats",
    "href": "eda.html#hypothesis-testing-with-scipy.stats",
    "title": "7  Exploratory Data Analysis",
    "section": "7.2 Hypothesis Testing with scipy.stats",
    "text": "7.2 Hypothesis Testing with scipy.stats\nThis section was written by Isabelle Perez.\n\n7.2.1 Introduction\nHello! My name is Isabelle Perez and I am a junior Mathematics-Statistics major and Computer Science minor. I am interested in learning about data science topics and have an interest in how statistics can improve sports analytics, specifically in baseball. Today’s topic is the scipy.stats package and the many hypothesis tests that we can perform using it.\n\n\n7.2.2 Scipy.stats\nThe package scipy.stats is a subpackage of Scipy and contains many methods useful for statistics such as probability distributions, summary statistics, statistical tests, etc. The focus of this presentation will be on some of the many hypothesis tests that can be easily conducted using scipy.stats and will provide examples of situations in which they may be useful.\nFirstly, ensure Scipy is installed by using pip install scipy or\nconda install -c anaconda scipy.\nTo import the package, use the command import scipy.stats.\n\n\n7.2.3 Basic Statistical Hypothesis Tests\n\n7.2.3.1 Two-sample t-test\nH0: \\(\\mu_1 = \\mu_2\\)\nH1: \\(\\mu_1 \\neq\\) or \\(&gt;\\) or \\(&lt;\\) \\(\\mu_2\\)\n\n\nCode: scipy.stats.ttest_ind(sample_1, sample_2)\nAssumptions: Observations are independent and identically distributed (i.i.d), normally distributed, and the two samples have equal variances.\nOptional Parameters:\n\nnan_policy can be set to propagate (return nan), raise (raise ValueError),\nor omit (ignore null values).\nalternative can be two-sided (default), less, or greater.\nequal_var is a boolean representing whether the variances of the two samples are equal\n(default is True).\naxis defines the axis along which the statistic should be computed (default is 0).\n\nReturns: The t-statisic, a corresponding p-value, and the degrees of freedom.\n\n\n7.2.3.2 Paired t-test\nH0: \\(\\mu_1 = \\mu_2\\)\nH1: \\(\\mu_1 \\neq\\) or \\(&gt;\\) or \\(&lt;\\) \\(\\mu_2\\)\n\n\nCode: scipy.stats.ttest_rel(sample_1, sample_2)\nAssumptions: Observations are i.i.d, normally distributed, and related, and the two samples have equal variances. The input arrays must also be of the same size since the observations are paired.\nOptional Parameters: Can use nan_policy or alternative.\nReturns: The t-statisic, a corresponding p-value, and the degrees of freedom. Also has a method called confidence_interval with input parameter confidence_level that returns a tuple with the confidence interval around the difference in population means of the two samples.\n\n\n7.2.3.3 ANOVA\nH0: \\(\\mu_1 = \\mu_2 = ... = \\mu_n\\)\nH1: at least two \\(\\mu_i\\) are not equal\n\n\nCode: scipy.stats.f_oneway(sample_1, sample_2, ..., sample_n)\nAssumptions: Samples are i.i.d., normally distributed, and the samples have equal variances.\nErrors:\n\nRaises ConstantInputWarning if all values in each of the inputs are identical.\nRaises DegenerateDataWarning if any input has length \\(0\\) or all inputs have length \\(1\\).\n\nReturns: The F-statistic and a corresponding p-value.\n\n\n7.2.3.4 Example: Comparison of Mean Response Times by Borough\nLooking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests,\nthere are many ways a two-sample t-test may be useful. For example, we can consider samples drawn from different boroughs and perform this hypothesis test to identify whether their mean response times differ. If so, this may suggest that some boroughs are being underserviced.\n\nimport pandas as pd \nimport numpy as np \nimport scipy.stats   \n\n# read in file \ndf = pd.read_csv('data/rodent_2022-2023.csv')  \n\n# data cleaning - change dates to timestamp object\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\ndf['Closed Date'] = pd.to_datetime(df['Closed Date'])\n\n# add column Response Time \ndf['Response Time'] = df['Closed Date'] - df['Created Date']\n\n# convert data to total seconds\ndf['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600)    \n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_21950/2664048187.py:9: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_21950/2664048187.py:10: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\nSince the two-sample t-test assumes the data is drawn from a normal distribution,\nwe need to ensure the samples we are comparing are normally distributed. According to the Central Limit theorem, the distribution of sample means from repeated samples of a population will be roughly normal. Therefore, we can take 100 samples of each borough’s response times, measure the mean of each sample, and perform the hypothesis test on the arrays of sample means.\n\nimport matplotlib.pyplot as plt \n\n# select Bronx and Queens boroughs \ndf_mhtn = df[df['Borough'] == 'MANHATTAN']['Response Time'] \ndf_queens = df[df['Borough'] == 'QUEENS']['Response Time']  \n\nmhtn_means = []\nqueens_means = []\n\n# create samples of sampling means \nfor i in range(100): \n  sample1 = df_mhtn.sample(1000, replace = True)\n  mhtn_means.append(sample1.mean())\n\n  sample2 = df_queens.sample(1000, replace = True) \n  queens_means.append(sample2.mean())  \n\n# plot distribution of sample means for Manhattan\nplt.hist(mhtn_means)\nplt.xlabel('Mean Response Times for Manhattan')\nplt.ylabel('Value Counts')\nplt.show()\n\n# plot distribution of sample means for Queens \nplt.hist(queens_means) \nplt.xlabel('Mean Response Times for Queens')\nplt.ylabel('Value Counts')\nplt.show() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe also need to check if the variances of the two samples are equal.\n\n# convert to numpy array \nmhtn_means = np.array(mhtn_means)\nqueens_means = np.array(queens_means)\n\nprint('Mean, variance for Manhattan', (mhtn_means.mean(), mhtn_means.std() ** 2))\nprint('Mean, variance for Queens:', (queens_means.mean(), queens_means.std() ** 2))\n\nMean, variance for Manhattan (42.51382936447074, 29.263089059077576)\nMean, variance for Queens: (70.31393657103288, 43.576834811630626)\n\n\nSince the ratio of the variances is less than \\(2\\), it is safe to assume equal variances.\n\nresult_ttest = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True)\n\nprint('t-statistic:', result_ttest.statistic)\nprint('p-value:', result_ttest.pvalue) \n# print('degrees of freedom:', result_ttest.df) \n\nt-statistic: -32.41002204216531\np-value: 4.174970235673649e-81\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that there is a statistically significant difference in the mean of sample means drawn from rodent sighting response times for Manhattan compared to Queens.\n\nresult_ttest2 = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True, \n                                                            alternative = 'less') \n\nprint('t-statistic:', result_ttest2.statistic)\nprint('p-value:', result_ttest2.pvalue) \n# print('degrees of freedom:', result_ttest2.df) \n\nt-statistic: -32.41002204216531\np-value: 2.0874851178368245e-81\n\n\nWe can also set the alternative equal to less to test if the mean of sample means drawn from the Manhattan response times is less than that of sample means drawn from Queens response times. At the alpha level of \\(0.05\\), we can also reject this null hypothesis and conclude that the mean of sample means is less for Manhattan than it is for Queens.\n\n\n\n7.2.4 Normality\n\n7.2.4.1 Shapiro-Wilk Test\nH0: data is drawn from a normal distribution\nH1: data is not drawn from a normal distribution\n\n\nCode: scipy.stats.shapiro(sample)\nAssumptions: Observations are i.i.d.\nReturns: The test statistic and corresponding p-value.\n\nMore appropriate for smaller sample sizes (\\(&lt;50\\)).\nThe closer the test statistic is to \\(1\\), the closer it is to a normal distribution, with \\(1\\) being a perfect match.\n\n\n\n7.2.4.2 NormalTest\nH0: data is drawn from a normal distribution\nH1: data is not drawn from a normal distribution\n\n\nCode: scipy.stats.normaltest(sample)\nAssumptions: Observations are i.i.d.\nOptional Parameters: Can use nan_policy.\nReturns: The test-statistic \\(s^2 + k^2\\), where \\(s^2\\) is from the skewtest and \\(k\\) is from the kurtosistest, and a corresponding p-value\nThis test is based on D’Agostino and Pearson’s test which combines skew and kurtosis (heaviness of the tail or how much data resides in the tails). The test compares the skewness and kurtosis of the sample to that of a normal distribution, which are \\(0\\) and \\(3\\), respectively.\n\n\n7.2.4.3 Example: Distribution of Response Times\nIt can be useful to identify the distribution of a population because it gives us the ability to summarize the data more efficiently. We can identify whether or not the distribution of a sample of response times\nfrom the rodent sighting dataset is normal by conducting a normality test using scipy.stats.\n\n# take a sample from Response Time column \nresp_time_samp = df['Response Time'].sample(10000, random_state = 0)  \n\nresults_norm = scipy.stats.normaltest(resp_time_samp, nan_policy = 'propagate')\n\nprint('test statistic:', results_norm.statistic) \nprint('p-value:', results_norm.pvalue)\n\ntest statistic: nan\np-value: nan\n\n\nBecause there are null values in the sample data, if we set the nan_policy to propagate, both the test statistic and p-value will return as nan. If we still want to obtain results when there is missing data, we must set the nan_policy to omit.\n\nresults_norm2 = scipy.stats.normaltest(resp_time_samp, nan_policy = 'omit') \n\nprint('test statistic:', results_norm2.statistic) \nprint('p-value:', results_norm2.pvalue)\n\ntest statistic: 12530.153548983568\np-value: 0.0\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that the data is not drawn from a normal distribution. We can further show this by plotting the data in a histogram.\n\nbins = [i for i in range(int(resp_time_samp.min()), int(resp_time_samp.max()), 300)]\n\nplt.hist(resp_time_samp, bins = bins)\nplt.xlabel('Response Times')\nplt.ylabel('Value Counts')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n7.2.5 Correlation\n\n7.2.5.1 Pearson’s Correlation\nH0: the correlations is \\(0\\)\nH1: the correlations is \\(\\neq\\), \\(&lt;\\), or \\(&gt; 0\\)\n\n\nCode: scipy.stats.pearsonr(sample_1, sample_2)\nAssumptions: Observations are i.i.d, normally distributed, and the two samples have equal variances.\nOptional Parameters: Can use alternative.\nErrors:\n\nRaises ConstantInputWarning if either input has all constant values.\nRaises NearConstantInputWarning if np.linalg.norm(x - mean(x)) &lt; 1e-13 * np.abs(mean(x)).\n\nReturns: The correlation coefficient and a corresponding p-value. It also has the confidence_interval method.\n\n\n7.2.5.2 Chi-Squared Test\nH0: the two variables are independent of one another\nH1: a dependency exists between the two variables\n\n\nCode: scipy.stats.chi2_contingency(table)\nAssumptions: The cells in the table contain frequencies, the levels of each variable are mutually exclusive, and observations are independent. [2]\nReturns: The test statistic, a corresponding p-value, the degrees of freedom, and an array of expected frequencies from the table.\n\ndof = table.size - sum(table.shape) + table.ndim - 1\n\n\n\n7.2.5.3 Example: Analyzing the Relationship Between Season and Response Time\nOne way in which the chi-squared test may prove useful with the 2022-2023 311 Service Request rodent sighting data is by allowing us to identify dependencies between different variables, categorical ones in particular. For example, we can choose a borough and test whether the season in which the request was created is independent of the type of sighting, using the Descriptor column.\n\n# return season based off month of created date\ndef get_season(month): \n  if month in [3, 4, 5]:\n    return 'Spring'\n  elif month in [6, 7, 8]: \n    return 'Summer'\n  elif month in [9, 10, 11]: \n    return 'Fall'\n  elif month in [12, 1, 2]:\n    return 'Winter' \n\n# add column for season \ndf['Season'] = df['Created Date'].dt.month.apply(get_season)\n\n# create df for Brooklyn \ndf_brklyn = df[df['Borough'] == 'BROOKLYN'] \n\nfreq_table_2 = pd.crosstab(df_brklyn.Season, df_brklyn.Descriptor) \n\nfreq_table_2\n\n\n\n\n\n\n\n\nDescriptor\nCondition Attracting Rodents\nMouse Sighting\nRat Sighting\nRodent Bite - PCS Only\nSigns of Rodents\n\n\nSeason\n\n\n\n\n\n\n\n\n\nFall\n1282\n464\n4653\n0\n710\n\n\nSpring\n1395\n579\n5396\n4\n850\n\n\nSummer\n1732\n509\n6631\n5\n881\n\n\nWinter\n923\n523\n3643\n3\n613\n\n\n\n\n\n\n\n\n\nresults_chi2 = scipy.stats.chi2_contingency(freq_table_2)\n\nprint('test statistic:', results_chi2.statistic)\nprint('p-value:', results_chi2.pvalue)\nprint('degrees of freedom:', results_chi2.dof) \n\ntest statistic: 120.07130925971065\np-value: 5.981181861531126e-20\ndegrees of freedom: 12\n\n\nAt an alpha level of \\(0.05\\), the p-value allows us to reject the null hypothesis and conclude that the Season and Descriptor columns are indeed dependent for Brooklyn. This can also be confirmed by plotting the descriptor frequencies in a stacked bar chart, where the four seasons represent different colored bars.\n\nx_labels = ['Condition', 'M.Sighting', 'R.Sighting', 'Bite', 'Signs of Rodents']\n\nfreq_table_2.rename(columns = {'Condition Attracting Rodents': 'Condition', \n                      'Rat Sighting': 'R.Sighting', 'Mouse Sighting': 'M.Sighting', \n                      'Rodent Bite - PCS Only': 'Bite'},\n                      inplace = True)\n\nfreq_table_2.T.plot(kind = 'bar', stacked = True) \n\n\n\n\n\n\n\n\nThe bar chart above shows that the ranking of each season by number of rodent sightings is consistent across all five types of rodent sightings. This further suggests that there exists dependency between season and rodent sighting in Brooklyn.\n\n\n\n7.2.6 Nonparametric Hypothesis Tests\n\n7.2.6.1 Mann-Whitney U (Wilcoxon Rank-Sum) Test\nH0: distribution of population 1 \\(=\\) distribution of population 2\nH1: distribution of population 1 \\(\\neq\\) or \\(&gt;\\) or \\(&lt;\\) distribution of population 2\n\n\nCode: scipy.stats.mannwhitneyu(sample_1, sample_2)\nAssumptions: Observations are independent and ordinal.\nOptional Parameters:\n\n\nalternative can allow us to test if one sample has a distribution that is stochastically less than or greater than that of the second sample.\n\nCan use nan_policy.\nmethod selects how the p-value is calculated and can be set to asymptotic, exact, or auto.\n\nasymptotic corrects for ties and compares the standardized test statistic to the normal distribution.\nexact does not correct for ties and computes the exact p-value.\nauto (default) chooses exact when there are no ties and the size of one sample is \\(&lt;=8\\), asymptotic otherwise.\n\n\n\nReturns: The Mann-Whitney U Statistic corresponding with the first sample and a corresponding p-value.\n\n\nThe statistic corresponding to the second sample is not returned but can be calculated as sample_1.shape * sample_2.shape - U1 where U1 is the test statistic associated with sample_1.\nFor large sample sizes, the distribution can be assumed to be approximately normal, so the statisic can be measured as \\(z = \\frac{U-\\mu_{U}}{\\sigma_{U}}\\).\nTo adjust for ties, the standard deviation is calculated as follows:\n\n\\(\\sigma_{U} = \\sqrt{\\frac{n_{1}n_{2}}{12}((n + 1) -\n\\frac{\\sum_{k = 1}^{K}(t^{3}_{k} - t_{k})}{n(n - 1)})}\\), where \\(t_{k}\\) is the number of ties.\n\nNon-parametric version of the two-sample t-test.\nIf the underlying distributions have similar shapes, the test is essentially a comparison of medians. [5]\n\n\n\n\n7.2.6.2 Wilcoxon Signed-Rank test\nH0: distribution of population 1 \\(=\\) distribution of population 2\nH1: distribution of population 1 \\(\\neq\\) or \\(&gt;\\) or \\(&lt;\\) distribution of population 2\n\n\nCode: scipy.stats.wilcoxon(sample_1, sample_2) or\nscipy.statss.wilcoxon(sample_diff, None)\nAssumptions: Observations are independent, ordinal, and the samples are paired.\nOptional Parameters:\n\nzero-method chooses how to handle pairs with the same value.\n\nwilcox (default) doesn’t include these pairs.\npratt drops ranks of pairs whose difference is \\(0\\).\nzsplit includes pairs and assigns half the ranks into the positive group and the other half in the negative group.\n\nCan use alternative and nan_policy.\nalternative allows us to identify whether the distribution of the difference is stochastically greater than or less than a distribution symmetric about \\(0\\).\nmethod selects how the p-value is calculated.\n\nexact computes the exact p-value.\napprox finds the p-value by standardizing the test statistic.\nauto (default) chooses exact when the sizes of the samples are \\(&lt;=50\\) and approx otherwise.\n\n\nReturns: The test statistic, a corresponding p-value, and the calculated z-score when the method is approx.\n\nNon-parametric version of the paired t-test.\n\n\n\n7.2.6.3 Kruskal-Wallis H-Test\nH0: all populations have the same distribution\nH1: \\(&gt;=2\\) populations are distributed differently\n\n\nCode: scipy.stats.kruskal(sample_1, ..., sample_n)\nAssumptions: Observations are independent, ordinal, and each sample has \\(&gt;=5\\) observations. [3]\nOptional Parameters: Can use nan_policy.\nReturns: The Kruskal-Wallis H-statisic (corrected for ties) and a corresponding p-value.\n\nNon-parametric version of ANOVA.\n\n\n\n7.2.6.4 Example: Distribution of Response Times for 2022 vs. 2023\nWe can use the Mann-Whitney test to compare the distributions of response times from our rodent data. For example, we can split the data into two groups, one for 2022 and the other for 2023, to compare their distributions.\n\n# create dfs for 2022 and 2023\ndf_2022 = df[df['Created Date'].dt.year == 2022]['Response Time'] \ndf_2023 = df[df['Created Date'].dt.year == 2023]['Response Time']\n\n# perform test with H_0 df_2022 &gt; df_2023\nresults_mw = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n                                                      alternative = 'greater')\n\n# perform test with H_0 df_2022 &lt; df_2023\nresults_mw2 = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n                                                        alternative = 'less')\n\nprint('test statistic:', results_mw.statistic)\nprint('p-value:', results_mw.pvalue)\nprint()\nprint('test statistic:', results_mw2.statistic)\nprint('p-value:', results_mw2.pvalue)\n\ntest statistic: 744300262.0\np-value: 1.0\n\ntest statistic: 744300262.0\np-value: 3.931744717049885e-98\n\n\nAt an alpha level of \\(0.05\\), the p-value of \\(1\\) is too large to reject the null hypothesis, therefore we cannot conclude that the distribution of response times for 2022 is stochastically greater than that for 2023. But when we set the alternative to less, our p-value is small enough to conclude that the distribution of response times for 2022 is stochastically greater than the distribution of response times for 2023.\n\nbins = [i for i in range(5, 500, 50)]\nplt.hist(df_2022, label = 2022, bins = bins, color = 'red') \nplt.hist(df_2023, label = 2023, bins = bins, color = 'blue', \n                                                  alpha = 0.5)\n\nplt.legend()\nplt.show()  \n\n\n\n\n\n\n\n\nThis small subset of data confirms the results of the one-sided hypothesis test, showing that in general, the counts of response times for 2022 are greater than those for 2023, suggesting the distribution for 2022 is stochastically larger than that of 2023 data.\n\n\n7.2.6.5 Example: Distribution of Response Times by Season\nSimilar to the previous, example, we can use a non-parametric test to compare the distribution of response times by season. Because in this case we have four samples to compare, we need to use the Kruskal Wallis H-Test.\n\ndf_summer = df[df['Season'] == 'Summer']['Response Time']\ndf_spring = df[df['Season'] == 'Spring']['Response Time']\ndf_fall = df[df['Season'] == 'Fall']['Response Time']\ndf_winter = df[df['Season'] == 'Winter']['Response Time']\n\nresults_kw = scipy.stats.kruskal(df_summer, df_spring, df_fall, df_winter,\n                                                                nan_policy = 'omit')\n\nprint('test statistic:', results_kw.statistic) \nprint('p-value:', results_kw.pvalue)\n\ntest statistic: 7.626073129122622\np-value: 0.05440606295505631\n\n\nAt an alpha level of \\(0.05\\), the p-value of \\(0.0496\\) is just small enough to reject the null hypothesis, suggesting that the distribution of response times differs by season, but not by much.\n\n\n\n7.2.7 References\n\nhttps://docs.scipy.org/doc/scipy/reference/stats.html (scipy.stats documentation)\nhttps://libguides.library.kent.edu/SPSS/ChiSquare\nhttps://library.virginia.edu/data/articles/getting-started-with-the-kruskal-wallis-test\nhttps://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/\nhttps://library.virginia.edu/data/articles/the-wilcoxon-rank-sum-test",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#exploring-nyc-rodent-dataset-by-xingye.zhang",
    "href": "eda.html#exploring-nyc-rodent-dataset-by-xingye.zhang",
    "title": "7  Exploratory Data Analysis",
    "section": "7.3 Exploring NYC Rodent Dataset (by Xingye.Zhang)",
    "text": "7.3 Exploring NYC Rodent Dataset (by Xingye.Zhang)\nThe main goal of my presentation is to show the process of ‘transforming raw dataset’ into ‘compelling insights’ using various data visualizing examples. And most importantly, I wish to get you guys ‘engaged’ and ‘come up with your insights’ about visualizing NYC dataset throughout the process of exploring.\n\n7.3.1 Personal Introduction\nMy name is Xingye Zhang, you can call me Austin, which may be easier to pronounce. I’m from China and currently a senior majoring in Statistics and Economics. I plan to graduate next semester, having taken a gap semester previously.\nMy experience with Python is quite recent. I had my first Python course in ECON prior to this course and I just started to learn how to use Github and Vs code in this semester.\nPlease feel free to interrupt if you have any questions or notice I made a mistake. I’m glad to answer your questions and learn from you guys!\n\n\n7.3.2 Dataset Format Selection\nWhy ‘Feather’?\n\nSpeed: Feather files are faster to read and write than CSV files.\nEfficiency in Storage: Feather files are often smaller in size than CSV files.\nSupport for Large Datasets: Feather files can handle large datasets more efficiently.\n\n\n\n7.3.3 Dataset Cleaning\n\n# Import basic packages\nimport pandas as pd\n# Pyarrow is better for reading feather file\nimport pyarrow.feather as pya\n\n# Load the original dataset\nrodent_data = pya.read_feather('data/rodent_2022-2023.feather')\n\n# Print columns in order to avoid 'Keyerror'\ncolumn_names = rodent_data.columns.tolist()\nprint(column_names)\n\n['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name', 'complaint_type', 'descriptor', 'location_type', 'incident_zip', 'incident_address', 'street_name', 'cross_street_1', 'cross_street_2', 'intersection_street_1', 'intersection_street_2', 'address_type', 'city', 'landmark', 'facility_type', 'status', 'due_date', 'resolution_description', 'resolution_action_updated_date', 'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)', 'y_coordinate_(state_plane)', 'open_data_channel_type', 'park_facility_name', 'park_borough', 'vehicle_type', 'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment', 'latitude', 'longitude', 'location', 'zip_codes', 'community_districts', 'borough_boundaries', 'city_council_districts', 'police_precincts', 'police_precinct']\n\n\n1. Checking Columns\nConclusion:: There are no columns with identical data, but some columns are highly correlated.\nEmpty Columns: ‘Facility Type’, ‘Due Date’, ‘Vehicle Type’, ‘Taxi Company Borough’,‘Taxi Pick Up Location’, ‘Bridge Highway Name’, ‘Bridge Highway Direction’, ‘Road Ramp’, ‘Bridge Highway Segment’.\nColumns we can remove to clean data: ‘Agency Name’, ‘Street Name’, ‘Landmark’, ‘Intersection Street 1’, ‘Intersection Street 2’, ‘Park Facility Name’, ‘Park Borough’, ‘Police Precinct’, ‘Facility Type’, ‘Due Date’, ‘Vehicle Type’, ‘Taxi Company Borough’, ‘Taxi Pick Up Location’, ‘Bridge Highway Name’, ‘Bridge Highway Direction’, ‘Road Ramp’, ‘Bridge Highway Segment’.\n2. Using reverse geocoding to fill the missing zip code\n\n# Find the missing zip code\nmissing_zip = rodent_data['zip_codes'].isnull()\nmissing_borough = rodent_data['borough'].isnull()\nmissing_zip_borough_correlation = (missing_zip == missing_borough).all()\n\n# Use reverse geocoding to fill the missing zip code\ngeocode_available = not (rodent_data['latitude'].isnull().any() \n                    or rodent_data['longitude'].isnull().any())\n\nmissing_zip_borough_correlation, geocode_available\n\n(False, False)\n\n\n3. Clean the Original Data\n\n# Removing redundant columns\ncolumns_to_remove = ['agency_name', 'street_name', 'landmark',\n                     'intersection_street_1', 'intersection_street_2',\n                     'park_facility_name', 'park_borough',\n                     'police_precinct', 'facility_type', 'due_date',\n                     'vehicle_type', 'taxi_company_borough', \n                     'taxi_pick_up_location', 'police_precinct',\n                     'bridge_highway_name', 'bridge_highway_direction', \n                     'road_ramp','bridge_highway_segment']\n\ncleaned_data = rodent_data.drop(columns=columns_to_remove)\n\n#Create the file_path\nfile_path = 'data/cleaned_rodent_data.feather'\n\n# Feather Export (removing non-supported types like datetime)\ncleaned_data['created_date'] = cleaned_data['created_date'].astype(str)\ncleaned_data['closed_date'] = cleaned_data['closed_date'].astype(str)\ncleaned_data.to_feather(file_path)\n\n# Check the cleaned columns\nprint(cleaned_data.columns)\n\nIndex(['unique_key', 'created_date', 'closed_date', 'agency', 'complaint_type',\n       'descriptor', 'location_type', 'incident_zip', 'incident_address',\n       'cross_street_1', 'cross_street_2', 'address_type', 'city', 'status',\n       'resolution_description', 'resolution_action_updated_date',\n       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',\n       'y_coordinate_(state_plane)', 'open_data_channel_type', 'latitude',\n       'longitude', 'location', 'zip_codes', 'community_districts',\n       'borough_boundaries', 'city_council_districts', 'police_precincts'],\n      dtype='object')\n\n\n\n\n7.3.4 Categorizing the Columns\nHighly suggest to use ‘Chatgpt’ first and then revise it yourself.\n\nIdentification Information: ‘Unique Key’.\nTemporal Information: ‘Created Date’, ‘Closed Date’.\nAgency Information: ‘Agency’.\nComplaint Details: ‘Complaint Type’, ‘Descriptor’, ‘Resolution Description’, ‘Resolution Action Updated Date’.\nLocation and Administrative Information: ‘Location Type’, ‘Incident Zip’, ‘Incident Address’, ‘Cross Street 1’, ‘Cross Street 2’, ‘City’,‘Borough’, ‘Community Board’, ‘Community Districts’, ‘Borough Boundaries’, ‘BBL’. ‘City Council Districts’, ‘Police Precincts’.\nGeographical Coordinates: ‘X Coordinate (State Plane)’, ‘Y Coordinate (State Plane)’, ‘Location’.\nCommunication Channels: ‘Open Data Channel Type’.\n\n\n\n7.3.5 Question based on Dataset\nAgency: 1. Temporal Trends in Rodent Complaints 2. Relationship between Rodent Complaints Location Types 3. Spatial Analysis of Rodent Complaints\nComplainer: 1. Agency Resolution Time 2. Impact of Rodent Complaints on City Services:\n\n\n7.3.6 Temporal Trends in Rodent Complaints\n\n# Import basic packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Ensure 'created_date' is in datetime format and extract 'Year' and 'Month'\ncleaned_data['created_date'] = pd.to_datetime(cleaned_data['created_date'], \nerrors='coerce')\ncleaned_data['Year'] = cleaned_data['created_date'].dt.year\ncleaned_data['Month'] = cleaned_data['created_date'].dt.month\n\n# Use data from year 2023 as example\ndata_2023 = cleaned_data[cleaned_data['Year'] == 2023]\n\n# Group by Month to get the count of complaints\nmon_complaints_23= data_2023.groupby('Month').size().reset_index(name='Counts')\n\n# Plotting\nplt.figure(figsize=(7, 3))\nsns.barplot(data=mon_complaints_23, x='Month', y='Counts')\nplt.title('Monthly Rodent Complaints in 2023')\nplt.xlabel('Month')\nplt.ylabel('Number of Complaints')\nplt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n                       'Sep', 'Oct', 'Nov', 'Dec'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\nSeasonal Trend\n\n# Categorize month into seasons\ndef categorize_season(month):\n    if month in [3, 4, 5]:\n        return 'Spring'\n    elif month in [6, 7, 8]:\n        return 'Summer'\n    elif month in [9, 10, 11]:\n        return 'Fall'\n    else:  # Months 12, 1, 2\n        return 'Winter'\n\n# Applying the function to create a 'Season' column\ndata_2023 = cleaned_data[cleaned_data['Year'] == 2023].copy()\ndata_2023['Season'] = data_2023['Month'].apply(categorize_season)\n\n# Grouping by Season to get the count of complaints\nseason_com_2023 = data_2023.groupby('Season').size().reset_index(name='Counts')\n\n# Ordering the seasons for the plot\nseason_order = ['Spring', 'Summer', 'Fall', 'Winter']\nseason_com_2023['Season'] = pd.Categorical(season_com_2023['Season'],\n                            categories=season_order, ordered=True)\nseason_com_2023 = season_com_2023.sort_values('Season')\n\n# Plotting\nplt.figure(figsize=(7, 3))\nsns.barplot(data=season_com_2023, x='Season', y='Counts')\nplt.title('Seasonal Rodent Complaints in 2023')\nplt.xlabel('Season')\nplt.ylabel('Number of Complaints')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nComparing 2022 and 2023 Seasonal Trend\n\n# Filter data for two specific years, e.g., 2022 and 2023\ndata_filtered = cleaned_data[cleaned_data['Year'].isin([2022, 2023])]\n\n# Group by Year and Month to get the count of complaints\nmon_counts = data_filtered.groupby(['Year', \n'Month']).size().reset_index(name='Counts')\n\n# Pivot the data for easy plotting\nmon_counts_pivot = mon_counts.pivot(index='Month', columns='Year', \n                   values='Counts')\n\n# Plotting\nplt.figure(figsize=(7, 3))\nsns.lineplot(data=mon_counts_pivot)\nplt.title('Comparison of Monthly Rodent Complaints between 2022 and 2023')\nplt.xlabel('Month')\nplt.ylabel('Number of Complaints')\nplt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \n                          'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nplt.legend(title='Year', labels=mon_counts_pivot.columns)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nComparing Temporal Trends from Boroughs in 2023\n\ndata_2023 = cleaned_data[cleaned_data['Year'] == 2023]\n\n# Group by Month and Borough to get the count of complaints\nmon_borough_counts = data_2023.groupby(['Month',\n'borough']).size().reset_index(name='Counts')\n\n# Pivot the data for easy plotting\nmon_borough_counts_pivot = mon_borough_counts.pivot(index='Month', \n                           columns='borough', values='Counts')\n\n# Plotting\nplt.figure(figsize=(7, 3))\nsns.lineplot(data=mon_borough_counts_pivot)\nplt.title('Monthly Trend of Rodent Complaints by Borough in 2023')\nplt.xlabel('Month')\nplt.ylabel('Number of Complaints')\nplt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \n                          'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nplt.legend(title='Borough')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAdding the Location Types\n\nimport warnings\nfrom plotnine.exceptions import PlotnineWarning\nfrom plotnine import (\n    ggplot, aes, geom_line, geom_point, facet_wrap,\n    labs, theme, element_text, scale_x_continuous\n)\n\n# Suppress specific Plotnine warnings\nwarnings.filterwarnings('ignore', category=PlotnineWarning)\n\n# get the count of complaints per month per location type and borough\nmonthly_data = (data_2023.groupby(['borough', 'location_type', 'Month'])\n                               .size()\n                               .reset_index(name='Counts'))\n\n# Create the plot with adjusted figure size and legend properties\nplot = (ggplot(monthly_data, aes(x='Month', y='Counts', color='location_type')) +\n        geom_line() +\n        geom_point() +\n        facet_wrap('~borough', scales='free_y', ncol=3) +\n        labs(x='Month', y='Number of Complaints', color='Location Type', \n             title='Monthly Rodent Complaints by Location Type and Borough') +\n        scale_x_continuous(breaks=range(2, 13, 2)) +\n        theme(\n            figure_size=(20, 10),  # Adjusted figure size\n            text=element_text(size=10),\n            legend_position='right',\n            axis_text_x=element_text(rotation=0, hjust=0.5)\n            # Removed subplots_adjust\n        )\n)\n\n# Save the plot to a file with high resolution\nplot.save('rodent_complaints_plot.jpeg', width=20, height=10, dpi=300)\n\n# Corrected way to show the plot\nplot.show()\n\n\n\n\n\n\n\n\n\n\n7.3.7 Interactive Graph\nPlotly Example of Monthly Rodents Complaints in Bronx\n\nimport plotly.express as px\nimport pandas as pd\n\n# Load your dataset\n# Replace with the path to your dataset\ndata = pya.read_feather('data/cleaned_rodent_data.feather')\n\n# Convert 'Created Date' to datetime and extract 'Year' and 'Month'\ndata['created_date'] = pd.to_datetime(data['created_date'], errors='coerce')\ndata['Year'] = data['created_date'].dt.year.astype(int)\ndata['Month'] = data['created_date'].dt.month\n\n# Filter the dataset for the years 2022 and 2023\ndata_filtered = data[(data['Year'] == 2022) | (data['Year'] == 2023)]\n\n# Further filter to only include the Bronx borough\ndata_bronx = data_filtered[data_filtered['borough'] == 'BRONX'].copy()\n\n# Combine 'Year' and 'Month' to a 'Year-Month' format for more granular plotting\ndata_bronx['Year-Month'] = (data_bronx['Year'].astype(str) \n                          + '-' + data_bronx['Month'].astype(str).str.pad(2, \n                          fillchar='0'))\n\n# Group data by 'Year-Month' and 'Location Type' and count the complaints.\nmonthly_data_bronx = (data_bronx.groupby(['Year-Month', 'location_type'], \n                    as_index=False)\n                    .size()\n                    .rename(columns={'size': 'Counts'}))\n\n# Create an interactive plot with Plotly Express\nfig = px.scatter(monthly_data_bronx, x='Year-Month', y='Counts', \n                color='location_type',\n                size='Counts', hover_data=['location_type'],\n                title='Monthly Rodent Complaints by Location Type in Bronx')\n\n# Adjust layout for better readability\nfig.update_layout(\n    height=400, width=750,\n    legend_title='Location Type',\n    xaxis_title='Year-Month',\n    yaxis_title='Number of Complaints',\n    # Rotate the labels on the x-axis for better readability\n    xaxis=dict(tickangle=45)  \n)\n\n# Show the interactive plot\nfig.show()\n\n                                                \n\n\n\n\n7.3.8 Interactive Map using Google\n\n# Shapely for converting latitude/longtitude to geometry\nfrom shapely.geometry import Point \n# To create GeodataFrame\nimport geopandas as gpd\n\n# cutting the length of dataset to avoid over-capacity\nsub_data = data.iloc[:len(data)//20] # Shorten dataset for illustration.\n\n# Drop rows with missing latitude or longitude to match the lengths\nsub_data_cleaned = sub_data.dropna(subset=['latitude', 'longitude'])\n\n# creating geometry using shapely (removing empty points)\ngeometry = [Point(xy) for xy in zip(sub_data_cleaned[\"longitude\"], \\\n            sub_data_cleaned[\"latitude\"]) if not Point(xy).is_empty]\n\n# creating geometry column to be used by geopandas\ngeometry2 = gpd.points_from_xy(sub_data_cleaned[\"longitude\"],\n            sub_data_cleaned[\"latitude\"])\n\n# coordinate reference system.\ncrs = \"EPSG:4326\"\n\n# Create GeoDataFrame directly using geopandas points_from_xy utility\nrodent_geo = gpd.GeoDataFrame(sub_data_cleaned,\n                              crs=crs, \n                              geometry=gpd.points_from_xy(\n                                       sub_data_cleaned['longitude'],\n                                       sub_data_cleaned['latitude']))\n\nrodent_geo.plot(column='borough', legend=True)\n\n\n\n\n\n\n\n\n\n# Converts timestamps into strings for JSON serialization\nrodent_geo['created_date'] = rodent_geo['created_date'].astype(str)\nrodent_geo['closed_date'] = rodent_geo['closed_date'].astype(str)\n\nmap = rodent_geo.explore(column='borough', legend=True)\nmap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTips in using this map - Due to the length of information shown in ‘resolution_description’, and the amount of total columns, the information are hard to be shown fully and clearly.\n\nPlease drag the google map to keep the coordinates at the left side of the google map, so that the information could be shown on the right side.\nIn this case, the information shown could be more readable and organized.\n\n\n\n7.3.9 References\nFor more information see the following:\n\nPlotly Basic Charts\n\nhttps://plotly.com/python/basic-charts/\n\nPlotnine Tutorial\n\nhttps://plotnine.org/\n\nGeoPandas Documentation\n\nhttps://geopandas.org/en/stable/docs.html\n\nNYC Borough Data\n\nhttps://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm\n\nNYC Zip Code Data\n\nhttps://data.beta.nyc/en/dataset/nyc-zip-code-tabulation-areas/resource/894e9162-871c-4552-a09c-c6915d8783fb",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "8  Visualization",
    "section": "",
    "text": "8.1 Data Visualization with matplotlib\nThis section was written by Weijia Wu.\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nurl = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/cleaning_data_rodent3.csv'\n\ndf = pd.read_csv(url)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#data-visualization-with-matplotlib",
    "href": "visualization.html#data-visualization-with-matplotlib",
    "title": "8  Visualization",
    "section": "",
    "text": "8.1.1 Introduction\nHi Class, my name is Weijia Wu and I’m a senior double majored in Applied Math and Statistics. The following shows a basic concepts of visulization of python.\n\n\n8.1.2 Matplotlib\nMatplotlib is a desktop plotting package designed for plotting and arranging data visually in Python, usually in two-dimensional. It was created by Dr. John Hunter in 2003 as an alternative to Matlab to facilitate scientific computation and data visualization in Python.\nMatplotlib is widely used because of its simplicity and effectiveness.\n\n8.1.2.1 Installation of Matplotlib\nThe library can be installed by typing pip install matplotlib in your terminal\npip install matplotlib\n\n\n8.1.2.2 Line Plot\n\n8.1.2.2.1 Single plot with pyplot submodule\nLet’s Start with an sample Line Plot example:\n\nt = range(0, 10) \nr = [i**2 for i in t]\n\nplt.figure(figsize=(4, 4)) \n## Width and height in inches\nplt.plot(t, r)\nplt.title('Line Plot Example')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.1.2.2.2 x-label, y-label, and grid:\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r)\nplt.title('Line Plot Example2')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\n8.1.2.2.3 Add legend:\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r)\nplt.title('Line Plot Example3')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\nplt.legend()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nTo add a legend to a plot in Matplotlib, you can use the legend() function.\nA legend is a small area on the plot that describes each element of the graph.\nTo effectively use the legend, you typically need to label the elements of the plot that you want to appear in the legend using the label parameter when plotting them.\n\nplt.legend(loc='lower right', title='Legend Title', fontsize='small')\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nThe help(plt.legend) command in Python is used to display the documentation for the legend function from the Matplotlib library. This documentation includes a description of what the function does, the parameters it accepts, and other relevant information such as return values and examples of how to use the function.\n\nhelp(plt.legend)\n\nHelp on function legend in module matplotlib.pyplot:\n\nlegend(*args, **kwargs) -&gt; 'Legend'\n    Place a legend on the Axes.\n\n    Call signatures::\n\n        legend()\n        legend(handles, labels)\n        legend(handles=handles)\n        legend(labels)\n\n    The call signatures correspond to the following different ways to use\n    this method:\n\n    **1. Automatic detection of elements to be shown in the legend**\n\n    The elements to be added to the legend are automatically determined,\n    when you do not pass in any extra arguments.\n\n    In this case, the labels are taken from the artist. You can specify\n    them either at artist creation or by calling the\n    :meth:`~.Artist.set_label` method on the artist::\n\n        ax.plot([1, 2, 3], label='Inline label')\n        ax.legend()\n\n    or::\n\n        line, = ax.plot([1, 2, 3])\n        line.set_label('Label via method')\n        ax.legend()\n\n    .. note::\n        Specific artists can be excluded from the automatic legend element\n        selection by using a label starting with an underscore, \"_\".\n        A string starting with an underscore is the default label for all\n        artists, so calling `.Axes.legend` without any arguments and\n        without setting the labels manually will result in no legend being\n        drawn.\n\n\n    **2. Explicitly listing the artists and labels in the legend**\n\n    For full control of which artists have a legend entry, it is possible\n    to pass an iterable of legend artists followed by an iterable of\n    legend labels respectively::\n\n        ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n    **3. Explicitly listing the artists in the legend**\n\n    This is similar to 2, but the labels are taken from the artists'\n    label properties. Example::\n\n        line1, = ax.plot([1, 2, 3], label='label1')\n        line2, = ax.plot([1, 2, 3], label='label2')\n        ax.legend(handles=[line1, line2])\n\n\n    **4. Labeling existing plot elements**\n\n    .. admonition:: Discouraged\n\n        This call signature is discouraged, because the relation between\n        plot elements and labels is only implicit by their order and can\n        easily be mixed up.\n\n    To make a legend for all artists on an Axes, call this function with\n    an iterable of strings, one for each legend item. For example::\n\n        ax.plot([1, 2, 3])\n        ax.plot([5, 6, 7])\n        ax.legend(['First line', 'Second line'])\n\n\n    Parameters\n    ----------\n    handles : list of (`.Artist` or tuple of `.Artist`), optional\n        A list of Artists (lines, patches) to be added to the legend.\n        Use this together with *labels*, if you need full control on what\n        is shown in the legend and the automatic mechanism described above\n        is not sufficient.\n\n        The length of handles and labels should be the same in this\n        case. If they are not, they are truncated to the smaller length.\n\n        If an entry contains a tuple, then the legend handler for all Artists in the\n        tuple will be placed alongside a single label.\n\n    labels : list of str, optional\n        A list of labels to show next to the artists.\n        Use this together with *handles*, if you need full control on what\n        is shown in the legend and the automatic mechanism described above\n        is not sufficient.\n\n    Returns\n    -------\n    `~matplotlib.legend.Legend`\n\n    Other Parameters\n    ----------------\n\n    loc : str or pair of floats, default: :rc:`legend.loc`\n        The location of the legend.\n\n        The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n        ``'lower right'`` place the legend at the corresponding corner of the\n        axes.\n\n        The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n        ``'center right'`` place the legend at the center of the corresponding edge\n        of the axes.\n\n        The string ``'center'`` places the legend at the center of the axes.\n\n        The string ``'best'`` places the legend at the location, among the nine\n        locations defined so far, with the minimum overlap with other drawn\n        artists.  This option can be quite slow for plots with large amounts of\n        data; your plotting speed may benefit from providing a specific location.\n\n        The location can also be a 2-tuple giving the coordinates of the lower-left\n        corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n        will be ignored).\n\n        For back-compatibility, ``'center right'`` (but no other location) can also\n        be spelled ``'right'``, and each \"string\" location can also be given as a\n        numeric value:\n\n        ==================   =============\n        Location String      Location Code\n        ==================   =============\n        'best' (Axes only)   0\n        'upper right'        1\n        'upper left'         2\n        'lower left'         3\n        'lower right'        4\n        'right'              5\n        'center left'        6\n        'center right'       7\n        'lower center'       8\n        'upper center'       9\n        'center'             10\n        ==================   =============\n\n    bbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n        Box that is used to position the legend in conjunction with *loc*.\n        Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n        `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n        placement of the legend.\n\n        Bbox coordinates are interpreted in the coordinate system given by\n        *bbox_transform*, with the default transform\n        Axes or Figure coordinates, depending on which ``legend`` is called.\n\n        If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n        ``(x, y, width, height)`` that the legend is placed in.\n        To put the legend in the best location in the bottom right\n        quadrant of the axes (or figure)::\n\n            loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n        A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n        x, y.  For example, to put the legend's upper right-hand corner in the\n        center of the axes (or figure) the following keywords can be used::\n\n            loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\n    ncols : int, default: 1\n        The number of columns that the legend has.\n\n        For backward compatibility, the spelling *ncol* is also supported\n        but it is discouraged. If both are given, *ncols* takes precedence.\n\n    prop : None or `~matplotlib.font_manager.FontProperties` or dict\n        The font properties of the legend. If None (default), the current\n        :data:`matplotlib.rcParams` will be used.\n\n    fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n        The font size of the legend. If the value is numeric the size will be the\n        absolute font size in points. String values are relative to the current\n        default font size. This argument is only used if *prop* is not specified.\n\n    labelcolor : str or list, default: :rc:`legend.labelcolor`\n        The color of the text in the legend. Either a valid color string\n        (for example, 'red'), or a list of color strings. The labelcolor can\n        also be made to match the color of the line or marker using 'linecolor',\n        'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n        Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n        use :rc:`text.color`.\n\n    numpoints : int, default: :rc:`legend.numpoints`\n        The number of marker points in the legend when creating a legend\n        entry for a `.Line2D` (line).\n\n    scatterpoints : int, default: :rc:`legend.scatterpoints`\n        The number of marker points in the legend when creating\n        a legend entry for a `.PathCollection` (scatter plot).\n\n    scatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n        The vertical offset (relative to the font size) for the markers\n        created for a scatter plot legend entry. 0.0 is at the base the\n        legend text, and 1.0 is at the top. To draw all markers at the\n        same height, set to ``[0.5]``.\n\n    markerscale : float, default: :rc:`legend.markerscale`\n        The relative size of legend markers compared to the originally drawn ones.\n\n    markerfirst : bool, default: True\n        If *True*, legend marker is placed to the left of the legend label.\n        If *False*, legend marker is placed to the right of the legend label.\n\n    reverse : bool, default: False\n        If *True*, the legend labels are displayed in reverse order from the input.\n        If *False*, the legend labels are displayed in the same order as the input.\n\n        .. versionadded:: 3.7\n\n    frameon : bool, default: :rc:`legend.frameon`\n        Whether the legend should be drawn on a patch (frame).\n\n    fancybox : bool, default: :rc:`legend.fancybox`\n        Whether round edges should be enabled around the `.FancyBboxPatch` which\n        makes up the legend's background.\n\n    shadow : None, bool or dict, default: :rc:`legend.shadow`\n        Whether to draw a shadow behind the legend.\n        The shadow can be configured using `.Patch` keywords.\n        Customization via :rc:`legend.shadow` is currently not supported.\n\n    framealpha : float, default: :rc:`legend.framealpha`\n        The alpha transparency of the legend's background.\n        If *shadow* is activated and *framealpha* is ``None``, the default value is\n        ignored.\n\n    facecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n        The legend's background color.\n        If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\n    edgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n        The legend's background patch edge color.\n        If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\n    mode : {\"expand\", None}\n        If *mode* is set to ``\"expand\"`` the legend will be horizontally\n        expanded to fill the axes area (or *bbox_to_anchor* if defines\n        the legend's size).\n\n    bbox_transform : None or `~matplotlib.transforms.Transform`\n        The transform for the bounding box (*bbox_to_anchor*). For a value\n        of ``None`` (default) the Axes'\n        :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\n    title : str or None\n        The legend's title. Default is no title (``None``).\n\n    title_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n        The font properties of the legend's title. If None (default), the\n        *title_fontsize* argument will be used if present; if *title_fontsize* is\n        also None, the current :rc:`legend.title_fontsize` will be used.\n\n    title_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n        The font size of the legend's title.\n        Note: This cannot be combined with *title_fontproperties*. If you want\n        to set the fontsize alongside other font properties, use the *size*\n        parameter in *title_fontproperties*.\n\n    alignment : {'center', 'left', 'right'}, default: 'center'\n        The alignment of the legend title and the box of entries. The entries\n        are aligned as a single block, so that markers always lined up.\n\n    borderpad : float, default: :rc:`legend.borderpad`\n        The fractional whitespace inside the legend border, in font-size units.\n\n    labelspacing : float, default: :rc:`legend.labelspacing`\n        The vertical space between the legend entries, in font-size units.\n\n    handlelength : float, default: :rc:`legend.handlelength`\n        The length of the legend handles, in font-size units.\n\n    handleheight : float, default: :rc:`legend.handleheight`\n        The height of the legend handles, in font-size units.\n\n    handletextpad : float, default: :rc:`legend.handletextpad`\n        The pad between the legend handle and text, in font-size units.\n\n    borderaxespad : float, default: :rc:`legend.borderaxespad`\n        The pad between the axes and legend border, in font-size units.\n\n    columnspacing : float, default: :rc:`legend.columnspacing`\n        The spacing between columns, in font-size units.\n\n    handler_map : dict or None\n        The custom dictionary mapping instances or types to a legend\n        handler. This *handler_map* updates the default handler map\n        found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\n    draggable : bool, default: False\n        Whether the legend can be dragged with the mouse.\n\n\n    See Also\n    --------\n    .Figure.legend\n\n    Notes\n    -----\n    Some artists are not supported by this function.  See\n    :ref:`legend_guide` for details.\n\n    Examples\n    --------\n    .. plot:: gallery/text_labels_and_annotations/legend.py\n\n\n\n\n\n8.1.2.2.4 Colors, Markers, and Line Styles\nIf we want two plots in the same, we need to find a way to make the distinction between them.\n\nr2 = [i**3 for i in t]\n\nplt.figure(figsize=(4, 4)) \n\nplt.plot(t, r, linestyle = '--', color = 'r', marker = 'o', label = 'r')\nplt.plot(t, r2, linestyle = '-', color = 'b', marker = 'v', label = 'r2')\n\nplt.title('Line Plot Example2')\nplt.xlabel('t value')\nplt.ylabel('r value')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nUse linestyle, color, and Markers to set linestyles:\n\n## help(plt.plot)\n\n\n\n\n\n8.1.3 Example with rodent data:\nLet’s use our rodent data to demonstrate the Monthly Reported data:\n\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\n\ndf['Month'] = df['Created Date'].dt.to_period('M')\nmonthly_counts = df.groupby('Month').size()\n\nplt.figure(figsize=(10, 8))\nmonthly_counts.plot(kind='line')\nplt.title('Monthly Report Count')\nplt.xlabel('Month')\nplt.ylabel('Number of Reports')\nplt.grid(True)\nplt.xticks(rotation=45)\n\nplt.show()\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_20038/3959770936.py:1: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\n\n\n\n\n\n\n\nThis plot shows the number of rodents in each month’s report, and we can draw the following conclusions: rodent sights occur mostly in the spring and summer, and they fall dramatically after the start of autumn (post-August).\n\n8.1.3.1 Scatter plot\n\nnp.random.seed(8465);\n\nx = np.random.uniform(0, 3, 10);\ny = np.random.uniform(0, 3, 10);\nz = np.random.uniform(0, 3, 10);\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.1.3.2 Bar Plot\n\nborough_counts = df['Borough'].value_counts()\n\nplt.figure(figsize=(8, 6))  \nplt.bar(borough_counts.index, borough_counts.values, color='green')\nplt.xlabel('Borough')  \nplt.ylabel('Number of Rodent Sightings')  \nplt.title('Rodent Sightings by Borough') \nplt.xticks(rotation=45)  # Rotate the X axis by 45 degrees to show the long labels\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.1.3.3 Multiple plots using subplots submodule\n\ndf['Created Date'] = pd.to_datetime(df['Created Date'])\ndf['Date'] = df['Created Date'].dt.date\ndaily_reports = df.groupby(['Date', 'Incident Zip']).size().reset_index(name='Counts')\nsample_zip = daily_reports['Incident Zip'].dropna().iloc[0]\nsample_data = daily_reports[daily_reports['Incident Zip'] == sample_zip]\n\n## 2x2 Plot\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\n## Line Plot\naxs[0, 0].plot(sample_data['Date'], sample_data['Counts'], '-o', color='green')\naxs[0, 0].set_title(f'Linear Plot of Reports for Zip {sample_zip}')\naxs[0, 0].tick_params(labelrotation=45)\n\n## Box Plot\naxs[0, 1].boxplot(df['Y Coordinate (State Plane)'].dropna())\naxs[0, 1].set_title('Boxplot of Y Coordinate')\n\n## barplot\nstatus_counts = df['Status'].value_counts()\naxs[1, 0].bar(status_counts.index, status_counts.values, color='skyblue')\naxs[1, 0].set_title('Barplot of Status Counts')\naxs[1, 0].tick_params(labelrotation=45)\n\n## histogram\naxs[1, 1].hist(df['Latitude'].dropna(), bins=30, color='orange')\naxs[1, 1].set_title('Histogram of Latitude')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.1.3.4 Save the files\nhelp(plt.savefig)allows you to save the current figure created by Matplotlib to a file. You can specify the filename and various options to control the format, quality, and layout of the output file.\n\n## help(plt.savefig)\n\n\n\n\n8.1.4 Pandas\nPandas plotting is built on top of Matplotlib, and one of its main benefits is that it allows you to generate plots with fewer lines of code directly from Pandas data structures like DataFrames and Series. This integration simplifies the process of visualizing data for analysis.\n\n8.1.4.1 Line Plot\n\n8.1.4.1.1 Single plot\n\nmonthly_counts.plot(kind='line')\n\n\n\n\n\n\n\n\nBecause the line plot is default in pandas plots, you can omit the (kind=‘line’)\nWhen plotting with the .plot() method in Pandas, it is true that you can generate basic plots with fewer lines of code, due to the fact that Pandas automatically handles some of the basic settings, such as setting the x-axis labels automatically. However, for more detailed chart customization, such as setting gridlines, rotating x-axis labels, and so on, you may need additional Matplotlib commands to implement them.\n\nplt.figure(figsize=(8, 6))\nmonthly_counts.plot(kind='line')\n\nplt.title('Monthly Report Count')\nplt.xlabel('Month')\nplt.ylabel('Number of Reports')\nplt.grid(True)\nplt.xticks(rotation=45)\n## For longer tags, avoid overlapping\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.1.4.1.2 Multi-Lineplot\nThe following is showing several line plots in the same figure.\n\ncommunity_counts = df['Community Districts'].value_counts().sort_index()\ncity_council_counts = df['City Council Districts'].value_counts().sort_index()\npolice_precincts_counts = df['Police Precincts'].value_counts().sort_index()\n\ncounts_df = pd.DataFrame({\n    'Community Districts': community_counts,\n    'City Council Districts': city_council_counts,\n    'Police Precincts': police_precincts_counts\n})\ncounts_df = counts_df.fillna(0) \n##Fill missing values to 0\n\ncounts_df[['Community Districts', 'City Council Districts', \n'Police Precincts']].plot() \n\n\n\n\n\n\n\n\nWhen you use the .plot() method on a Pandas DataFrame to create a multi-line plot, each line in the plot is automatically assigned a different color to help distinguish between the different data columns visually. The colors are chosen from a default color cycle provided by Matplotlib.\nIf you want to customize the color:\n\ncounts_df[['Community Districts', 'City Council Districts', 'Police Precincts']].plot(\n    color=['red', 'green', 'blue']  # Custom colors for each line\n)\n\n\n\n\n\n\n\n\n\n\n\n8.1.4.2 Additional arguments\nFor more info pleased check:\n\n![additional arguments](https://drive.google.com/file/d/1j5T7_VMT1Nt4myukcmar0UMcZOHqurCk/view?usp=sharing)\n\nzsh:1: bad pattern: [additional\n\n\n\n## help(plt.plot)\n\n\n\n8.1.4.3 Bar Plot\nFor categorical data, one of common visualization is the barplot.\n\nGenerated using df.plot.bar() method, for horizontal version df.plot.barh().\n\n\n8.1.4.3.1 Side-by-side Bar Plot:\nLet’s use Borough and Location Type to generate a side-by-side bar plot, one horizontal and one vertical:\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 6))\n\n## Vertical bar plot for Borough counts\ndf.groupby(['Borough']).size().plot.bar(ax=axs[0], color='skyblue', rot=0)\naxs[0].set_title('Bar plot for Borough')\n\n## Horizontal bar plot for Location Type counts\ndf.groupby(['Location Type']).size().plot.barh(ax=axs[1], color='lightgreen')\naxs[1].set_title('Bar plot for Location Type')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimiliar with axs in matplotlib:\n\nnrows=1 means there will be 1 row of subplots.\nncols=2means there will be 2 columns of subplots.\n\n\n\n8.1.4.3.2 Grouped Bar Plot\nThis type of plot is useful for comparing the distribution within each class side by side.\n\nclass_Borough = pd.crosstab(df[\"Borough\"], df[\"Status\"])\n\nclass_Borough.plot.bar(rot=45, figsize=(8, 6))\n\n\n\n\n\n\n\n\n\n\n8.1.4.3.3 Stacked Bar Plot\nThis plot is useful for comparing the total counts across borough while still being able to see the proportion of each borough within each class.\n\nclass_Borough.plot.bar(stacked=True)\n\n\n\n\n\n\n\n\n\n\n\n8.1.4.4 Histogram and Density Plots\nFor numeric data, histogram allows us to see the distribution (center shape, skewness) of the data.\nHistogram can be generated using df.plot.hist() method\nSince we have limited numeric data in our rodent data, I used student achievement data to present it:\n\nurl2 = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/grades_example.csv'\ndf2 = pd.read_csv(url2)\n\n\ndf2[\"Grade\"].plot.hist(bins = 10, figsize=(8, 6))\n\n\n\n\n\n\n\n\nAs can be seen from the plot, the students’ scores show a normal distribution, with most of them clustered in the 70-80 range\n\ndf2[\"Grade\"].plot.density()\n\n\n\n\n\n\n\n\n\n\n8.1.4.5 Scatter Plots\nWhen dealing with two variables, scatter plot allow us to examine if there is any correlation between them.\nScatter can be generated using df.plot.scatter(x = col1, y = col2) method.\n\nurl3 = 'https://raw.githubusercontent.com/JoannaWuWeijia/Data_Store_WWJ/main/student_example3.csv'\ndf3 = pd.read_csv(url3)\n\n\ndf3.plot.scatter(x=\"Weight\", y=\"Height\", figsize=(8, 6))\n\n\n\n\n\n\n\n\nAs you can see it’s roughly a linear regression, and I’ll cover how to add a regression line in the next sns section.\n\n\n\n8.1.5 Seaborn\n\nSeaborn is designed to work directly with pandas DataFrames, making plotting more convenient by allowing direct use of DataFrame columns for specifying data in plots.\nSeaborn makes it easy to add linear regression lines and other statistical models to your charts, simplifying the process of statistical data visualization.\nSeaborn’s default styles and color are more aesthetically pleasing and modern compared to Matplotlib.\n\n\n8.1.5.1 Installation of Seaborn\npip install seaborn\n\n\n8.1.5.2 Histogram and Density Plots\n\n## help(sns.histplot) \n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df2['Grade'], bins=10, kde = True)\n\n\n\n\n\n\n\n\nbins: The number of bars in the histogram. More bins can make the data distribution more detailed, but too many may cause the chart to be difficult to understand; fewer bins may not be able to show the data distribution accurately. kde: (Kernel Density Estimate Line) a density curve will be added to the histogram, which is generated by kernel density estimation and can help understand the shape of the data distribution\n\n\n8.1.5.3 Scatter plot with Regression line\nI used an example with less data to be able to show it. We can see that the height and weight of the students are directly proportional.\n\ndf4 = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie', 'David', \n    'Eva', 'Fiona', 'George', 'Hannah', 'Ian', 'Julia'],\n    'Height': [160, 172, 158, 165, 170, 162, 175, 168, 180, 155],\n    'Weight': [55, 72, 60, 68, 62, 56, 80, 65, 75, 50]})\n\nplt.figure(figsize = (8, 6))\nsns.regplot(x='Weight', y='Height', data=df4)\n\n\n\n\n\n\n\n\n\n\n8.1.5.4 Categorical Data\n\n8.1.5.4.1 barplot\n\nnp.random.seed(0) \ngenders = np.random.choice(['Male', 'Female'], size=500)\nclasses = np.random.choice(['A', 'B', 'C', 'D'], size=500)\ngrades = np.random.choice(['Excellent', 'Good', 'Average', 'Poor'], size=500)\ndf4 = pd.DataFrame({'Gender': genders, 'Class': classes, 'Grades': grades})\n\n\nsns.catplot(x='Class', hue='Gender', col='Grades', \nkind='count', data=df4, height=5, col_wrap=2)\nplt.show()\n\n\n\n\n\n\n\n\n\nx='Class': This sets the x-axis to represent different classes, so each class will have its own set of bars in the plot.\nhue='Gender': This parameter adds a color coding (hue) based on the ‘Gender’ column\ncol='Grades': This creates separate subplots (columns) for each unique value in the ‘Grades’ column (e.g., Excellent, Good, Average, Poor), effectively grouping the data by grades.\ncol_wrap=2: Limits the number of these subplots to 2 per row. If there are more than 2 unique grades, additional rows will be created to accommodate all the subplots.\nkind='count': Specifies the kind of plot to draw. In this case, 'count'means it will count the occurrences of each category combination and display this as bars in the plot.\nheight=5: Sets the height of each subplot to 5 inches.\n\n\n\n8.1.5.4.2 Box Plot\n\nsns.boxplot(x='Gender', y='Grades', hue='Class', data=df4)\nplt.show()\n\n\n\n\n\n\n\n\n\nx='Gender': x-axis variable\ny='Grades': y-axis variable, which in this case is ‘Grades’. Since ‘Grades’ is a categorical variable with values like ‘Excellent’, ‘Good’, ‘Average’, ‘Poor’\ncol='Class': Creates separate subplots for each unique value in the ‘Class’ column, effectively grouping the data by class.\n\n\n\n8.1.5.4.3 Categorical Data Help\n\n##help(sns.catplot)\n\n\n\n\n\n8.1.6 Conclusion\nMatplotlib is the foundation for making plots in Python.\npandas uses Matplotlib for its plotting features but is mainly for handling data.\nSeaborn makes Matplotlib prettier and easier to use, especially with pandas data.\n\n\n8.1.7 References\n\nhttps://matplotlib.org/stable/users/project/history.html\nhttps://matplotlib.org/stable/gallery/lines_bars_and_markers/simple_plot.html\nhttps://www.simplilearn.com/tutorials/python-tutorial/matplotlib\nhttps://www.w3schools.com/python/pandas/pandas_plotting.asp\nhttps://github.com/mwaskom/seaborn/tree/master/seaborn\nhttps://seaborn.pydata.org/installing.html\nhttps://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#grammar-of-graphics-with-plotnine",
    "href": "visualization.html#grammar-of-graphics-with-plotnine",
    "title": "8  Visualization",
    "section": "8.2 Grammar of Graphics with Plotnine",
    "text": "8.2 Grammar of Graphics with Plotnine\nThis section was written by Olivia Massad.\n\n8.2.1 Introduction\nHello everyone! My name is Olivia Massad and I am a junior Statistical Data Science Major. I am very interested in sports statistics and analytics, especially involving football, and am very excited to learn more about coding and data science in this class. Today I will be talking about grammar of graphics for python, using Plotnine. This is a new topic for me so I am very excited to show you all what we can do with it.\n\n\n8.2.2 What is Grammar of Graphics?\nSimilarly to how languages have grammar in order to structure language and create a standard for how sentences and words should be arranged, grammar of graphics provides the framework for a consistent way to structure and create statistical visualizations. This framework helps us to create graphs and visualizations which can be widely understood due to the consistent structure. The major components of grammar of graphics are:\n\nData: our datasets and the what components you want to visualize.\nAesthetics: axes, position of data points, color, shape, size.\nScale: scale values or use specific scales depending on multiple values and ranges.\nGeometric objects: how data points are depicted, whether they’re points, lines, bars, etc.\nStatistics: statistical measures of the data included in the graphic, including mean, spread, confidence intervals, etc.\nFacets: subplots for specific data dimensions.\nCoordinate system: cartesian or polar.\n\n\n\n8.2.3 What can you do with Plotnine?\nPlotnine is a program which implements grammar of graphics in order to create data visualizations and graphs using python. It is based on ggplot2 and allows for many variations within graphs. Some examples of things we can create with plotnine are:\n\nBar Charts\nHistograms\nBox Plots\nScatter Plots\nLine Charts\nTime Series\nDensity Plots\netc.\n\n\n\n8.2.4 Using Plotnine\nIn order to use plotnine we first need to install the package using our command line.\nWith conda: “conda install -c conda-forge plotnine”\nWith pip: “pip install plotnine pip install plotnine[all]”\nNow that plotnine is installed, we must call the it in python.\n\nfrom plotnine import *\nfrom plotnine.data import *\n\nNow that plotnine is installed and imported, we can begin to make graphs and plots. Below are different examples of visualizations we can make using plotnine and the personalizations we can add to them. For these graphics I used the rodent sighting data from the NYC open data 311 requests. We also will need pandas and numpy for some of these graphs so we need to import those as well. Additionally, because the data set is so large, we will only be lookng at the first 500 complaints.\n\nfrom plotnine import *\nfrom plotnine.data import *\nimport pandas as pd \nimport numpy as np \nimport os\nfolder = 'data'\nfile = 'rodent_2022-2023.feather'\npath = os.path.join(folder, file)\ndata = pd.read_feather(path)\ndata_used = data.head(500)\n\n\n8.2.4.1 Bar Chart\nOne common type of visualization we can create with plotnine is a bar chart. For this graph we will look at the data for the descriptors of each complaint.\n\n(ggplot(data_used, aes(x = 'descriptor')) \n    + geom_bar())\n\n\n\n\n\n\n\n\nWhile this code provides us with a nice simple chart, because we are using plotnine, we can make some major improvements to the visualization to make it easier to read and more appealing. Some simple things we can do are:\n\nAdd a title.\nColor code the bars.\nChange the orientation of the graph.\nAdd titles to the axes.\n\n\n(ggplot(data_used, aes(x = 'descriptor', fill = 'descriptor')) \n        # Color code the bars.\n    + geom_bar() # Bar Chart\n    + ggtitle('Descriptor Counts') # Add a title.\n    + coord_flip() # Change the orientation of the graph.\n    + xlab(\"Descriptor\") # Add title to x axis.\n    + ylab(\"Number of Complaints\") # Add titles to y axis.\n)\n\n\n\n\n\n\n\n\nSome more complex changes we can make to our graph are:\n\nChange the orientation of the words on the axes to make them easier to read.\nAdd color coded descriptors to each bar.\n\n\n(ggplot(data_used, aes(x = 'descriptor', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_bar() # Bar Chart\n    + ggtitle('Descriptor Counts') # Add a title.\n    + xlab(\"Descriptor\") # Add title to x axis.\n    + ylab(\"Number of Complaints\") # Add titles to y axis.\n    + theme(axis_text_x=element_text(angle=45))\n     # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n8.2.4.2 Scatter Plot\nAnother common visualization we can create is a scatterplot. When looking at the data from the 311 requests, we can see that there are many data points for locations of these complaints. A scatter plot would be a great way to see the location of the complaints by graphing the longitudes and latitudes. In order to better see the points, for this graph we will only use the first 200 complaints.\n\ndata_scatter = data.tail(200)\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude')) \n    + geom_point())\n\n\n\n\n\n\n\n\nSimilarly to the original code for the bar chart, this code provides a very simple scatter plot. Plotnine allows us to add many specializations to the scatterplot in order to differentiate the points from each other. We can:\n\nAdd color to the points.\nDifferentiate using point size.\nDifferentiate using point shape.\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n       color = 'location_type')) # Add color to the points.\n    + geom_point())\n\n\n\n\n\n\n\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n    size = 'descriptor', # Differentiate using point size.\n    shape = 'borough')) # Differentiate using point shape.\n    + geom_point())\n\n/Users/junyan/work/teaching/ids-s24/ids-s24/.myvenv/lib/python3.12/site-packages/plotnine/scales/scale_size.py:51: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nWe can see that due to the close data points, filtering the data using size and shape can become a little congested. One thing we can do to fix this while still viewing the same data is through the use of “facet_grid”.\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude',\n    shape = 'borough')) # Differentiate using point shape.\n    + geom_point()\n    + facet_grid('descriptor ~ .') # Create multiple plots.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_scatter, aes(x = 'longitude', y = 'latitude'))\n    + geom_point()\n    + facet_grid('descriptor ~ borough') \n        # Create multiple plots with 2 conditions.\n    + theme(strip_text_y = element_text(angle = 0), # change facet text angle\n        axis_text_x=element_text(angle=45)) # change x axis text angle\n)\n\n\n\n\n\n\n\n\n\n\n8.2.4.3 Histogram\nThe last common graph we will cover using plotnine is a histogram. Here we will use the created date data as a continuous variable. Using plotnine we are able to make many of the same personalizations we were able to do with bar charts.\n\ndata_used['created_date']=pd.to_datetime(\n  data_used['created_date'],\n  format = \"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n(ggplot(data_used, aes(x='created_date'))\n    + geom_histogram())\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_20038/966048317.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/Users/junyan/work/teaching/ids-s24/ids-s24/.myvenv/lib/python3.12/site-packages/plotnine/stats/stat_bin.py:109: PlotnineWarning: 'stat_bin()' using 'bins = 10'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\n\n\nNow that we have a simple histogram with our data we can add specializations, inclduing:\n\nChange width of bins.\nChange oreintation of graph.\nAdd color coded descriptors.\nChange outline color.\nChange the orientation of the words on the axes to make them easier to read.\n\n\n(ggplot(data_used, aes(x='created_date', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_histogram(binwidth=1,  # Change width of bins\n      color = 'black') # Change outline color.\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_used, aes(x='created_date', fill = 'borough')) \n        # Add color coded descriptors.\n    + geom_histogram(binwidth=1,  # Change width of bins\n      colour = 'black') # Change outline color.\n    + coord_flip() # Change oreintation of graph.\n)\n\n\n\n\n\n\n\n\nWhile we’re able to color code the histogram to show other descriptors of the data, another way we can do this with plotnine is through the use of multiple graphs. Using “facet_wrap” we can create a multi facet graph with the same data.\n\n(ggplot(data_used, aes(x='created_date')) \n    + geom_histogram(binwidth=1) # Change width of bins\n    + facet_wrap('borough') # Create multiple graphs.\n    + theme(axis_text_x=element_text(angle=45)) \n    # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n8.2.4.4 Density Plot\nThe last visualization we’re going to look at is density plots. While less common than the graphs previously discussed, density plots show the distribution of a specific variable.\n\n(ggplot(data_used, aes(x='created_date'))\n    + geom_density())\n\n\n\n\n\n\n\n\nAbove we can see a very simple density graph with very little description. Using plotnine we are able to:\n\nAdd color coded descriptors.\nScale groups by relative size.\nChange the orientation of the words on the axes to make them easier to read.\n\n\n(ggplot(data_used, aes(x='created_date', color = 'descriptor')) \n        #Add color coded descriptors.\n    + geom_density()\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n(ggplot(data_used, aes(x='created_date', color = 'descriptor')) \n        #Add color coded descriptors.\n    + geom_density(aes(y=after_stat('count'))) \n        # Scale groups by relative size.\n    + theme(axis_text_x=element_text(angle=45)) \n        # Change the orientation of the words.\n)\n\n\n\n\n\n\n\n\n\n\n\n8.2.5 Resources\n\nhttps://plotnine.readthedocs.io/en/v0.12.4/gallery.html\n\n\n\n8.2.6 References\n\n“Plotnine.Geoms.Geom_bar¶.” Plotnine.Geoms.Geom_bar - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/stable/generated/ plotnine.geoms.geom_bar.html. Accessed 13 Feb. 2024.\n“Plotnine.Geoms.Geom_density¶.” Plotnine.Geoms.Geom_density - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_density.html. Accessed 17 Feb. 2024.\n“Plotnine.Geoms.Geom_histogram¶.” Plotnine.Geoms.Geom_histogram - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_histogram.html#plotnine. geoms.geom_histogram. Accessed 17 Feb. 2024.\n“Plotnine.Geoms.Geom_point¶.” Plotnine.Geoms.Geom_point - Plotnine Commit: D1f7dbf Documentation, plotnine.readthedocs.io/en/ stable/generated/plotnine.geoms.geom_point.html. Accessed 16 Feb. 2024.\n“Plotnine.” PyPI, pypi.org/project/plotnine/. Accessed 13 Feb. 2024.\nSarkar, Dipanjan (DJ). “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-Dimensional…” Medium, Towards Data Science, 13 Sept. 2018, towardsdatascience.com/a-comprehensive-guide-to-the- grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#handling-spatial-data-with-geopandas",
    "href": "visualization.html#handling-spatial-data-with-geopandas",
    "title": "8  Visualization",
    "section": "8.3 Handling Spatial Data with GeoPandas",
    "text": "8.3 Handling Spatial Data with GeoPandas\nThis section was written by Pratham Patel.\n\n8.3.1 Introduction and Installation\nHello! my name is Pratham Patel and I am a Senior due to graduate this semster with a Bachelor’s Degree of Science in Mathematics/Statistics with a Computer Science minor. I hope to gain skills in using various different packages of Python in this course, as well as understand even more about the Data Science field. An example of learning new Python packages is the topic I will present today on the geopandas package. GeoPandas is an extension of the pandas package to support geographic data in its dataframes.\nThe GeoPandas package can be installed via the terminal using any of the following commands.\nThe documentation recommends: conda install -c conda-forge geopandas\nStandard conda install: conda install geopandas\nUsing pip: pip install geopandas\n\n\n8.3.2 Base Concepts\nGeoPandas relvolves around the GeoDataFrame object, which is essentially the pandas DataFrame object, with all the traditional capabilities in addition to the ability store and operate on geometry columns.\nThe geometry types include points, lines and closed polygons (the first and last coordinates in the list must be the same).\nThe objects made by shapely.geometry can represent these geometry types:\n\nfrom shapely.geometry import Point, LineString, Polygon\nimport geopandas as gpd\n\npoint = Point(0, 1)\ngdf1 = gpd.GeoDataFrame(geometry=[point])\n\nline = LineString([(0, 0), (1, 1)])\ngdf2 = gpd.GeoDataFrame(geometry=[line])\n\n#note: the first and last element of \n#the list of tupled points are the same\npolygon = Polygon([(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)])\ngdf3 = gpd.GeoDataFrame(geometry=[polygon])\n\n\ngdf1\n\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (0.00000 1.00000)\n\n\n\n\n\n\n\n\nSome of the basic attributes of a GeoSeries include: * length: returns length of a line\n\ngdf2.length\n\n0    1.414214\ndtype: float64\n\n\n\narea: returns the area of the polygon\n\n\ngdf3.area\n\n0    4.0\ndtype: float64\n\n\n\nbounds: gives the bounds of each row in a column of geometry\ntotal_bounds: gives the bounds of a geometry series\ngeom_type: returns geometry type\n\n\ngdf1.geom_type\n\n0    Point\ndtype: object\n\n\n\nis_valid: return True for valid geometries and false otherwise (mostly important for polygons).\n\n\ngdf3.is_valid\n\n0    True\ndtype: bool\n\n\nNext, we will cover various methods to be used on GeoSeries objects:\n\ndistance(): returns the Series with the minimum distance from each entry to another geometry or Series (argument other).\n\nNote: a secondary argument align is a boolean to align the GeoSeries by index if set to True\n\n\n\ngdf2.distance(Point((1,0)))\ngdf2.distance(LineString([(0, 2), (1, 2)]))\n\n0    1.0\ndtype: float64\n\n\n\ncentroid: returns a new GeoSeries with the center of each row’s geometry.\n\n\ngdf3.centroid\n\n0    POINT (1.00000 1.00000)\ndtype: geometry\n\n\n\ncontains(): returns True if the shape contains a specific geometry or Series.\n\nparameters other and align\n\n\n\ngdf3.contains(Point((0.5, 1.5)))\n\n0    True\ndtype: bool\n\n\n\ngdf3.contains(gdf1)\n\n0    False\ndtype: bool\n\n\n\nintersects() returns true if shape intersects another geometry of series\n\nparameters other and align\n\n\n\n\n8.3.3 Reading Files into GeoDataFrame’s\nThe function geopandas.read_file() is the best way to read a file with both data and geometry into a GeoDataFrame object. From here, we will be using the nyc rodent data and visualize it. The code below converts every incident’s location into a point on the geometry.\n\n# Reading csv file \nimport pandas as pd \nimport numpy as np\n# Shapely for converting latitude/longtitude to a point\nfrom shapely.geometry import Point \n# To create GeoDataFrame\nimport geopandas as gpd \n\n#read in the feather file as a generic pandas DataFrame\nrat_22_23 = pd.read_feather('data/rodent_2022-2023.feather')\n\n# creating geometry using shapely (removing missing points) for the already built in longitude and latitude coordinates\ngeometry = [Point(xy) for xy in zip(rat_22_23[\"longitude\"], rat_22_23[\"latitude\"]) if not Point(xy).is_empty]\n\n# creating geometry column to be used by geopandas using the points_from_xy method\ngeo = gpd.points_from_xy(rat_22_23[\"longitude\"], rat_22_23[\"latitude\"])\n\n# coordinate reference system (epsg:4326 implies geographic coordinates)\ncrs = {'init': 'epsg:4326'}\n\n# create GeoDataFrame (takes care of the missing coordinates) \nrodent_gdf = gpd.GeoDataFrame(rat_22_23.loc[~pd.isna(rat_22_23[\"longitude\"]) & ~pd.isna(rat_22_23[\"latitude\"])], crs=crs, geometry=geometry)\n\n/Users/junyan/work/teaching/ids-s24/ids-s24/.myvenv/lib/python3.12/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n\n\nHere, we can take a view at the new GeoDataFrame:\n\nrodent_gdf.head()\n\n\n\n\n\n\n\n\n\nunique_key\ncreated_date\nclosed_date\nagency\nagency_name\ncomplaint_type\ndescriptor\nlocation_type\nincident_zip\nincident_address\n...\nlatitude\nlongitude\nlocation\nzip_codes\ncommunity_districts\nborough_boundaries\ncity_council_districts\npolice_precincts\npolice_precinct\ngeometry\n\n\n\n\n0\n59893776\n2023-12-31 23:05:41\n2023-12-31 23:05:41\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n3+ Family Apt. Building\n11216\n265 PUTNAM AVENUE\n...\n40.683857\n-73.951645\n(40.683855196486164, -73.95164557951071)\n17618.0\n69.0\n2.0\n49.0\n51.0\n51.0\nPOINT (-73.95164 40.68386)\n\n\n1\n59887523\n2023-12-31 22:19:22\n2024-01-03 08:47:02\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\nCommercial Building\n10028\n1538 THIRD AVENUE\n...\n40.779243\n-73.953690\n(40.77924175816874, -73.95368859796383)\n10099.0\n23.0\n4.0\n1.0\n11.0\n11.0\nPOINT (-73.95369 40.77924)\n\n\n2\n59891998\n2023-12-31 22:03:12\n2023-12-31 22:03:12\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n3+ Family Apt. Building\n10458\n2489 TIEBOUT AVENUE\n...\n40.861694\n-73.894989\n(40.861693023118924, -73.89499228560491)\n10936.0\n6.0\n5.0\n22.0\n29.0\n29.0\nPOINT (-73.89499 40.86169)\n\n\n3\n59887520\n2023-12-31 21:13:02\n2024-01-03 09:33:43\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nMouse Sighting\n3+ Family Apt. Building\n11206\n116 JEFFERSON STREET\n...\n40.699741\n-73.930733\n(40.69974221739347, -73.93073474327662)\n17213.0\n42.0\n2.0\n30.0\n53.0\n53.0\nPOINT (-73.93073 40.69974)\n\n\n4\n59889297\n2023-12-31 20:50:10\nNaT\nDOHMH\nDepartment of Health and Mental Hygiene\nRodent\nRat Sighting\n1-2 Family Dwelling\n11206\n114 ELLERY STREET\n...\n40.698444\n-73.948578\n(40.69844506428295, -73.94858040356128)\n17213.0\n69.0\n2.0\n49.0\n51.0\n51.0\nPOINT (-73.94858 40.69844)\n\n\n\n\n5 rows × 48 columns\n\n\n\n\n\n\n8.3.4 Plotting\nThe new geometry allows us to plot the data easily.\n\n#standard plot of every single rodent incident\nrodent_gdf.plot()\n\n#color the plot by borough\nrodent_gdf.plot(column = 'borough', legend=True)\n\n#color the plot by borough, with more transparent markers\nrodent_gdf.plot(column = 'borough', alpha = 0.01)\n\n#color by the descriptor of the incident\nrodent_gdf.plot(column = 'descriptor', legend=True)\n\n#Plot the missing information for borough\nrodent_gdf.plot(column='borough', missing_kwds={'color':'red'})\n\n#color the plot by zipcode\nrodent_gdf.plot(column = 'incident_zip', legend=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that if an integer column is passed, the legend will present the key as a gradient by default.\nYou can individualize each zipcode using categorical=True, though be sure the list of unique integers is not too large.\nrodent_gdf.plot(column = 'incident_zip', legend=True, categorical=True)\nThe geographic visualizations allow us to try to observe some trends amongst the reported rodent incident we see.\n\n\n8.3.5 Interactive Maps\nA very interesting aspect is the ability to create interactive graphs using the .explore() method.\nNote that folium, matplotlib, and mapclassify are necessary for the .explore() function.\n\n#interactive map with incidents colored by borough\nrodent_gdf.explore(column='borough', legend=True)\n\nTypeError: Object of type Timestamp is not JSON serializable\n\n\n&lt;folium.folium.Map at 0x12ac8e360&gt;\n\n\nThis map lets us specifically find various points and examine them and their surroudings.\n\n\n8.3.6 Setting and Changing Projections\nIn the code, a Coordinate Reference System(CRS) was set using crs = {'init':'epsg:4326'}. CRS can be set on on initialized GeoDataFrame using the .set_crs function. We can do this for our previous example gdf3:\n\ngdf3 = gdf3.set_crs(\"EPSG:4326\")\ngdf3.plot()\n\n\n\n\n\n\n\n\nThere are other CRS’s that can be set by the .to_crs() function. Examples include: * ESPG:2263 - coordinates labeled in feet * ESPG:3395 - World Mercator System\n\n\n8.3.7 References\n\nGeoPandas Documentation:\n\nhttps://geopandas.org/en/stable/index.html\n\nNYC 311 Service Request\n\nhttps://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "statsmod.html",
    "href": "statsmod.html",
    "title": "9  Statistical Models",
    "section": "",
    "text": "9.1 Introduction\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statsmod.html#introduction",
    "href": "statsmod.html#introduction",
    "title": "9  Statistical Models",
    "section": "",
    "text": "9.1.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n9.1.2 Features of statsmodels\nPackage statsmodels offers a comprehensive range of statistical models and tests, making it a powerful tool for a wide array of data analysis tasks:\n\nLinear Regression Models: Essential for predicting quantitative responses, these models form the backbone of many statistical analysis operations.\nGeneralized Linear Models (GLM): Expanding upon linear models, GLMs allow for response variables that have error distribution models other than a normal distribution, catering to a broader set of data characteristics.\nTime Series Analysis: This includes models like ARIMA for analyzing and forecasting time-series data, as well as more complex state space models and seasonal decompositions.\nNonparametric Methods: For data that does not fit a standard distribution, statsmodels provides tools like kernel density estimation and smoothing techniques.\nStatistical Tests: A suite of hypothesis testing tools allows users to rigorously evaluate their models and assumptions, including diagnostics for model evaluation.\n\nIntegrating statsmodels into your data science workflow enriches your analytical capabilities, allowing for both exploratory data analysis and complex statistical modeling.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statsmod.html#generalized-linear-models",
    "href": "statsmod.html#generalized-linear-models",
    "title": "9  Statistical Models",
    "section": "9.2 Generalized Linear Models",
    "text": "9.2 Generalized Linear Models\nGeneralized Linear Models (GLM) extend the classical linear regression to accommodate response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGeneralized Linear Models (GLM) adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function (\\(g(\\mu) = \\mu\\)) is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity (\\(g(\\mu) = \\mu\\))\nDistribution: Normal\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit (\\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\))\nDistribution: Binomial\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log (\\(g(\\mu) = \\log(\\mu)\\))\nDistribution: Poisson\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse (\\(g(\\mu) = \\frac{1}{\\mu}\\))\nDistribution: Gamma\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statsmod.html#statistical-modeling-with-statsmodels",
    "href": "statsmod.html#statistical-modeling-with-statsmodels",
    "title": "9  Statistical Models",
    "section": "9.3 Statistical Modeling with statsmodels",
    "text": "9.3 Statistical Modeling with statsmodels\nThis section was written by Leon Nguyen.\n\n9.3.1 Introduction\nHello! My name is Leon Nguyen (they/she) and I am a second-year undergraduate student studying Statistical Data Science and Mathematics at the University of Connecticut, aiming to graduate in Fall 2025. One of my long-term goals is to make the field of data science more accessible to marginalized communities and minority demographics. My research interests include data visualization and design. Statistical modeling is one of the most fundamental skills required for data science, and it’s important to have a solid understanding of how models work for interpretable results.\nThe statsmodels Python package offers a diverse range of classes and functions tailored for estimating various statistical models, conducting statistical tests, and exploring statistical data. Each estimator provides an extensive array of result statistics, rigorously tested against established statistical packages to ensure accuracy. This presentation will focus on the practical applications of the statistical modeling aspect.\n\n\n9.3.2 Key Features and Capabilities\nSome key features and capabilities of statsmodels are:\n\nGeneralized Linear Models\nDiagnostic Tests\nNonparametric methods\n\nIn this presentation, we will work with practical applications of statistical modeling in statsmodels. We will briefly cover how to set up linear, logistic, and Poisson regression models, and touch upon kernel density estimation and diagnostics. By the end of this presentation, you should be able to understand how to use statsmodels to analyze your own datasets using these fundamental techniques.\n\n\n9.3.3 Installation and Setup\nTo install statsmodels, use pip install statsmodels or conda install statsmodels, depending on whether you are using pip or conda.\nOne of the major benefits of using statsmodels is their compatability with other commnonly used packages, such as NumPy, SciPy, and Pandas. These packages provide foundational scientific computing functionalities that are crucial for working with statsmodels. To ensure everything is set up correctly, import the necessary libraries at the beginning of your script:\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\nHere are some minimum dependencies:\n\nPython &gt;= 3.8\nNumPy &gt;= 1.18\nSciPy &gt;= 1.4\nPandas &gt;= 1.0\nPatsy &gt;= 0.5.2\n\nThe last item listed above, patsy, is a Python library that provides simple syntax for specifying statistical models in Python. It allows users to define linear models using a formula syntax similar to the formulas used in R and other statistical software. More patsy documentation can be found here. This library is not used this demonstration, but is still worth noting.\n\n\n9.3.4 Importing Data\nThere are a few different options to import data. For example, statsmodels documentation demonstrates how to importing from a CSV file hosted online from the Rdatasets repository:\n\n# Reads the 'avocado' dataset from the causaldata package into df\ndf0 = sm.datasets.get_rdataset(dataname='avocado', package=\"causaldata\").data\n# We will be using this dataset later!\n\n# Print out the first five rows of our dataframe\nprint(df0.head())\n\n         Date  AveragePrice  TotalVolume\n0  2015-12-27          0.90   5040365.47\n1  2015-12-20          0.94   4695737.21\n2  2015-12-13          0.87   5259354.30\n3  2015-12-06          0.78   5775536.27\n4  2015-11-29          0.91   4575710.62\n\n\nWe can also read directly from a local CSV file with pandas. For example, we will be using the NYC 311 request rodent data:\n\n# Reads the csv file into df\ndf = pd.read_csv('data/rodent_2022-2023.csv')\n\n# Brief data pre-processing\n# Time reformatting\ndf['Created Date'] = pd.to_datetime(df['Created Date'], format = \"%m/%d/%Y %I:%M:%S %p\")\ndf['Closed Date'] = pd.to_datetime(df['Closed Date'], format = \"%m/%d/%Y %I:%M:%S %p\")\ndf['Created Year'] = df['Created Date'].dt.year\ndf['Created Month'] = df['Created Date'].dt.month\n# Response time\ndf['Response Time'] = df['Closed Date'] - df['Created Date'] \ndf['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600) # in hours\n# Remove unspecified borough rows\ndf = df.drop(df[df['Borough']=='Unspecified'].index)\n# Remove 'other' open data channel type rows\ndf = df.drop(df[df['Open Data Channel Type']=='OTHER'].index)\n\nprint(df.head())\n\n   Unique Key        Created Date         Closed Date Agency  \\\n0    59893776 2023-12-31 23:05:41 2023-12-31 23:05:41  DOHMH   \n1    59887523 2023-12-31 22:19:22 2024-01-03 08:47:02  DOHMH   \n2    59891998 2023-12-31 22:03:12 2023-12-31 22:03:12  DOHMH   \n3    59887520 2023-12-31 21:13:02 2024-01-03 09:33:43  DOHMH   \n4    59889297 2023-12-31 20:50:10                 NaT  DOHMH   \n\n                               Agency Name Complaint Type      Descriptor  \\\n0  Department of Health and Mental Hygiene         Rodent    Rat Sighting   \n1  Department of Health and Mental Hygiene         Rodent    Rat Sighting   \n2  Department of Health and Mental Hygiene         Rodent    Rat Sighting   \n3  Department of Health and Mental Hygiene         Rodent  Mouse Sighting   \n4  Department of Health and Mental Hygiene         Rodent    Rat Sighting   \n\n             Location Type  Incident Zip      Incident Address  ...  \\\n0  3+ Family Apt. Building         11216     265 PUTNAM AVENUE  ...   \n1      Commercial Building         10028     1538 THIRD AVENUE  ...   \n2  3+ Family Apt. Building         10458   2489 TIEBOUT AVENUE  ...   \n3  3+ Family Apt. Building         11206  116 JEFFERSON STREET  ...   \n4      1-2 Family Dwelling         11206     114 ELLERY STREET  ...   \n\n                                   Location Zip Codes Community Districts  \\\n0  (40.683855196486164, -73.95164557951071)   17618.0                69.0   \n1   (40.77924175816874, -73.95368859796383)   10099.0                23.0   \n2  (40.861693023118924, -73.89499228560491)   10936.0                 6.0   \n3   (40.69974221739347, -73.93073474327662)   17213.0                42.0   \n4   (40.69844506428295, -73.94858040356128)   17213.0                69.0   \n\n  Borough Boundaries City Council Districts Police Precincts Police Precinct  \\\n0                2.0                   49.0             51.0            51.0   \n1                4.0                    1.0             11.0            11.0   \n2                5.0                   22.0             29.0            29.0   \n3                2.0                   30.0             53.0            53.0   \n4                2.0                   49.0             51.0            51.0   \n\n  Created Year  Created Month Response Time  \n0         2023             12      0.000000  \n1         2023             12     58.461111  \n2         2023             12      0.000000  \n3         2023             12     60.344722  \n4         2023             12           NaN  \n\n[5 rows x 50 columns]\n\n\n\n\n9.3.5 Troubleshooting\nWhenever you are having problems with statsmodels, you can access the official documentation by visiting this link. If you are working in a code editor, you can also run the following in a code cell:\n\nsm.webdoc() \n# Opens the official documentation page in your browser\n\nTo look for specific documentation, for example sm.GLS, you can run the following:\n\nsm.webdoc(func=sm.GLS, stable=True)\n# func : string* or function to search for documentation \n# stable : (True) or development (False) documentation, default is stable\n\n# *Searching via string has presented issues?\n\n\n\n9.3.6 Statistical Modeling and Analysis\nConstructing statistical models with statsmodels generally follows a step-by-step process:\n\nImport necessary libraries: This includes both numpy and pandas, as well as statsmodels.api itself (sm).\nLoad the data: This could be data from the rdataset repository, local csv files, or other formats. In general, it’s best practice to load your data into a pandas DataFrame so that it can easily be manipulated using pandas functions.\nPrepare the data: This involves converting variables into appropriate types (e.g., categorical into factors), handling missing values, and creating appropriate interaction terms.\nDefine our model: what model is the appropriate representation of our research question? This could be an OLS regression (sm.OLS), logistic regression (sm.Logit), or any number of other models depending on the nature of our data.\nFit the model to our data: we use the .fit() method which takes as input our dependent variable and independent variables.\nAnalyze the results of the model: this is where we can get things like parameter estimates, standard errors, p-values, etc. We use the .summary() method to print out these statistics.\n\n\n\n9.3.7 Generalized Linear Models\nGLM models allow us to construct a linear relationship between the response and predictors, even if their underlying relationship is not linear. This is done via a link function, which is a transformation which links the response variable to a linear model.\nKey points of GLMs:\n\nData should be independent and random.\nThe response variable \\(Y\\) does not need to be normally distributed, but the distribution is from an exponential family (e.g. binomial, Poisson, normal).\nGLMs do not assume a linear relationship between the response variable and the explanatory variables, but assume a linear relationship between the transformed expected response in terms of the link function and the explanatory variables.\nGLMs are useful when the range of your response variable is constrained and/or the variance is not constant or normally distributed.\nGLM models transform the response variable to allow the fit to be done by least squares. The transformation done on the response variable is defined by the link function.\n\n\n9.3.7.1 Linear Regression\nSimple and muliple linear regression are special cases where the expected value of the dependent value is equal to a linear combination of predictors. In other words, the link function is the identity function \\(g[E(Y)]=E(Y)\\). Make sure assumptions for linear regression hold before proceeding. The model for linear regression is given by: \\[y_i = X_i\\beta + \\epsilon_i\\] where \\(X_i\\) is a vector of predictors for individual \\(i\\), and \\(\\beta\\) is a vector of coefficients that define this linear combination.\nWe will be working with the avocado dataset from the package causaldata which contains information about the average price and total amount of avocados that were sold in California from 2015-2018. AveragePrice of a single avocado is our predictor, and TotalVolume is our outcome variable as a count of avocados.\nHere is an application of SLR with statsmodels:\n\n# We can use .get_rdataset() to load data into Python from a repositiory of R packages.\ndf1 = sm.datasets.get_rdataset('avocado', package=\"causaldata\").data\n\n# Fit regression model\nresults1 = smf.ols('TotalVolume ~ AveragePrice', data=df1).fit()\n\n# Analyze results\nprint(results1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            TotalVolume   R-squared:                       0.441\nModel:                            OLS   Adj. R-squared:                  0.438\nMethod:                 Least Squares   F-statistic:                     132.0\nDate:                Mon, 25 Mar 2024   Prob (F-statistic):           7.04e-23\nTime:                        16:27:18   Log-Likelihood:                -2550.3\nNo. Observations:                 169   AIC:                             5105.\nDf Residuals:                     167   BIC:                             5111.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept     9.655e+06    3.3e+05     29.222      0.000       9e+06    1.03e+07\nAveragePrice -3.362e+06   2.93e+05    -11.487      0.000   -3.94e+06   -2.78e+06\n==============================================================================\nOmnibus:                       50.253   Durbin-Watson:                   0.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              165.135\nSkew:                           1.135   Prob(JB):                     1.38e-36\nKurtosis:                       7.278   Cond. No.                         9.83\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can interpret some values:\n\ncoef: the coefficient of AveragePrice tells us how much adding one unit of AveragePrice changes the predicted value of TotalVolume. An important interpretation is that if AveragePrice was to increase by one unit, on average we could expect TotalVolume to change by this coefficient based on this linear model. This makes sense since higher prices should result in a smaller amount of avocados sold.\nP&gt;|t|: p-value to test significant effect of the predictor on the response, compared to a significance level \\(\\alpha=0.05\\). When this p-value \\(\\leq \\alpha\\), we would reject the null hypothesis that there is no effect of AveragePrice on TotalVolume, and conclude that AveragePrice has a statistically significant effect on TotalVolume.\nR-squared: indicates the proportion of variance explained by the predictors (in this case just AveragePrice). If it’s close to 1 then most of the variability in TotalVolume is explained by AveragePrice, which is good! However, only about 44.1% of the variability is explained, so this model could use some improvement.\nProb (F-statistic): indicates whether or not the linear regression model provides a better fit to a dataset than a model with no predictors. Assuming a significance level of 0.05, we would reject the null hypothesis (model with just the intercept does just as well with a model with predictors) since our F-value probability is less than 0.05. We know that AveragePrice gives at least some significant information about TotalVolume. (This makes more sense in MLR where you are considering multiple predictors.)\nSkew: measures asymmetry of a distribution, which can be positive, negative, or zero. If skewness is positive, the distribution is more skewed to the right; if negative, then to the left. We ideally want a skew value of zero in a normal distribution.\nKurtosis: a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution. For a normal distribution, we expect a kurtosis of 3. If our kurtosis is greater than 3, there are more outliers on the tails. If less than 3, then there are less.\nProb (Jarque-Bera): indicates whether or not the residuals are normally distributed, which is required for the OLS linear regression model. In this case the test rejects the null hypothesis that the residuals come from a normal distribution. This is concerning because non-normality can lead to misleading conclusions and incorrect standard errors.\n\n\n\n9.3.7.2 Logistic Regression\nLogistic regression is used when the response variable is binary. The response distribution is logistic which means it has support (input) on \\((0,1)\\) and\nis invertible. The log-odds link function is defined as \\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\), where \\(\\mu\\) is the predicted probability.\nHere we have an example from our rodents dataset, where the response variable Under 3h indicates whether the response time for a 311 service request was under 3 hours. 1 indicates that the response time is less than 3 hours, and 0 indications greater than or equal to 3 hours. We are creating a logistic regression model that can be used to estimate the odds ratio of 311 requests having a response time under 3 hours based on Borough and Open Data Channel Type (method of how 311 service request was submitted) as predictors.\n\n# Loaded the dataset in a previous cell as df\n# Create binary variable\ndf['Under 3h'] = (df['Response Time'] &lt; 3).astype(int)\n\n# Convert the categorical variable to dummy variables\ndf = df.loc[:, ['Borough', 'Open Data Channel Type', 'Under 3h']]\ndf = pd.get_dummies(df, dtype = int)\n\n# Remove reference dummy variables\ndf.drop(\n  columns=['Borough_QUEENS', 'Open Data Channel Type_MOBILE'], \n  axis=1, \n  inplace=True\n)\n\nFor this regression to run properly, we needed to create \\(k-1\\) dummy variables with \\(k\\) levels in a given predictor. Here we have two categorical variables that we used pd.get_dummies() function to change from a categorical variable into dummy variables. We then dropped one dummy variable level from each category: 'Borough_QUEENS' and 'Open Data Channel Type_MOBILE'.\n\n# Drop all rows with NaN values\ndf.dropna(inplace = True)\n\n# Fit the logistic regression model using statsmodels\nY = df['Under 3h']\nX = sm.add_constant(df.drop(columns = 'Under 3h', axis=1))\n# need to consider constant manually\n\nlogitmod = sm.Logit(Y, X)\nresult = logitmod.fit(maxiter=30)\n# Summary of the fitted model\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.616020\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               Under 3h   No. Observations:                82846\nModel:                          Logit   Df Residuals:                    82839\nMethod:                           MLE   Df Model:                            6\nDate:                Mon, 25 Mar 2024   Pseudo R-squ.:                 0.02459\nTime:                        16:27:19   Log-Likelihood:                -51035.\nconverged:                       True   LL-Null:                       -52321.\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n=================================================================================================\n                                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nconst                             0.8691      0.022     39.662      0.000       0.826       0.912\nBorough_BRONX                     0.1051      0.029      3.644      0.000       0.049       0.162\nBorough_BROOKLYN                 -0.4675      0.023    -20.232      0.000      -0.513      -0.422\nBorough_MANHATTAN                -0.7170      0.024    -29.892      0.000      -0.764      -0.670\nBorough_STATEN ISLAND             0.0122      0.050      0.243      0.808      -0.086       0.110\nOpen Data Channel Type_ONLINE     0.2933      0.019     15.402      0.000       0.256       0.331\nOpen Data Channel Type_PHONE      0.4540      0.018     25.836      0.000       0.420       0.488\n=================================================================================================\n\n\n\ncoef: the coefficients of the independent variables in the logistic regression equation are interpreted a little bit differently than linear regression; for example, if borough_MANHATTAN increases by one unit and all else is held constant, we expect the log odds to decrease by 0.7192 units. According to this model, we can expect it is less likely for response time to be under three hours for a 311 service request in Manhattan compared to Queens (reference level). On the other hand, if borough_BRONX increases by one unit and all else is held constant, we expect the log odds to increase by 0.1047 units. We can expect it is more likely for response time to be under three hours for a 311 service request in the Bronx compared to Queens. If we want to look at comparisons between Open Data Channel Type, from this model, we can also see that 311 requests in the dataset that were submitted via phone call are more likely to have a response time under three hours compared to those that were submitted via mobile.\nLog-Likelihood: the natural logarithm of the Maximum Likelihood Estimation(MLE) function. MLE is the optimization process of finding the set of parameters that result in the best fit. Log-likelihood on its own doesn’t give us a lot of information, but comparing this value from two different models with the same number of predictors can be useful. Higher log-likelihood indicates a better fit.\nLL-Null: the value of log-likelihood of the null model (model with no predictors, just intercept).\nPseudo R-squ.: similar but not exact equivalent to the R-squared value in Least Squares linear regression. This is also known as McFadden’s R-Squared, and is computed as \\(1-\\dfrac{L_1}{L_0}\\), where \\(L_0\\) is the log-likelihood of the null model and \\(L_1\\) is that of the full model.\nLLR p-value: the p-value of log-likelihood ratio test statistic comparing the full model to the null model. Assuming a significance level \\(\\alpha\\) of 0.05, if this p-value \\(\\leq \\alpha,\\) then we reject the null hypothesis that the model is not significant. We reject the null hypothesis; thus we can conclude this model has predictors that are significant (non-zero coefficients).\n\nAnother example:\nWe will use the macrodata dataset directly from statsmodels, which contains information on macroeconomic indicators in the US across different quarters from 1959 to 2009, such as unemployment rate, inflation rate, real gross domestic product, etc. I have created a binary variable morethan5p that has a value of 1 when the unemployment rate is more than 5% in a given quarter, and is 0 when it is equal to or less than 5%. We are creating a logistic regression model that can be used to estimate the odds ratio of the unemployment rate being greater than 5% based on cpi (end-of-quarter consumer price index) and pop (end-of-quarter population) as predictors.\n\n# df2 is an instance of a statsmodels dataset class\ndf2 = sm.datasets.macrodata.load_pandas()\n# add binary variable\ndf2.data['morethan5p'] = (df2.data['unemp']&gt;5).apply(lambda x:int(x))\n# Subset data\ndf2 = df2.data[['morethan5p','cpi','pop']]\n# Logit regression model\nmodel = smf.logit(\"morethan5p ~ cpi + pop\", df2)\nresult2 = model.fit()\nsummary = result2.summary()\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 0.559029\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:             morethan5p   No. Observations:                  203\nModel:                          Logit   Df Residuals:                      200\nMethod:                           MLE   Df Model:                            2\nDate:                Mon, 25 Mar 2024   Pseudo R-squ.:                 0.05839\nTime:                        16:27:19   Log-Likelihood:                -113.48\nconverged:                       True   LL-Null:                       -120.52\nCovariance Type:            nonrobust   LLR p-value:                 0.0008785\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     24.0603      6.914      3.480      0.001      10.510      37.611\ncpi            0.0744      0.023      3.291      0.001       0.030       0.119\npop           -0.1286      0.038     -3.349      0.001      -0.204      -0.053\n==============================================================================\n\n\nWe can compute odds ratios and other information by calling methods on the fitted result object. Below are the 95% confidence intervals of the odds ratio \\(e^{\\text{coef}}\\) of each coefficient:\n\nodds_ratios = pd.DataFrame(\n    {\n        \"Odds Ratio\": result2.params,\n        \"Lower CI\": result2.conf_int()[0],\n        \"Upper CI\": result2.conf_int()[1],\n    }\n)\nodds_ratios = np.exp(odds_ratios)\n\nprint(odds_ratios)\n\n             Odds Ratio      Lower CI      Upper CI\nIntercept  2.813587e+10  36683.855503  2.157971e+16\ncpi        1.077211e+00      1.030540  1.125997e+00\npop        8.793232e-01      0.815568  9.480629e-01\n\n\nNote these are no longer log odds we are looking at! We estimate with 95% confidence that the true odds ratio lies between the lower CI and upper CI for each coefficient. A larger odds ratio is associated with a larger probability that the unemployment rate is greater than 5%.\n\n\n9.3.7.3 Poisson Regression\nThis type of regression is best suited for modeling the how the mean of a discrete variable depends on one or more predictors.\nThe log of the probability of success is modeled by:\n\\(\\log(\\mu) = b_0 + b_1x_1 + ... + b_kx_k\\)\nwhere \\(\\mu\\) is the probability of success (the response variable). The intercept b0 is assumed to be 0 if not provided in the model. We will use .add_constant to indicate that our model includes an intercept term.\nLet’s use the sunspots dataset from statsmodels. This is a one variable dataset that counts the number of sunspots that occur in a given year (from 1700 - 2008). Note that the link function for Poisson regression is a log function, which means \\(\\log{E(Y)}=X\\beta.\\)\nWe first load an instance of a statsmodels dataset class, analogous to a pandas dataframe:\n\ndf3 = sm.datasets.sunspots.load_pandas()\ndf3 = df3.data\n\ndf3['YEAR'] = df3['YEAR'].apply(lambda x: x-1700)\n# YEAR is now number of years after 1700, scaling the data for better results\ndf3['YEAR2'] = df3['YEAR'].apply(lambda x: x**2)\n# YEAR2 is YEAR squared, used as additional predictor\n\nX = sm.add_constant(df3[['YEAR','YEAR2']]) \n# .add_constant indicates that our model includes an intercept term\nY = df3['SUNACTIVITY']\n\nprint(df3[['YEAR','YEAR2','SUNACTIVITY']].head())\n\n   YEAR  YEAR2  SUNACTIVITY\n0   0.0    0.0          5.0\n1   1.0    1.0         11.0\n2   2.0    4.0         16.0\n3   3.0    9.0         23.0\n4   4.0   16.0         36.0\n\n\nIn the code above, we are altering our predictors a little bit from the orignal dataset; we are substracting the minimum year 1700 from all YEAR values so it is more centered. It is generally good practice to scale and center your data so that the model can have better fit. In our case this also aids the interpretability of the intercept coefficient we will see later. We are adding the varaible YEAR2, which is the number of years since 1700 squared to see if there is some non-linear relationship that may exist.\nWe can use the .GLM function with the family='poisson' argument to fit our model. Some important parameters:\n\ndata.endog acts as a series of observations for the dependent variable \\(Y\\)\ndata.exog acts as a series of observations for each predictor\nfamily specifies the distribution appropriate for the model\n\n\nresult3 = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\nprint(result3.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            SUNACTIVITY   No. Observations:                  309\nModel:                            GLM   Df Residuals:                      306\nModel Family:                 Poisson   Df Model:                            2\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -5548.3\nDate:                Mon, 25 Mar 2024   Deviance:                       9460.6\nTime:                        16:27:19   Pearson chi2:                 9.37e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.8057\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.6781      0.026    140.479      0.000       3.627       3.729\nYEAR           0.0003      0.000      0.747      0.455      -0.000       0.001\nYEAR2       5.368e-06   1.13e-06      4.755      0.000    3.16e-06    7.58e-06\n==============================================================================\n\n\n\ncoef: In this model, increasing the YEAR seems to increase the log of the expected count of Sunspot activity (SUNACTIVITY) by a small amount; the expected count of sunspot activty increases by \\(e^{.0003}\\) (note that increasing YEAR also increases YEAR2 so we have to be careful with interpretability!) This model also suggests that the number of sunspots is for the year 1700 is estimated to be \\(e^{3.6781}\\approx39.57\\), while the number of actual sunspots that year was 5.\nDeviance: two times the difference between the log-likelihood of a fitted GLM and the log-likelihood of a perfect model where fitted responses match observed responses. A greater deviance indicates a worse fit.\nPearson chi2: measures the goodness-of-fit of the model based on the square deviations between observed and expected values based on the model. A large value suggests that the model does not fit well.\n\n\n\n\n9.3.8 Diagnostic Tests\nThroughout the GLMs listed above, we can find different statistics to assess how well the model fits the data. They include:\n\nDeviance: Measures the goodness-of-fit by taking the difference between the log-likelihood of a fitted GLM and the log-likelihood of a perfect model where fitted responses match observed responses. A larger deviance indicates a worse fit for the model. This is a test statistic for Likelihood-ratio tests compared to a chi-squared distribution with \\(df=df_{\\text{full}}-df_{\\text{null}}\\), for comparing a full model against a null model (or some reduced model) similar to a partial F-test.\n\n\nprint(\"Poisson Regression Deviance:\", result3.deviance)\n\nPoisson Regression Deviance: 9460.616202027577\n\n\n\nPearson’s chi-square test: This tests whether the predicted probabilities from the model differ significantly from the observed counts. The test statistic is calculated by taking the difference between the null deviance (deviance of a model with just the intercept term) and residual deviance (how well the response variable can be predicted by a model with a given number of predictors). Large Pearson’s chi-squares indicate poor fit.\n\n\nprint(\"Chi Squared Stat:\",result3.pearson_chi2)\n\nChi Squared Stat: 9368.0333091202\n\n\n\nResidual Plots: Like in linear regression, we can visually plot residuals to look for patterns that shouldn’t be there. There are different types of residuals that we can look at, such as deviance residuals:\n\n\nfig = plt.figure(figsize=(8, 4))\nplt.scatter(df3['YEAR'],result3.resid_deviance)\n\n\n\n\n\n\n\n\n\n\n9.3.9 Nonparametric Models\n\n9.3.9.1 Kernel Density Estimation\nstatsmodels has a non-parametric approach called kernel density estimation (KDE), which estimates the underlying probability of a given assortment of data points. KDE is used when you don’t have enough data points to form a parametric model. It estimates the density of continuous random variables, or extrapolates some continuous function from discrete counts. KDE is a non-parametric way to estimate the underlying distribution of data. The KDE weights all the distances of all data points relative to every location. The more data points there are at a given location, the higher the KDE estimate at that location. Points closer to a given location are generally weighted more than those further away. The shape of the kernel function itself indicates how the point distances are weighted. For example, a uniform kernel function will give equal weighting across all values within a bandwidth, whereas a triangle kernel function gives weighting dependent on linear distance.\nKDE can be applied for univariate or multivariate data. statsmodels has two methods for this: - sm.nonparametric.KDEunivariate: For univariate data. This estimates the bandwidth using Scott’s rule unless specified otherwise. Much faster than using .KDEMultivariate due to its use of Fast Fourier Transforms on univariate, continuous data. - sm.nonparametric.KDEMultivariate: This applies to both univariate and multivariate data, but tends to be slower. Can use mixed types of data but requires specification.\nHere we will demonstrate how to apply it to univariate data, based off of examples provided in the documentation. We will generate a histogram of based off of geyser waiting time data from Rdatasets. This dataset records the waiting time between “Old Faithful” geyser’s eruptions in Yellowstone National Park. Our goal is to fit a KDE with a Gaussian kernel function to this data.\n\n# Load data\ndf5 = sm.datasets.get_rdataset(\"faithful\", \"datasets\")\nwaiting_obs = df5.data['waiting'] \n\n# Scatter plot of data samples and histogram\nfig = plt.figure(figsize=(8, 4))\nax = fig.add_subplot()\nax.set_ylabel(\"Count\")\nax.set_xlabel(\"Time (min)\")\n\nax.hist(\n    waiting_obs,\n    bins=25, \n    color=\"darkblue\",\n    edgecolor=\"w\", \n    alpha=0.8,\n    label=\"Histogram\"\n)\n\nax.scatter(\n    waiting_obs,\n    np.abs(np.random.randn(waiting_obs.size)),\n    color=\"orange\",\n    marker=\"o\",\n    alpha=0.5,\n    label=\"Samples\",\n)\n\nax.legend(loc=\"best\")\nax.grid(True, alpha=0.35)\n\n\n\n\n\n\n\n\nNow we want to fit our KDE based on our waiting_obs sample:\n\nkde = sm.nonparametric.KDEUnivariate(waiting_obs)\nkde.fit()  # Estimate the densities\nprint(\"Estimated Bandwidth:\", kde.bw)  \n\n# Scatter plot of data samples and histogram\nfig = plt.figure(figsize=(8, 4))\nax1 = fig.add_subplot()\nax1.set_ylabel(\"Count\")\nax1.set_xlabel(\"Time (min)\")\n\nax1.hist(\n    waiting_obs,\n    bins=25, \n    color=\"darkblue\",\n    edgecolor=\"w\", \n    alpha=0.8,\n    label=\"Histogram\",\n)\n\nax1.scatter(\n    waiting_obs,\n    np.abs(np.random.randn(waiting_obs.size)),\n    color=\"orange\",\n    marker=\"o\",\n    alpha=0.5,\n    label=\"Waiting times\",\n)\n\nax2 = ax1.twinx()\nax2.plot(\n    kde.support, \n    kde.density, \n    lw=3, \n    label=\"KDE\")\nax2.set_ylabel(\"Density\")\n\n# Joining legends\nlines, labels = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(lines + lines2, labels + labels2, loc=0)\n\nax1.grid(True, alpha=0.35)\n\nEstimated Bandwidth: 4.693019309795263\n\n\n\n\n\n\n\n\n\nWhen fitting the KDE, a kde.bw or bandwidth parameter is returned. We can alter this to see how it affects the fit and smoothness of the curve. The smaller the bandwidth, the more jagged the estimated distribution becomes.\n\n# Scatter plot of data samples and histogram\nfig = plt.figure(figsize=(8, 4))\nax1 = fig.add_subplot()\nax1.set_ylabel(\"Count\")\nax1.set_xlabel(\"Time (min)\")\n\nax1.hist(\n    waiting_obs,\n    bins=25, \n    color=\"darkblue\",\n    edgecolor=\"w\", \n    alpha=0.8,\n    label=\"Histogram\"\n)\n\nax1.scatter(\n    waiting_obs,\n    np.abs(np.random.randn(waiting_obs.size)),\n    color=\"orange\",\n    marker=\"o\",\n    alpha=0.5,\n    label=\"Samples\",\n)\n\n# Plot the KDE for various bandwidths\nax2 = ax1.twinx()\nax2.set_ylabel(\"Density\")\n\nfor (bandwidth, color) in [(0.5,\"cyan\"), (4,\"#bbaa00\"), (8,\"#ff79ff\")]:\n    kde.fit(bw=bandwidth)  # Estimate the densities\n    ax2.plot(\n        kde.support,\n        kde.density,\n        \"--\",\n        lw=2,\n        color=color,\n        label=f\"KDE from samples, bw = {bandwidth}\",\n        alpha=0.9\n    )\nax1.legend(loc=\"best\")\nax2.legend(loc=\"best\")\nax1.grid(True, alpha=0.35)\n\n\n\n\n\n\n\n\n\n\n\n9.3.10 References\n\nInstalling statsmodels:\n\nhttps://www.statsmodels.org/stable/install.html\n\nRdatasets repository and statsmodels datasets:\n\nhttps://github.com/vincentarelbundock/Rdatasets/blob/master/datasets.csv\nhttps://cran.r-project.org/web/packages/causaldata/causaldata.pdf\nhttps://hassavocadoboard.com/\nhttps://www.statsmodels.org/stable/datasets/index.html\nhttps://www.statsmodels.org/stable/datasets/generated/sunspots.html\nhttps://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/faithful\n\nNYC 311 Service Request Data:\n\nhttps://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data\n\nGetting help with statsmodels:\n\nhttps://www.statsmodels.org/stable/generated/statsmodels.tools.web.webdoc.html#statsmodels.tools.web.webdoc\nhttps://www.statsmodels.org/stable/endog_exog.html\n\nLoading data, model fit, and summary procedure:\n\nhttps://www.statsmodels.org/stable/gettingstarted.html\n\nSummary Data Interpretation:\n\nhttps://www.statology.org/a-simple-guide-to-understanding-the-f-test-of-overall-significance-in-regression/\nhttps://www.statology.org/linear-regression-p-value/\nhttps://www.statology.org/omnibus-test/\nhttps://www.statisticshowto.com/jarque-bera-test/\nhttps://www.statology.org/how-to-report-skewness-kurtosis/\nhttps://www.statology.org/interpret-log-likelihood/\nhttps://stackoverflow.com/questions/46700258/python-how-to-interpret-the-result-of-logistic-regression-by-sm-logit\nhttps://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/\nhttps://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\nhttps://vulstats.ucsd.edu/chi-squared.html\nhttps://roznn.github.io/GLM/sec-deviance.html\n\nGeneralized Linear Models:\n\nhttps://sscc.wisc.edu/sscc/pubs/glm-r/\nhttps://online.stat.psu.edu/stat504/lesson/6/6.1\nhttps://www.mygreatlearning.com/blog/generalized-linear-models/\nhttps://www.statsmodels.org/stable/examples/notebooks/generated/glm.html\n\nLogistic Regression:\n\nhttps://www.andrewvillazon.com/logistic-regression-python-statsmodels/\nhttps://towardsdatascience.com/how-to-interpret-the-odds-ratio-with-categorical-variables-in-logistic-regression-5bb38e3fc6a8\nhttps://towardsdatascience.com/a-simple-interpretation-of-logistic-regression-coefficients-e3a40a62e8cf\nhttps://www.statology.org/interpret-log-likelihood/\n\nPoisson Regression:\n\nhttps://tidypython.com/poisson-regression-in-python/\n\nNon-parametric Methods:\n\nhttps://mathisonian.github.io/kde/\nhttps://www.statsmodels.org/stable/nonparametric.html\nhttps://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kde.KDEUnivariate.html\nhttps://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html\nhttps://www.statsmodels.org/stable/examples/notebooks/generated/kernel_density.html\n\nDiagnostic tests:\n\nhttps://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests\nhttps://bookdown.org/ltupper/340f21_notes/deviance-and-residuals.html\nhttps://www.statology.org/null-residual-deviance/\n\nData Visualization:\n\nhttps://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1 Introduction\nSupervised and unsupervised learning represent two core approaches in the field of machine learning, each with distinct methodologies, applications, and goals. Understanding the differences and applicabilities of these learning paradigms is fundamental for anyone venturing into data science and machine learning.\nThe key difference between supervised and unsupervised learning lies in the presence or absence of labeled output data. Supervised learning depends on known outputs to train the model, making it suitable for predictive tasks where the relationship between the input and output is clear. Unsupervised learning, however, thrives on discovering the intrinsic structure of data, making it ideal for exploratory analysis and understanding complex data dynamics without predefined labels.\nBoth supervised and unsupervised learning have their place in the machine learning ecosystem, often complementing each other in comprehensive data analysis and modeling projects. While supervised learning allows for precise predictions and classifications, unsupervised learning offers deep insights and uncovers underlying patterns that might not be immediately apparent.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#introduction",
    "href": "supervised.html#introduction",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "Supervised Learning Supervised learning is characterized by its use of labeled datasets to train algorithms. In this paradigm, the model is trained on a pre-defined set of training examples, which include an input and the corresponding output. The goal of supervised learning is to learn a mapping from inputs to outputs, enabling the model to make predictions or decisions based on new, unseen data. This approach is widely used in applications such as spam detection, image recognition, and predicting consumer behavior. Supervised learning is further divided into two main categories: regression, where the output is continuous, and classification, where the output is categorical.\nUnsupervised Learning In contrast, unsupervised learning involves working with datasets without labeled responses. The aim here is to uncover hidden patterns, correlations, or structures from input data without the guidance of an explicit output variable. Unsupervised learning algorithms are adept at clustering, dimensionality reduction, and association tasks. They are invaluable in exploratory data analysis, customer segmentation, and anomaly detection, where the structure of the data is unknown, and the goal is to derive insights directly from the data itself.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#classification-versus-regression",
    "href": "supervised.html#classification-versus-regression",
    "title": "10  Supervised Learning",
    "section": "10.2 Classification versus Regression",
    "text": "10.2 Classification versus Regression\nThe main tasks in supervised learning can broadly be categorized into two types: classification and regression. Each task utilizes algorithms to interpret the input data and make predictions or decisions based on that data.\n\nClassification Classification tasks involve categorizing data into predefined classes or groups. In these tasks, the output variable is categorical, such as “spam” or “not spam” in email filtering, or “malignant” or “benign” in tumor diagnosis. The aim is to accurately assign new, unseen instances to one of the categories based on the learning from the training dataset. Classification can be binary, involving two classes, or multiclass, involving more than two classes. Common algorithms used for classification include Logistic Regression, Decision Trees, Support Vector Machines, and Neural Networks.\nRegression Regression tasks predict a continuous quantity. Unlike classification, where the outcomes are discrete labels, regression models predict a numeric value. Examples of regression tasks include predicting the price of a house based on its features, forecasting stock prices, or estimating the age of a person from a photograph. The goal is to find the relationship or correlation between the input features and the continuous output variable. Linear regression is the most basic form of regression, but there are more complex models like Polynomial Regression, Ridge Regression, Lasso Regression, and Regression Trees.\n\nBoth classification and regression are foundational to supervised learning, addressing different types of predictive modeling problems. Classification is used when the output is a category, while regression is used when the output is a numeric value. The choice between classification and regression depends on the nature of the target variable you are trying to predict. Supervised learning algorithms learn from labeled data, refining their models to minimize error and improve prediction accuracy on new, unseen data.\n\n10.2.1 Classification Metrics\n\n10.2.1.1 Confusion matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTP: number of true positives\nFN: number of false negatives\nFP: number of false positives\nTN: number of true negatives\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTPR: TP / (TP + FN). Also known as sensitivity.\nFNR: FN / (TP + FN). Also known as miss rate.\nFPR: FP / (FP + TN). Also known as false alarm, fall-out.\nTNR: TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour rates from the confusion matrix with predicted (column) margins:\n\nPPV: TP / (TP + FP). Also known as precision.\nFDR: FP / (TP + FP).\nFOR: FN / (FN + TN).\nNPV: TN / (FN + TN).\n\n\n\n10.2.1.2 Measure of classification performance\nMeasures for a given confusion matrix:\n\nAccuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.\nRecall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.\nPrecision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.\nF-beta score: Harmonic mean of precision and recall with \\(\\beta\\) chosen such that recall is considered \\(\\beta\\) times as important as precision, \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}\n\\] See stackexchange post for the motivation of \\(\\beta^2\\).\n\nWhen classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\n\n\n\n10.2.2 Cross-validation\n\nGoal: strike a bias-variance tradeoff.\nK-fold: hold out each fold as testing data.\nScores: minimized to train a model\n\nCross-validation is an important measure to prevent over-fitting. Good in-sample performance does not necessarily mean good out-sample performance. A general work flow in model selection with cross-validation is as follows.\n\nSplit the data into training and testing\nFor each candidate model \\(m\\) (with possibly multiple tuning parameters)\n\nFit the model to the training data\nObtain the performance measure \\(f(m)\\) on the testing data (e.g., CV score, MSE, loss, etc.)\n\nChoose the model \\(m^* = \\arg\\max_m f(m)\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#tree-based-machine-learning-models",
    "href": "supervised.html#tree-based-machine-learning-models",
    "title": "10  Supervised Learning",
    "section": "10.3 Tree Based Machine Learning Models",
    "text": "10.3 Tree Based Machine Learning Models\n\nPresented by Emmanuel Yankson\n\n\n10.3.0.0.1 About Me\nMy name is Emmanuel Yankson, although some call me Emmy for short (first three letters of my first name and the initial of my last name). I am a junior studying statistical data science as a major and computer science as a minor. Hobbies include playing the piano, philosphy, building computers, and co-Teaching STAT 4188.\n\n\n10.3.1 What is Machine Learning?\nSimilar to how humans learn where there is a focus on acquisition of knowledge that would better inform decisions made in the future, take that knowledge into our own personal exeprience which can then be used to inform us about the knowledge we gather and decisions we make in the future, a machine learns when we give it data, set criteria for training and testing, and then set metrics for the outcomes of these tests which can then be used to improve the effectiveness and learning of our model.\nThere are different types of machine learning, namely supervised learning, unsupervised learning, and reinforcement learning. Decision Trees and Random Forest fall under the category of supervised, as given some labels Y, we train our model on correct examples of our labels, which will come later.\n\n\n10.3.2 Basic Terms\n\nTraining - The process involves pushing a separated section of your data to your model with the purpose of enabling the model to learn about which values are optimal for adjusting all the weights and biases, based on labeled examples. The training section of your data is typically the majority of your dataset, although the percentage depends on the size of the data you are working with\nTesting - It serves as a completely independent dataset to evaluate the model’s performance and generalization ability. The test section of your data is typically the minority of your dataset, although again the percentage depends on the size of the data you are working with\nModels - An algorithm meant to learn from the data it recieves\nPrediction - When the model recieves data and makes a prediction concerning our label of interest based on the previous data it received\nPerformance Metrics - A way to evaluate model effectiveness and performnace in different aspects. Different performance metrics are used depending on the type of machine learning task and the specific goals of the analysis. In this file we will be utilizing the performance metrics of accuracy (the proportion of correctly classified instances out of the total instances in the dataset), precision (out of all positives, how many are correctly identified), and F1 Score (the harmonic mean of precision and recall, it provides a balance between precision and recall)\n\n\n\n10.3.3 Training and Testing\nSo when it comes to training, testing, and evaluating our models, we of course cannot have the model trained on the entirity of our dataset. Why? Because by having a separate testing set, we can evaluate how well our trained model generalizes to new, unseen data. Additionally, if were to train our model on the entirity of our dataset, it would cause overfitting (Unexplained high accuracy, used to training noise). While the model would perform very well when it comes to the training set, it performs poorly for new data that it hasn’t seen before.\n\n\n10.3.4 Decision Trees\nA Decision Tree is hierarchical tree-like structure which frequently takes the form of a binary tree in our case, where starting from the root node, we recursively split our data until we reach an outcome. There are two types of decision trees, regression and classification, with classification being the focus of this discussion.\nExamining the structure, imagine a Decision Tree as a series of interconnected questions posed to our dataset. The root node plays the role of asking the first question that results in the first split of the data. Subsequent internal nodes act as further questions, introducing additional criteria that lead to successive splits. These splits create branches and sub-branches, forming a branching structure throughout the tree.\nAt each decision node, the dataset is partitioned based on the answers to the posed questions, guiding the data along different paths within the tree. This recursive process continues until we reach the leaves of the tree, which represent the outcomes or decisions made. The leaves contain the final classifications based on the criteria applied throughout the tree. When moving along the tree, we will have our left branch denote our true or ‘yes’ condition at that node, while we will have our right branch denote our false condition at that particular node.\nDue its easy accessbility through visuals like a flow chart, it almost appears that the decision tree algorithm is simpler than it actually is. One would foolishly assume that the algorithm is a series of if-else statements that parse through a dataset. How do we also decide the root note as well?\nHowever, simple if-else statements will not work as the purpose of this machine learning algorithm is to find the optimal split for each decision node. The hope is to get something called a pure leaf node, which is a leaf node a node where all instances of our datapoints share the same class or outcome.\nHow do we find the optimal split? While there are different forms of splitting criteria, the one we will be discussing today is known as GINI.\n\n10.3.4.1 GINI Index\nThe GINI Index, or GINI takes on values between 0 and 1 and calculated by the following formula:\nGini(p) = 1 - \\(\\Sigma_{i=1}^J\\) \\(p_{i}^2\\)\nWhere the probability value in this case is the probability of the incorrect classification squared subtracted by the probability of the correct classification squared. We will calculate the Gini index for each side of the split. And once we calculated the GINI index for both sides of the split we can calculate the weighted GINI index:\nWeighted GINI = 1 - \\(\\Sigma_{i=1}^J\\) \\(w_{i}\\) * \\(p_{i}^2\\)\nThis weighted Gini value will provide a measure between 0 and 1, where 0 signifies the purest node. In a pure node, only one class of outcome exists, and the data in that leaf node is entirely homogenous. On the other end of the spectrum, a value of 1 indicates the highest level of impurity, suggesting a mixture of outcomes present at that node.\nThe weighted Gini index serves as the splitting criterion for our decision nodes. The split that minimizes the weighted Gini index is chosen because it represents the most effective way to segregate the data. This process assigns more weight to important groups and less weight to less important ones, allowing the decision tree algorithm to make informed and balanced decisions at each internal node.\nBesides choosing the splitting criteria for your decision tree, there are other ways to tune your model in order to improve your classification results, using the following hyperparameters:\n\nsplitter: The strategy used to choose the split at each node\ncriterion: “Gini”, “Entropy”, “log_loss”\nmax_depth: Maximum depth of the tree\nmin_samples_split: min number of samples required to split decision node\nmin_samples_leaf: Min number of samples required to be at a leaf node\nmin_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights required to be at a leaf node\nmax_features: The number of features to consider when looking for the best split\nmax_leaf_nodes: Grow trees with max_leaf_nodes in best-first fashion\nmin_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\nclass_weight: Weights associated with classes in the form {class_label: weight}\nccp_alpha: Complexity parameter used for Minimal Cost-Complexity Pruning\nrandom_state: CControls the randomness of the estimator\n\nAlthough we will be using the model in its most basic form, using this form of machine learning requires extensive testing and model tuning in order to receive the best results.\nThe following example uses a synthetic dataset generated by myself which\nuses multiple features in order to predict whether or not a student will dine out while staying on campus.\n\n\n\n10.3.5 Generating an Example Dataset\nDo I eat out?\nCode\n\nfrom datetime import datetime\nimport random\nimport pandas as pd\nimport numpy as np\n\n\neat_out = np.random.choice([True, False], size=100000)\n\nin_class = []\n\n\nfor status in eat_out:\n    if status == True:\n       in_class.append(np.random.choice([True, False], p=[.10, .90]))\n    \n    else:\n        in_class.append(np.random.choice([False, True], p=[.40, .60]))\n\ngood_places_open = []\n\nfor status in eat_out:\n    if status == True:\n       good_places_open.append(np.random.choice([True, False], p=[.90, .10]))\n    \n    else:\n        good_places_open.append(np.random.choice([False, True], p=[.50, .50]))\n\n\namount_of_money = []\n\nfor status in eat_out:\n    if status:\n        choices = [round(random.uniform(55, 250), 2), \n        round(random.uniform(0.0, 55), 2)]\n        \n        probabilities = [.90, .10]\n        \n        dollar_amount = np.random.choice(choices, p=probabilities)\n        \n        amount_of_money.append(dollar_amount)\n    \n\n    else:\n        choices = [round(random.uniform(55, 250), 2), \n        round(random.uniform(0.0, 55), 2)]\n        \n        probabilities = [.41, .59]\n        \n        dollar_amount = np.random.choice(choices, p=probabilities)\n        \n        amount_of_money.append(dollar_amount)\n\n\ncurrent_time = []\n\nfor i in range(len(eat_out)):\n    early = random.randint(10, 19)\n    late = random.choice([20, 21, 22, 23, 0, 1, 2])\n    \n    if eat_out[i] and not in_class[i] and good_places_open[i]:\n        probabilities_t = [0.80, 0.20]\n        time_choices_hrs = [early, late]\n        \n    elif not eat_out[i] and not in_class[i] and not good_places_open[i]:\n        probabilities_t = [0.25, 0.75]\n        time_choices_hrs = [early, late]\n        \n    elif not eat_out[i] and in_class[i] and good_places_open[i]:\n        probabilities_t = [0.60, 0.40]\n        time_choices_hrs = [early, late]\n        \n    elif eat_out[i] and in_class[i]:\n        probabilities_t = [0.90, 0.10]\n        time_choices_hrs = [early, late]\n        \n    hour = np.random.choice(time_choices_hrs, p=probabilities_t)\n    minute = random.randint(0, 59)\n\n    now = datetime.now()\n    random_time = datetime(now.year, now.month, now.day, hour, minute)\n    given_time = random_time.strftime(\"%H:%M\")\n    current_time.append(given_time) \n\n\nmy_data = {'In Class?':in_class, 'Are there good places open?':good_places_open, \n           'Amount Of Money (In Dollars)':amount_of_money, \n           'Time of Day (24-Hour)':current_time, \n           'Do I eat out?':eat_out}\n\ndine_out_df = pd.DataFrame(data=my_data)\n\nThe following code takes the example dataset and attempts to use a decision tree model to predict whether or not a student is likely to dine out based on the features of:\nIn Class?: Whether or not the student is in class (Boolean)\nAre there good places open?: Asks if good restaurants are open (Boolean)\nAmount of Money (In Dollars): Money in account (Continuous)\nTime of Day (24 Hour): Time of the day when eating (Continuous)\nDo I eat out?: Asks if the student eats out (Target Label, Boolean)\n\n10.3.5.1 Decision Tree Code\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score \nfrom sklearn.metrics import precision_score\n\n\n#displays the first five rows of the dataset\ndine_out_df.head()\n\n\n\n\n\n\n\n\n\nIn Class?\nAre there good places open?\nAmount Of Money (In Dollars)\nTime of Day (24-Hour)\nDo I eat out?\n\n\n\n\n0\nFalse\nTrue\n59.09\n15:01\nTrue\n\n\n1\nTrue\nFalse\n90.13\n11:13\nTrue\n\n\n2\nFalse\nTrue\n87.06\n18:35\nTrue\n\n\n3\nFalse\nFalse\n18.29\n22:59\nFalse\n\n\n4\nFalse\nTrue\n2.39\n19:54\nTrue\n\n\n\n\n\n\n\n\n\ndef encode_time(time_str):\n    hours, minutes = map(int, time_str.split(':'))\n    \n    normalized_hours = hours / 24.0\n    \n    return pd.Series({'encoded_hours': normalized_hours})\n\ndine_out_df[['encoded_hours']] = dine_out_df['Time of Day (24-Hour)'].apply(encode_time)\n\n\nX_dtc = dine_out_df.drop(['Do I eat out?', 'Time of Day (24-Hour)'], axis=1)\nY_dtc = dine_out_df['Do I eat out?']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_dtc, Y_dtc, \ntest_size=0.30, random_state=42, shuffle=True, stratify=Y_dtc)\n\nThe code directly above using the train_test_split function is part of the training and testing phase of machine learning discussed earlier. The dataset is divided into two parts:\n\nTraining Set: This portion of the data, typically the majority (e.g., 70% of the dataset in this example), is used to train the machine learning model. For instance, the variable X_train is trained on 70% of the dataset containing features like OverTime, BusinessTravel, and MaritalStatus, while the variable y_train is trained on 70% of the Attrition feature column. The model learns the patterns and relationships between these features and the target variable (Attrition) from this training data.\nTest Set: The remaining portion of the data, 30% of the dataset in this case is set aside and not used during the training phase.\n\n\n# Creates and Trains a Decision Tree Classifier object\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, Y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\ny_pred_dtc = dtc.predict(X_test)\n\naccuracy_dtc = accuracy_score(Y_test, y_pred_dtc)\n\nprecision_dtc = precision_score(Y_test, y_pred_dtc, average='binary')\n\nrecall_dtc = recall_score(Y_test, y_pred_dtc, average='binary')\n\nf1_dtc = f1_score(Y_test, y_pred_dtc, average='binary')\n\n\nprint(f'Accuracy: {accuracy_dtc:.4f}')\nprint(f'Precision: {precision_dtc:.4f}')\nprint(f'Recall: {recall_dtc:.4f}')\nprint(f'F1 Score: {f1_dtc:.4f}')\n\nAccuracy: 0.7690\nPrecision: 0.7688\nRecall: 0.7695\nF1 Score: 0.7692\n\n\nHowever, decision trees on their own can often not be enough to make accurate choices based on the data available. Decision Trees have some pitfalls, including\n\nInstability with large amounts of data\nOverfitting\nSensitive to small variations in input data\nHigh Variance\n\nSome of these effects can be mitigated through the usage of many, many, many, decision trees. What if we had many decision trees such that the variance can be accounted for based on there existing multiple different combinations of each possible tree? What if we wanted the root of our tree to be different in order to get outcomes? What if we needed our trees to compensate for large amounts of data? Then, how about we use more than one tree? More than two? More than three? How about we use an entire forest of trees? Go big or go home. You run into the idea of using a Random Forest Algorithm.\n\n\n\n10.3.6 Random Forest\nIn simple terms, you can envision the Random Forest Algorithm as a group of decision trees combined together. Each decision tree operates independently, making its own decisions similar to a single decision tree. After training, when a data point is fed into the random forest, it passes through all the decision trees created during training. The final prediction is determined by the majority choice among all the decision trees, hopefully providing an accurate outcome. More accurately, it is an ensemble learning method (making a decision based on multiple models), which combines the predictions of multiple decision trees.\nBut how is a random forest generated? As previously stated, there are many ways to generate a decision tree, and for vast amounts of data creating an individual decision trees on it every single time can be a costly process in terms of resources and time. The roots of our trees can also differ as well. So how are these trees being made?\n\n10.3.6.1 Bagging & Bootstrapping\nBootstrapping involves making multiple datasets based on our original dataset. These new datasets are sampled with replacement from the original dataset. Each bootstrap dataset is likely to be different promoting diversity amongst the tress in the forest.\nThe process involves the random selection of a row from the original dataset and then putting it into a separate smaller dataset called the bootstrapped dataset until the number of rows in your bootstrapped dataset equals the number of rows in your original dataset.\nAdditionally, more randomness is introduced by randomly excluding columns in the bootstrap dataset during the construction of the tree. When building a decision tree on the bootstrap dataset and a split occurs, the algorithm randomly chooses to exclude one of the features from the bootstrapped dataset to introduce additional randomness. Although in the Scikit-Learn library you cannot directly specify which feature to exclude randomly, you can control the number of features excluded simultaneously.\nBagging is the technique used to aggregate the predictions of the trees in an instance of Random Forest. Each tree in the forest is trained on a bootstrap sample and makes predictions independently. The final prediction of the Random Forest is obtained by taking the majority (in this case majority classification) across all of the trees.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#random-forest-code",
    "href": "supervised.html#random-forest-code",
    "title": "10  Supervised Learning",
    "section": "10.4 Random Forest Code",
    "text": "10.4 Random Forest Code\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom datetime import datetime\nimport random\n\n\nX_rf = dine_out_df.drop(['Do I eat out?', 'Time of Day (24-Hour)'], axis=1)\nY_rf = dine_out_df['Do I eat out?']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_rf, Y_rf, \ntest_size=0.30, random_state=42, shuffle=True, stratify=Y_rf)\n\nAlthough the random forest model shown below is only the default hyperparameters, we can tune the following hyperparameters to have our model more closely follow the data presented:\n\nn_estimators: Number of trees in forest\ncriterion: “Gini”, “Entropy”, “log_loss”\nmax_depth: Maximum depth of the tree\nmin_samples_split: min number of samples required to split decision node\nmin_samples_leaf: Min number of samples required to be at a leaf node\nmin_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights required to be at a leaf node\nmax_features: The number of features to consider when looking for the best split\nmax_leaf_nodes: Grow trees with max_leaf_nodes in best-first fashion\nmin_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\nboostrap: Whether bootstrap samples are used when building trees\noob_score: Whether to use out-of-bag samples to estimate the generalization score\nn_jobs: The number of jobs to run in parallel\nrandom_state: Controls both the randomness of the bootstrapping of the samples used when building trees\nVerbose: Controls the verbosity when fitting and predicting\n\nWhat you’ll notice is that besides some new parameters, many of the parameters of the random forest classifier are the same as the parameters of the decision tree classifier. This is of course because the random forest is an ensemble of decision trees.\n\nrf_classifier = RandomForestClassifier() \n\nrf_classifier.fit(X_train, Y_train)\n\nY_pred_rf = rf_classifier.predict(X_test)\n\n\naccuracy_rf = accuracy_score(Y_test, Y_pred_rf)\n\nprecision_rf = precision_score(Y_test, Y_pred_rf, average='binary')\n\nrecall_rf = recall_score(Y_test, Y_pred_rf, average='binary')\n\nf1_rf = f1_score(Y_test, Y_pred_rf, average='binary')\n\n\nprint(f'Accuracy: {accuracy_rf:.4f}')\nprint(f'Precision: {precision_rf:.4f}')\nprint(f'Recall: {recall_rf:.4f}')\nprint(f'F1 Score: {f1_rf:.4f}')\n\nAccuracy: 0.7794\nPrecision: 0.7764\nRecall: 0.7851\nF1 Score: 0.7807\n\n\nIn comparison, we see that even without hyperparameter tuning the random forest model outperforms the decision tree model that we have previously trained.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#references",
    "href": "supervised.html#references",
    "title": "10  Supervised Learning",
    "section": "10.5 References",
    "text": "10.5 References\n\nTree Based Models - UCONN Data Science\nDecision Tree Classification Clearly Explained!\n\nhttps://www.youtube.com/watch?v=ZVR2Way4nwQ\n\nWhat is Random Forest? - IBM\n\nhttps://www.ibm.com/topics/random-forest#:~:text=Random%20forest%20is%20a%20commonly,both%20classification%20and%20regression%20problems.\n\nsklearn.ensemble.RandomForestClassifier - Scikit Learn Documentation\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nsklearn.tree.DecisionTreeClassifier - Scikit Learn Documentation\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n\nGini Index and Entropy|Gini Index and Information gain in Decision Tree| Decision tree splitting rule\n\nhttps://www.youtube.com/watch?v=-W0DnxQK1Eo\n\nRandom Forest | ScienceDirect\n\nhttps://www.sciencedirect.com/topics/engineering/random-forest\n\nBootstrapping and OOB samples in Random Forests | Medium\n\nhttps://medium.com/analytics-vidhya/bootstrapping-and-oob-samples-in-random-forests-6e083b6bc341",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#boosted-tree",
    "href": "supervised.html#boosted-tree",
    "title": "10  Supervised Learning",
    "section": "10.6 Boosted Tree",
    "text": "10.6 Boosted Tree\nBoosted trees are a powerful ensemble technique in machine learning that combines multiple weak learners, typically decision trees, to form a strong learner. The essence of the boosting approach is to fit sequential models, where each new model attempts to correct errors made by the previous ones. Gradient boosting, one of the most popular boosting methods, optimizes a loss function over weak learners by iteratively adding trees that correct the residuals of the combined ensemble.\n\n10.6.1 Introduction\nBoosted trees build on the concept of boosting, an ensemble technique that aims to create a strong classifier from a number of weak classifiers. In the context of boosted trees, the weak learners are decision trees, usually of a fixed size, which are added sequentially to the model. The key idea is to improve the model iteratively by focusing on examples that are hard to predict, thus enhancing the overall predictive accuracy of the model.\n\n\n10.6.2 Boosting Process\nIn the context of gradient boosted trees, the ensemble model is built iteratively, where each new tree is fitted to the residual errors made by the previous ensemble of trees. The aim is to minimize a loss function, and each tree incrementally improves the model by reducing the loss. The ensemble model after \\(M\\) trees can be described more accurately as:\nThe final predictive model is \\(F_M(x)\\), which represents the accumulated predictions of all \\(M\\) trees, adjusted by their respective learning rates.\nThe process can be delineated as follows:\n\nInitialization: Begin with a base model \\(F_0(x)\\), often the mean of the target variable for regression or the log odds for classification.\nIterative Boosting:\n\nAt each stage \\(m\\), compute the pseudo-residuals, which are the gradients of the loss function with respect to the predictions of the current model, \\(F_{m-1}(x)\\).\nFit a new tree, \\(h_m(x)\\), to these pseudo-residuals.\nUpdate the model by adding this new tree, weighted by a learning rate \\(\\lambda\\), to minimize the loss, resulting in \\(F_m(x) =\nF_{m-1}(x) + \\lambda \\cdot h_m(x)\\).\n\nFinal Model: After \\(M\\) iterations, the ensemble model \\(F_M(x)\\) represents the sum of the initial model and the incremental improvements made by each of the \\(M\\) trees, where each tree is fitted to correct the residuals of the model up to that point.\n\nKey Concepts\n\nLoss Function (\\(L\\)): A measure of how far the ensemble’s predictions are from the actual values. Common choices include the squared error for regression and logistic loss for classification.\nLearning Rate (\\(\\lambda\\)): A small positive number that scales the contribution of each tree. It helps in controlling the speed of learning and prevents overfitting.\n\nThis iterative approach, focusing on correcting the errors of the ensemble up to the current step, distinguishes gradient boosting from other ensemble methods and allows for a nuanced adjustment of the model to the data.\n\n\n10.6.3 Boosting vs Bagging in Ensemble Learning\nBoosting and bagging are foundational ensemble learning techniques in machine learning, designed to improve the accuracy of predictive models by combining the strengths of multiple weaker models. Despite their common goal, they differ significantly in methodology and application.\nBoosting builds models in a sequential manner, focusing each subsequent model on correcting the errors made by the previous ones. The process initiates with a base model, with each new model added aiming to correct its predecessor, culminating in a weighted sum of all models.\n\nSequential Model Building: Models are built sequentially, each correcting the error of the ensemble before it.\nFocus on Misclassification: Boosting prioritizes instances misclassified by earlier models, adapting to the “harder” cases.\nVariance Reduction: It mainly reduces model variance, carefully avoiding overfitting despite increasing complexity.\n\nExamples include AdaBoost, Gradient Boosting, and XGBoost.\nBagging, or Bootstrap Aggregating, constructs multiple models independently and combines their predictions through averaging or majority voting. Each model is trained on a randomly drawn subset of the data, allowing for parallel model construction.\n\nParallel Model Building: Models are built independently, enabling parallelization.\nUniform Attention: All instances are equally considered, with the diversity of the ensemble reducing variance.\nBias and Variance Reduction: Effective at reducing both, but particularly good at cutting down variance in complex models.\n\nRandom Forest is a prominent example of bagging.\n\n10.6.3.1 Differences\n\nModel Dependency: Boosting’s models are interdependent, building upon the errors of their predecessors, unlike bagging’s independent models.\nError Correction Focus: Boosting directly targets previous errors, while bagging reduces error through diversity.\nComputational Complexity: Boosting can be more computationally intensive due to its sequential nature.\nOverfitting Risk: Boosting may overfit on noisy data, whereas bagging remains robust, especially as the number of models increases.\n\nUnderstanding the distinctions between boosting and bagging is crucial for selecting the appropriate ensemble method for specific machine learning tasks. Both strategies offer unique advantages and can be applied based on the problem, data characteristics, and desired outcomes.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#handling-imbalanced-data",
    "href": "supervised.html#handling-imbalanced-data",
    "title": "10  Supervised Learning",
    "section": "10.7 Handling Imbalanced Data",
    "text": "10.7 Handling Imbalanced Data\nHandling imbalanced data is a critical task in machine learning, particularly in classification problems where the distribution of instances across the known classes is uneven. This imbalance can significantly bias the model’s training process, leading to poor predictive performance, especially for the minority class. Understanding and addressing this issue is crucial for building effective and fair models.\n\n10.7.1 Imbalanced Data\nImbalanced data refers to a situation where the number of observations in one class significantly outweighs those in one or more other classes. This is a common scenario in various domains:\n\nIn fraud detection, legitimate transactions vastly outnumber fraudulent ones.\nIn medical diagnosis, the dataset might have more negative results (no disease) than positive results (disease presence).\nIn email filtering, spam emails are less common than non-spam.\n\nSuch disparities can lead to models that are overly biased towards the majority class, as they tend to optimize overall accuracy by simply predicting the majority class.\n\n\n10.7.2 Approaches to Handling Imbalanced Data\n\nResampling Techniques:\n\nOversampling Minority Class: Enhancing the representation of the minority class by replicating instances or generating synthetic samples. SMOTE (Synthetic Minority Over-sampling Technique) is a popular method for creating synthetic samples (Chawla et al. 2002).\nUndersampling Majority Class: Reducing the size of the majority class to balance the dataset. Careful selection or clustering methods are employed to retain information (Liu, Wu, and Zhou 2008).\n\nCost-sensitive Learning: Adjusting the classification cost to make errors on the minority class more impactful than errors on the majority class. This approach often involves modifying the algorithm to be more sensitive to the minority class (Elkan 2001).\nEnsemble Methods: Leveraging multiple models to improve classification, particularly of the minority class. Techniques such as Balanced Random Forest (Chen, Liaw, and Breiman 2004) and AdaBoost (Sun, Wong, and Kamel 2007) have been adapted for imbalanced datasets.\nAlgorithmic Adjustments: Some algorithms have built-in mechanisms for dealing with imbalanced data, such as adjusting class weights. This is evident in algorithms like SVM and logistic regression, where class weights can be inversely proportional to class frequencies (King and Zeng 2001).\n\n\n\n10.7.3 SMOTE\nThe Synthetic Minority Over-sampling Technique (SMOTE) offers a nuanced approach to mitigating the challenges posed by imbalanced datasets. Unlike simple oversampling techniques that replicate minority class instances, SMOTE generates synthetic samples in a feature space, enhancing the diversity and representation of the minority class without directly copying existing observations (Chawla et al. 2002).\n\n10.7.3.1 Algorithm\nSMOTE’s algorithm can be succinctly described using the following steps for each minority class sample \\(x\\):\n\nIdentify the \\(k\\) nearest neighbors in the minority class for \\(x\\).\nRandomly choose one of these \\(k\\) neighbors, call it \\(x_{\\text{nn}}\\).\nGenerate a synthetic sample \\(x_{\\text{new}}\\) along the line connecting \\(x\\) and \\(x_{\\text{nn}}\\). Mathematically, \\(x_{\\text{new}} = x + \\lambda (x_{\\text{nn}} - x)\\), where \\(\\lambda\\) is a random number between \\(0\\) and \\(1\\).\n\nThis process is repeated until the class distribution is deemed balanced.\nSeveral software tools and libraries have made implementing SMOTE straightforward in various programming environments. In Python, the imbalanced-learn library provides an efficient and flexible implementation of SMOTE and its variants, allowing seamless integration with scikit-learn workflows. R users can utilize the DMwR package, which includes functions for applying SMOTE to datasets. These tools not only offer basic SMOTE functionality but also support advanced variants and allow for easy experimentation with different oversampling strategies.\n\n\n10.7.3.2 Applications, Considerations, and Variants\nApplications: SMOTE’s approach to enriching datasets by synthesizing new examples has proven beneficial in various fields. For instance, in fraud detection, where fraudulent transactions are rare (Dal Pozzolo et al. 2015), and in medical diagnostics, where certain conditions may be underrepresented in datasets (Johnson and Khoshgoftaar 2019).\nConsiderations: While SMOTE enhances the representation of the minority class, it may also lead to overfitting, especially if the minority class is dispersed or if there’s significant overlap with the majority class (Guo et al. 2017). Choosing the right variant of SMOTE and adjusting its parameters requires careful consideration of the dataset’s characteristics.\nAdvanced Variants: Specific challenges in datasets have led to the development of SMOTE variants like Borderline-SMOTE, which generates samples closer to the decision boundary to improve classifier sensitivity to the minority class (Han, Wang, and Mao 2005). Each variant addresses particular issues, such as noise or the risk of generating ambiguous synthetic samples.\n\n\n10.7.3.3 Conclusion\nSMOTE and its variants offer a sophisticated approach to mitigating the challenges posed by imbalanced datasets. Through synthetic sample generation, these methods enhance the diversity and representation of the minority class, facilitating improved model accuracy and generalizability. The choice of variant and parameters should be guided by the dataset’s characteristics and the problem context, underscoring the importance of careful experimentation and validation.\n\n\n\n\nChawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002. “SMOTE: Synthetic Minority over-Sampling Technique.” Journal of Artificial Intelligence Research 16: 321–57.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest to Learn Imbalanced Data.” University of California, Berkeley.\n\n\nDal Pozzolo, Andrea, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. 2015. “Calibrating Probability with Undersampling for Unbalanced Classification.” In 2015 IEEE Symposium Series on Computational Intelligence, 159–66. IEEE.\n\n\nElkan, Charles. 2001. “The Foundations of Cost-Sensitive Learning.” In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, 973–78. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nGuo, Haixiang, Yijing Li, Jennifer Shang, Mingyun Gu, Yuanyue Huang, and Bing Gong. 2017. “Learning from Class-Imbalanced Data: Review of Methods and Applications.” Expert Systems with Applications 73: 220–39.\n\n\nHan, Hui, Wen-Yuan Wang, and Bing-Huan Mao. 2005. “Borderline-SMOTE: A New over-Sampling Method in Imbalanced Data Sets Learning.” In International Conference on Intelligent Computing, 878–87. Springer.\n\n\nJohnson, Justin M., and Taghi M. Khoshgoftaar. 2019. “Survey on Deep Learning with Class Imbalance.” Journal of Big Data 6 (1): 27.\n\n\nKing, Gary, and Langche Zeng. 2001. “Logistic Regression in Rare Events Data.” Political Analysis 9 (2): 137–63.\n\n\nLiu, Xu-Ying, Jianxin Wu, and Zhi-Hua Zhou. 2008. “Exploratory Undersampling for Class-Imbalance Learning.” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 39 (2): 539–50.\n\n\nSun, Yanmin, Andrew KC Wong, and Mohamed S Kamel. 2007. “Cost-Sensitive Boosting for Classification of Imbalanced Data.” Pattern Recognition 40 (12): 3358–78.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "11  Exercises",
    "section": "",
    "text": "Git basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix retult from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\nThe NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.\n\nNYC Rodents Rats in NYC are widespread, as they are in many densely populated areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. In the 311 Service Request Data, there is one complain type Rodent. Extract all the requests with complain type Rodent, created between January 1, 2022 and December 31, 2023. Save them into a csv file named rodent_2022-2023.csv.\n\nAre there any complains that are not closed yet?\nAre there any complains with a closed data before the created date?\nHow many agencies were this complain type reported to?\nSummarize the missingess for each variable.\nSummarize a frequency table for the descriptor variable, and summarize a cross table by year.\nWhich types of ‘DESCRIPTOR’ do you think should be included if our interest is rodent sighting?\nTake a subset of the data with the descriptors you chose and summarize the response time by borough.\n\nNYC rodent sightings data cleaning The data appears to need some cleaning before any further analysis. Some missing values could be filled based on other columns.\n\nChecking all 47 column names suggests that some columns might be redundant. Identify them and demonstrate the redundancy.\nAre zip code and borough always missing together? If geocodes are available, use reverse geocoding to fill the zip code.\nExport the cleaned data in both csv and feather format. Comment on the file sizes.\n\nSQL Practice on NYC rodent sightings The NYC rodent sightings data that we prepared could be stored more efficiently using a database. Let us start from the csv file you exported from the last problem.\n\nCreate a table called rodent from the csv file.\nThe agency and agency_name columns are redundant in the table. Create a table called agency, which contains only these two columns, one agency a row.\nDrop the agency_name name from the rodent table. Justify why we do not need it here.\nComment on the sizes of the table (or exported csv file) of rodent before and after dropping the agency_name column.\nCome up with a scheme for the two tables that allows even more efficient storage of the agency column in the rodent table. _Hint: use an integer to code the agencies.\n\nLogistic Modeling The response time to 311 service requests is a measure of civic service quality. Let us model the response time to 311 requests with complain type Rodent.\n\nCompute the response time in hours. Note that some response will be missing because of unavailable closed date.\nCompute a binary variable over3d, which is one if the response time is greater than 3 days, and zero otherwise. Note that this variable should have no missing values.\nUse the package uszipcode to obtain the zip code level covaraites such as median house income and median home value. Merge these variables to the rodent data.\nSplit the data at random into training (80%) and testing (20%). Build a logistic model to predict over3d on the training data, and validate the performance on the testing data.\nBuild a lasso logistic model to predict over3d, and justify your choice of the tuning parameter. Validate on the testing data.\n\nMidterm Project: Rodents in NYC Rodents in NYC are widespread, as they are in many densely populated areas. As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. Rat sightings in NYC was analyzed by Dr. Michael Walsh in a 2014 PeerJ article. We investigate this problem from a different angle with the NYC Rodent Inspection data, provided by the Department of Health and Mental Hygiene (DOHMH). Download the 2022-2023 data by filtering the INSPECTION_DATE to between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024 and INSPECTION_TYPE is either Initial or Compliance (which should be about 108 MB). Read the meta data information to understand the data.\n\nData cleaning.\n\nThere are two zipcode columns: ZIP_CODE and Zipcodes. Which one represent the zipcode of the inspection site? Comment on the data dictionary.\nSummarize the missing information. Are their missing values that can be filled using other columns? Fill them if yes.\nAre their redundant information in the data? Try storing the data using arrow and comment on the efficiency gain.\nAre there invalid zipcode or borough? Justify and clean them up if yes.\n\nData exploration.\n\nCreate binary variable passing indicating passing or not for the inspection result. Does passing depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.\nAre the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.\nIf we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passting rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.\nPerform a formal hypothesis test to confirm the observations from your visualization.\n\nData analytics.\n\nAggregate the inspections by zip code to create a dataset with five columns. The first three columns are zipcode; n_initial, the count of the initial inspections in that zipcode; and n_initpass, the number of initial inspections with a passing result in that zipcode. The other two variables are n_compliance and n_comppass, the counterpart for compliance inspections.\nAdd a variable to your dataset, n_sighting, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.\nMerge your dataset with the simple zipcode table in package uszipcode by zipcode to obtain demographic and socioeconomic variables at the zipcode level.\nBuild a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.\n\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002.\n“SMOTE: Synthetic Minority over-Sampling\nTechnique.” Journal of Artificial Intelligence Research\n16: 321–57.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest\nto Learn Imbalanced Data.” University of California, Berkeley.\n\n\nDal Pozzolo, Andrea, Olivier Caelen, Reid A Johnson, and Gianluca\nBontempi. 2015. “Calibrating Probability with Undersampling for\nUnbalanced Classification.” In 2015 IEEE Symposium Series on\nComputational Intelligence, 159–66. IEEE.\n\n\nElkan, Charles. 2001. “The Foundations of Cost-Sensitive\nLearning.” In Proceedings of the Seventeenth International\nJoint Conference on Artificial Intelligence, 973–78. San Francisco,\nCA, USA: Morgan Kaufmann Publishers Inc.\n\n\nGuo, Haixiang, Yijing Li, Jennifer Shang, Mingyun Gu, Yuanyue Huang, and\nBing Gong. 2017. “Learning from Class-Imbalanced Data: Review of\nMethods and Applications.” Expert Systems with\nApplications 73: 220–39.\n\n\nHan, Hui, Wen-Yuan Wang, and Bing-Huan Mao. 2005.\n“Borderline-SMOTE: A New over-Sampling Method in Imbalanced Data\nSets Learning.” In International Conference on Intelligent\nComputing, 878–87. Springer.\n\n\nJohnson, Justin M., and Taghi M. Khoshgoftaar. 2019. “Survey on\nDeep Learning with Class Imbalance.” Journal of Big Data\n6 (1): 27.\n\n\nKearns, Michael, and Aaron Roth. 2019. The Ethical Algorithm: The\nScience of Socially Aware Algorithm Design. Oxford University\nPress.\n\n\nKing, Gary, and Langche Zeng. 2001. “Logistic Regression in Rare\nEvents Data.” Political Analysis 9 (2): 137–63.\n\n\nLiu, Xu-Ying, Jianxin Wu, and Zhi-Hua Zhou. 2008. “Exploratory\nUndersampling for Class-Imbalance Learning.” IEEE\nTransactions on Systems, Man, and Cybernetics, Part B (Cybernetics)\n39 (2): 539–50.\n\n\nNoble, Safiya Umoja. 2018. Algorithms of Oppression: How Search\nEngines Reinforce Racism. New York University Press.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data\nIncreases Inequality and Threatens Democracy. Crown.\n\n\nSaltz, Jeffrey S., and Neil Dewar. 2019. “Data Science Ethical\nConsiderations: A Systematic Literature Review and Proposed\nProject Framework.” Ethics and Information Technology 21\n(3): 197–208.\n\n\nSun, Yanmin, Andrew KC Wong, and Mohamed S Kamel. 2007.\n“Cost-Sensitive Boosting for Classification of Imbalanced\nData.” Pattern Recognition 40 (12): 3358–78.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]