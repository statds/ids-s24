[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-s24.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2023 and Spring 2024 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nOur mid-term project on rat sightings in New York City is to be showcased in a virtual session at the NYC Open Data Week, 2024, the week following the Spring Break. Four students will be selected to present topics on the mid-term project.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Add yours through a pull request; note the syntax of nested list in Markdown.\n\nChugh, Charitarth\n\nGet better at analyzing data/features\nLearn about more xgboost & gradient boosted trees.\n\nDennison, Jack\n\nLearn how to use Git and GitHub\nBe able to apply my skills in Python and Git to data analytics tasks\n\nElliott, Matt\n\nFaciliate myself into becoming a Data Scientist\nLearn new skills such as Quarto and GitHub\n\nLee, Joshua\n\nImprove model optimization techniques\nlearn how to conduct better feature engineering\nlearn how to perform better model selection and feature selection\nlearn how to deploy ml models and processes to the cloud\n\nMori, Abigail\n\nBecome proficient using Git\nLearn how to properly communiacte statistical evidence and findings\n\nMassad, Olivia\n\nBe able to use Git effectively\nGain knowledge about Data Science and its importance\n\nNguyen, Leon\n\nBecome proficient in utilizing Git and GitHub workflow processes\nDevelop proficiency in Quarto and Python packages\nCreate a data science project start to finish for portfolio work\n\nPatel, Pratham\n\nBecome more proficient and efficient with GitHub and Python\nGet a deeper understanding and appreciate of the Data Science workflow\nUnderstand collaboration and project creation on GitHub\n\nPerez, Isabelle\n\nBecome comfortable working with git and quarto\nLearn data management strategies and the relevant programming skills\n\nPugh, Alex\n\nIncrease my knowledge of Git and Python\nLearn to efficiently clean a data set\n\nQualls, William\n\nBetter understand the Data Science Pipeline\nGain practical knowledge with tools such as Github that aren’t covered in other classes\n\nSchober, Henry\n\nBe more proficient in Git and Python\nDeepen my understanding of Data Science\n\nTaki, William\n\nGet comfortable with Git and Python\nUse the learnings from this class to help with STAT 33494W\n\nWoo, Madison\n\nBe able to comfortably use Git and Python\nLearn about project managment and data science\n\nXie, Vincent\n\nBecome more proficient with Git.\nLearn how to create a proper data science project.\nBe introduced to core concepts in data science.\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\nYankson, Emmanuel\n\nGet better with python\nGet an A in STAT 3255\n\nZhang, Xingye\n\nGet better with computers.\nGet an A in STAT 3255.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\nrandom.seed(4737 + 8852 + 3196 + 2344 + 47) # jointly set by the class on 01/24/2024\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Elliott,Matt A',\n 'Wu,Weijia',\n 'Lek,Victor Khun',\n 'Taki,William Hiroyasu',\n 'Schober,Henry',\n 'Lee,Joshua Jian',\n 'Patel,Pratham Subhas',\n 'Li,Ge',\n 'Zhang,Xingye',\n 'Dennison,Jack Thomas',\n 'Massad,Olivia Grace',\n 'Perez,Isabelle Daenerys Halpine',\n 'Yankson,Emmanuel Opoku',\n 'Li,David',\n 'Mori,Abigail Kanoelani Shim',\n 'Nguyen,Leon Duc',\n 'Pugh,Alex',\n 'Chugh,Charitarth',\n 'Xie,Vincent',\n 'Vijayaraghavendra,Jyothsna',\n 'Qualls,William Wayne',\n 'Woo,Madison Nicole',\n 'Hook,Braedon',\n 'Chowaniec,Amelia Elizabeth']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nDescriptive statistics\nStatistical hypothesis tests\nStatistical modeling\nData visualization\nAccessing census and ACS data\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/07\nMatt Elliott\nData science communication skills\n\n\n02/07\nWeijia Wu\n\n\n\n02/12\nDr. Haim Bar\nDatabase management\n\n\n02/14\nWillam Taki\n\n\n\n02/19\nJoshua Lee\nDescriptive Statistics\n\n\n02/21\nPratham Patel\nHandling spatial data with geopandas\n\n\n02/21\nOlivia Massad\nGrammar of Graphics plotnine\n\n\n02/26\nGe Li\n\n\n\n02/26\nXingye Zhang\n\n\n\n02/28\nJack Dennison\nGeographic Data Analysis\n\n\n02/28\nIsabelle Perez\nStatistical hypothesis tests scypy.stats\n\n\n03/02\nEmmanuel Yankson\nRandom Forest\n\n\n03/02\nDavid Li\n\n\n\n03/04\nAbigail Mori\nHandling spatial data with geopandas\n\n\n03/04\nLeon Nguyen\nStatistical Modeling with statsmodels\n\n\n03/25\nAlex Pugh\n\n\n\n03/25\nCharitath Chugh\nPyTorch\n\n\n03/27\nVincent Xie\nDatabase Operations with SQL\n\n\n03/27\n\n\n\n\n04/01\nWilliam Qualls\nWeb Scraping\n\n\n04/01\nMadison Woo\nData Science Ethics\n\n\n04/03\nBraedon Hook\n\n\n\n04/03\n\n\n\n\n04/08\n\n\n\n\n04/08\n\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/15\nMatt Elliott; Weijia Wu; William Taki; Joshua Lee; Pratham Patel\n\n\n04/17\nOlivia Massad; Ge Li; Xingye Zhang; Jack Dennison; Isabelle Perez\n\n\n04/22\nEmmanual Yankson; Davi Li; Abigail Mori; Leon Nguyen; Alex Pugh\n\n\n04/24\nCharitath Chugh; Vincent Xie; Madison Woo; Braedon Hook\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nA template for homework is in this repo: hwtemp.qmd.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-2.5348863 ,  2.1189087 ,  4.83327938,  6.44154669,  5.57171101,\n        4.25033713, -1.46731659,  2.17816408,  9.91171522, -3.2258789 ])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.05244999, 0.09969151, 0.07760749, 0.05384142, 0.0669444 ,\n       0.08513786, 0.06849947, 0.09963669, 0.01410347, 0.04248244])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n7.56 µs ± 312 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.83 µs ± 56.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n52.1 µs ± 1.4 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 243, 'switch': 370}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n5449703168\n5449703168\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4407640272\n4517736560\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Data Import/Export",
    "section": "",
    "text": "5.1 Using the Pandas Package\nThe pandas library simplifies data manipulation and analysis. It’s especially handy for dealing with CSV files.\nimport pandas as pd\n\n# Define the file name\ncsvnm = \"data/rodent_2022-2023.csv\"\n\n# Specify the strings that indicate missing values\n# Q: How would you know these?\nna_values = [\n    \"\",\n    \"0 Unspecified\",\n    \"N/A\",\n    \"na\",\n    \"na na\",\n    \"Unspecified\",\n    \"UNKNOWN\",\n]\n\ndef custom_date_parser(x):\n    return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n\n# Read the CSV file\ndf = pd.read_csv(\n    csvnm,\n    na_values = na_values,\n    parse_dates = ['Created Date', 'Closed Date'], \n    date_parser = custom_date_parser,\n    dtype = {'Latitude': 'float32', 'Longitude': 'float32'},\n)\n\n# Strip leading and trailing whitespace from the column names\ndf.columns = df.columns.str.strip()\ndf.columns = df.columns.str.replace(' ', '_', regex = False).str.lower()\n\n# Drop the 'Location' since it is redundant\n# df.drop(columns=['Location'], inplace=True)\nThe pandas package also provides some utility functions for quick summaries about the data frame.\ndf.shape\ndf.describe()\ndf.isnull().sum()\n\nunique_key                            0\ncreated_date                          0\nclosed_date                        2787\nagency                                0\nagency_name                           0\ncomplaint_type                        0\ndescriptor                            0\nlocation_type                         0\nincident_zip                          0\nincident_address                      0\nstreet_name                           0\ncross_street_1                       90\ncross_street_2                       55\nintersection_street_1               104\nintersection_street_2                69\naddress_type                          0\ncity                               1384\nlandmark                           3820\nfacility_type                     82869\nstatus                                2\ndue_date                          82869\nresolution_description             2787\nresolution_action_updated_date     2787\ncommunity_board                       8\nbbl                                2914\nborough                               8\nx_coordinate_(state_plane)          520\ny_coordinate_(state_plane)          520\nopen_data_channel_type                0\npark_facility_name                82869\npark_borough                          8\nvehicle_type                      82869\ntaxi_company_borough              82869\ntaxi_pick_up_location             82869\nbridge_highway_name               82869\nbridge_highway_direction          82869\nroad_ramp                         82869\nbridge_highway_segment            82869\nlatitude                            520\nlongitude                           520\nlocation                            520\nzip_codes                           622\ncommunity_districts                 526\nborough_boundaries                  526\ncity_council_districts              526\npolice_precincts                    526\npolice_precinct                     526\ndtype: int64\nWhat are the unique values of descriptor?\ndf.descriptor.unique()\n\narray(['Rat Sighting', 'Mouse Sighting', 'Condition Attracting Rodents',\n       'Signs of Rodents', 'Rodent Bite - PCS Only'], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#filling-missing-values",
    "href": "import.html#filling-missing-values",
    "title": "5  Data Import/Export",
    "section": "5.2 Filling Missing Values",
    "text": "5.2 Filling Missing Values\nIf geocodes are available but zip code is missing, we can use reverse geocoding to fill the zip code. This process involves querying a geocoding service with latitude and longitude to get the corresponding address details, including the ZIP code. This can be done with package geopy, which needs to be installed first: pip install geopy.\n\nimport pandas as pd\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut, GeocoderServiceError\n\n# Initialize the geocoder\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n\n# Function for reverse geocoding\ndef reverse_geocode(lat, lon):\n    try:\n        location = geolocator.reverse((lat, lon), exactly_one=True)\n        address = location.raw.get('address', {})\n        zip_code = address.get('postcode')\n        return zip_code\n    except (GeocoderTimedOut, GeocoderServiceError):\n        # Handle errors or timeouts\n        return None\n\n# Apply reverse geocoding to fill missing ZIP codes\nfor index, row in df.iterrows():\n    if pd.isnull(row['incident_zip']) and pd.notnull(row['latitude']) and pd.notnull(row['longitude']):\n        df.at[index, 'incident_zip'] = reverse_geocode(row['latitude'], row['longitude'])\n\n# Note: This can be slow for large datasets due to API rate\n# limits and network latency",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#using-appache-arrow-library",
    "href": "import.html#using-appache-arrow-library",
    "title": "5  Data Import/Export",
    "section": "5.3 Using Appache Arrow Library",
    "text": "5.3 Using Appache Arrow Library\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R.\n\ndf.to_feather('data/rodent_2022-2023.feather')\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/rodent_2022-2023.feather\")\ndff.shape\n\n(82869, 47)\n\n\nBenefits of Using Feather:\n\nEfficiency: Feather is designed to support fast reading and writing of data frames, making it ideal for analytical workflows that need to exchange large datasets between Python and R.\nCompatibility: Maintains data type integrity across Python and R, ensuring that numbers, strings, and dates/times are correctly handled and preserved.\nSimplicity: The API for reading and writing Feather files is straightforward, making it accessible to users with varying levels of programming expertise.\n\nBy using Feather format for data storage, you leverage a modern approach optimized for speed and compatibility, significantly enhancing the performance of data-intensive applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "import.html#accessing-the-census-data-with-uszipcode",
    "href": "import.html#accessing-the-census-data-with-uszipcode",
    "title": "5  Data Import/Export",
    "section": "5.4 Accessing the Census Data with uszipcode",
    "text": "5.4 Accessing the Census Data with uszipcode\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the incident_zip column is clean and consistent.\n\nprint(df['incident_zip'].isnull().sum())\n# Standardize to 5-digit codes, if necessary\ndf['incident_zip'] = df['incident_zip'].astype(str).str.zfill(5) \n\n0\n\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment by pip install uszipcode.\nNow let’s work on the rodent sightings data.\nWe will first clean the incident_zip column to ensure it only contains valid ZIP codes. Then, we will use a vectorized approach to fetch the required data for each unique ZIP code and merge this information back into the original DataFrame.\n\n# Remove rows where 'incident_zip' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['incident_zip']).copy()\nvalid_zip_df['incident_zip'] = valid_zip_df['incident_zip'].astype(str).str.zfill(5)\nunique_zips = valid_zip_df['incident_zip'].unique()\n\nSince uszipcode doesn’t inherently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple times.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"incident_zip\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'incident_zip'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"incident_zip\")\n\nmerged_df.columns\n\nIndex(['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',\n       'complaint_type', 'descriptor', 'location_type', 'incident_zip',\n       'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n       'intersection_street_1', 'intersection_street_2', 'address_type',\n       'city', 'landmark', 'facility_type', 'status', 'due_date',\n       'resolution_description', 'resolution_action_updated_date',\n       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',\n       'y_coordinate_(state_plane)', 'open_data_channel_type',\n       'park_facility_name', 'park_borough', 'vehicle_type',\n       'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name',\n       'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment',\n       'latitude', 'longitude', 'location', 'zip_codes', 'community_districts',\n       'borough_boundaries', 'city_council_districts', 'police_precincts',\n       'police_precinct', 'median_home_value', 'median_household_income'],\n      dtype='object')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Import/Export</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "6  Communicating Data Science",
    "section": "",
    "text": "6.1 Data Science Communication Skills\nThis section was written by Matt Elliott.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communicating Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#data-science-communication-skills",
    "href": "communication.html#data-science-communication-skills",
    "title": "6  Communicating Data Science",
    "section": "",
    "text": "6.1.1 Introduction\nHi! My name is Matt Elliott and I decided to start this presentation off by introducing myself and what I hope to become moving forward! I am currently a Senior aiming to graduate in Fall 2024 with a Bachelor’s of Arts in Individualized Data Science under the CLAS’ IISP program. My primary advisor is our professor Jun Yan! Moving forward, I hope to learn valuable skill sets and ideas from both this course and the Data Science field. Even using Quarto instead of typical Google Slides is a step for me in creating new skills! The topic I chose to discuss today in class is “Data Science Communication Skills” . I find this to be one of the most crucial topics to discuss about Data Science; since Data Scientists are often the glue that keeps projects, companies, and ideas together.\n\n\n6.1.2 Describing Data Science and its Rise\n\nAccording to IBM, Data Science “combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI), and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data.\nThese insights are then used to “guide decision making” and create “strategic planning”\nAccording to the U.S. Bureau of Labor Statistics, Data Science is\n“projected to grow 35 percent from 2022 to 2032, much faster than the average for all occupations”\nThe median annual pay for Data Scientists in May 2022 was around $103,500, showing its high demand in a monetary light\n\n\n\n6.1.3 Why does Communication Matter?\n\nCommunication matters in any professional setting in our lives, and especially in a field that can be extremely confusing and perplexing to those who are viewing it from an outside perspective.\nThe Data Incubator states that “You may work alongside data analysts or other scientists as part of a team, especially when handling large datasets or working on big projects. Beyond this, you may also frequently work with other teams of professionals who don’t work with data. Thus, it’s essential to be an excellent communicator to work with others effectively”. This is an important idea as being able to be a cooperative person will create partnerships that flow in the correct manner.\nData Scientists often present insights to other partners in order to facilitate goals and achievements in a professional setting.\nKaren Church for Medium writes that “Communication enables data scientists to gather all the necessary information, clarify needs and expectations of stakeholders, and align their work with broader business goals.”\n\n\n\n6.1.4 Inherent Communication Skills\n\nUsing the right communication methods\nFriendliness\nConfidence\nVolume and tone\nEmpathy\nRespect\nCues\n\n\n\n6.1.5 Identify your audience\n\nThe Data Incubator states that “Transferring knowledge across departments is crucial, so it’s vital to share insights and analyses in simple, clear terms that don’t overwhelm individuals with jargon or technical details.”\nIdentifying your audience and speaking their language is an important step, as it can vary to low familiarity to full comprehension of the topic\nIn a 2018 HBR.org article, Hugo Bowne-Anderson interviewed 35 data scientists on his podcast and found that their main issues were: “lack of management/financial support,” “lack of clear questions to answer,” “results not used by decision makers,” and “explaining data science to others.” from Harvard Business Review\nKnowing your audience in this situation can cover your back on how Data Scientists are treated in the field, communicating creates cooperation that can lead to the avoidance of these issues found.\n\n\n\n6.1.6 Data Fields\n\nData scientists have their hands full as many different fields and professions can use data analytics and information to facilitate their operations\nThese fields include:\n\nHealthcare\nMedia/Entertainment\nRetail\nTelecommunication\nAutomotive\nDigital Marketing\nCyber security\n\nData science communication skills vary in these fields, as some may prefer verbal information or visual information.\n\n\n\n6.1.7 Storytelling\n\nStorytelling in the context of Data Science gives the audience a shared goal within understanding the topics and information given.\nThe goals of promoting improved customer service, innovation, or operation optimization need to be conveyed in a manner that is direct and professional.\nAccording to Sonali Verghese for Medium, here are the possibilities of telling a story within the Data Science field:\n\nExplain how you arrived at a particular conclusion\nJustify rationally why you approached a problem in a specific manner\nConvey interesting insights in a way that gets people to think or act differently\nPersuade your audience that your results are conclusive and can be turned into something actionable\nExpress why your findings are valuable and how they fit into the overall picture\n\nThis is an inspiring quotation that I found while researching this presentation. Valentin Mucke for Medium: “Data science is about humans. Data scientists must remember that, and not just when presenting to people with a non-technical background. It’s important to find common ground with everyone you work with to build trust and move forward effectively.\n\n\n\n6.1.8 Data Visualization\n\nThe idea of expressing data through visualization has been a vital step for Data Scientists in the field\nExamples of Data Visualization for Data Scientists are:\n\nMany of these forms of visualization can be combined with other learned skill sets that will be mentioned in the next topic\n\n\n\n\n6.1.9 Usable Skill Sets for Data Communication\n\nCoding languages: Python, Structured Query Language, R, Visual Basic for Applications, Julia\nStatistical programming: the process of using computer programming languages to analyze and manipulate data for statistical purposes\nStatistics and probability: help predict the likelihood of future events and understand patterns in data\nMachine learning/Artificial intelligence: automates the data analysis process and makes predictions in real-time without human involvement, leading to further building and training of a data model to make real-time predictions\nStatistical visualization: the graphical representation of information and data that uses visual elements like charts, graphs, and maps, and tools to provide an accessible ways to see and understand trends, outliers, and patterns in data\nData management: process of collecting, storing, organizing and maintaining data to ensure that it is accurate and accessible to those who need it reliably throughout the data science project lifecycle\n\n\n\n6.1.10 Gather questions and Feedback\n\nAccording to the Data Incubator, a step to take before finalizing the project or an end of a report is to “consider soliciting direct feedback from your audience. It doesn’t matter if you have to prompt them to ask you questions or if they’re impatient to put your knowledge to the test—this form of interaction can help you improve your communication skills and establish a successful career as a data scientist.”\nBeing able to interact with your audience gives them a better understanding of the topic at hand, and can help avoid ambiguity that would occur if communication was not present",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communicating Data Science</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1 Matplotlib",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#matplotlib",
    "href": "visualization.html#matplotlib",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1.1 Introduction\nPlease use rodent sighting data for illustrations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "8  Exercises",
    "section": "",
    "text": "Git basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix retult from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\nThe NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.\n\nNYC Rodents Rats in NYC are widespread, as they are in many densely populated areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. In the 311 Service Request Data, there is one complain type Rodent. Extract all the requests with complain type Rodent, created between January 1, 2022 and December 31, 2023. Save them into a csv file named rodent_2022-2023.csv.\n\nAre there any complains that are not closed yet?\nAre there any complains with a closed data before the created date?\nHow many agencies were this complain type reported to?\nSummarize the missingess for each variable.\nSummarize a frequency table for the descriptor variable, and summarize a cross table by year.\nWhich types of ‘DESCRIPTOR’ do you think should be included if our interest is rodent sighting?\nTake a subset of the data with the descriptors you chose and summarize the response time by borough.\n\nNYC rodent sightings data cleaning The data appears to need some cleaning before any further analysis. Some missing values could be filled based on other columns.\n\nChecking all 47 column names suggests that some columns might be redundant. Identify them and demonstrate the redundancy.\nAre zip code and borough always missing together? If geocodes are available, use reverse geocoding to fill the zip code.\nExport the cleaned data in both csv and feather format. Comment on the file sizes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]