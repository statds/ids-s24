<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Data Science - 10&nbsp; Supervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./exercises.html" rel="next">
<link href="./statsmod.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./supervised.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/ids-s24.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Project Management</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducibile Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python Refreshment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./import.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Import/Export</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./communication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Communication and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statsmod.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">10.1</span> Introduction</a></li>
  <li><a href="#classification-versus-regression" id="toc-classification-versus-regression" class="nav-link" data-scroll-target="#classification-versus-regression"><span class="header-section-number">10.2</span> Classification versus Regression</a>
  <ul class="collapse">
  <li><a href="#classification-metrics" id="toc-classification-metrics" class="nav-link" data-scroll-target="#classification-metrics"><span class="header-section-number">10.2.1</span> Classification Metrics</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">10.2.2</span> Cross-validation</a></li>
  </ul></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="header-section-number">10.3</span> Decision Trees</a></li>
  <li><a href="#boosted-tree" id="toc-boosted-tree" class="nav-link" data-scroll-target="#boosted-tree"><span class="header-section-number">10.4</span> Boosted Tree</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">10.4.1</span> Introduction</a></li>
  <li><a href="#boosting-process" id="toc-boosting-process" class="nav-link" data-scroll-target="#boosting-process"><span class="header-section-number">10.4.2</span> Boosting Process</a></li>
  <li><a href="#boosting-vs-bagging-in-ensemble-learning" id="toc-boosting-vs-bagging-in-ensemble-learning" class="nav-link" data-scroll-target="#boosting-vs-bagging-in-ensemble-learning"><span class="header-section-number">10.4.3</span> Boosting vs Bagging in Ensemble Learning</a></li>
  </ul></li>
  <li><a href="#handling-imbalanced-data" id="toc-handling-imbalanced-data" class="nav-link" data-scroll-target="#handling-imbalanced-data"><span class="header-section-number">10.5</span> Handling Imbalanced Data</a>
  <ul class="collapse">
  <li><a href="#imbalanced-data" id="toc-imbalanced-data" class="nav-link" data-scroll-target="#imbalanced-data"><span class="header-section-number">10.5.1</span> Imbalanced Data</a></li>
  <li><a href="#approaches-to-handling-imbalanced-data" id="toc-approaches-to-handling-imbalanced-data" class="nav-link" data-scroll-target="#approaches-to-handling-imbalanced-data"><span class="header-section-number">10.5.2</span> Approaches to Handling Imbalanced Data</a></li>
  <li><a href="#smote" id="toc-smote" class="nav-link" data-scroll-target="#smote"><span class="header-section-number">10.5.3</span> SMOTE</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">10.1</span> Introduction</h2>
<p>Supervised and unsupervised learning represent two core approaches in the field of machine learning, each with distinct methodologies, applications, and goals. Understanding the differences and applicabilities of these learning paradigms is fundamental for anyone venturing into data science and machine learning.</p>
<ul>
<li><p><strong>Supervised Learning</strong> Supervised learning is characterized by its use of labeled datasets to train algorithms. In this paradigm, the model is trained on a pre-defined set of training examples, which include an input and the corresponding output. The goal of supervised learning is to learn a mapping from inputs to outputs, enabling the model to make predictions or decisions based on new, unseen data. This approach is widely used in applications such as spam detection, image recognition, and predicting consumer behavior. Supervised learning is further divided into two main categories: regression, where the output is continuous, and classification, where the output is categorical.</p></li>
<li><p><strong>Unsupervised Learning</strong> In contrast, unsupervised learning involves working with datasets without labeled responses. The aim here is to uncover hidden patterns, correlations, or structures from input data without the guidance of an explicit output variable. Unsupervised learning algorithms are adept at clustering, dimensionality reduction, and association tasks. They are invaluable in exploratory data analysis, customer segmentation, and anomaly detection, where the structure of the data is unknown, and the goal is to derive insights directly from the data itself.</p></li>
</ul>
<p>The key difference between supervised and unsupervised learning lies in the presence or absence of labeled output data. Supervised learning depends on known outputs to train the model, making it suitable for predictive tasks where the relationship between the input and output is clear. Unsupervised learning, however, thrives on discovering the intrinsic structure of data, making it ideal for exploratory analysis and understanding complex data dynamics without predefined labels.</p>
<p>Both supervised and unsupervised learning have their place in the machine learning ecosystem, often complementing each other in comprehensive data analysis and modeling projects. While supervised learning allows for precise predictions and classifications, unsupervised learning offers deep insights and uncovers underlying patterns that might not be immediately apparent.</p>
</section>
<section id="classification-versus-regression" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="classification-versus-regression"><span class="header-section-number">10.2</span> Classification versus Regression</h2>
<p>The main tasks in supervised learning can broadly be categorized into two types: classification and regression. Each task utilizes algorithms to interpret the input data and make predictions or decisions based on that data.</p>
<ul>
<li><p><strong>Classification</strong> Classification tasks involve categorizing data into predefined classes or groups. In these tasks, the output variable is categorical, such as “spam” or “not spam” in email filtering, or “malignant” or “benign” in tumor diagnosis. The aim is to accurately assign new, unseen instances to one of the categories based on the learning from the training dataset. Classification can be binary, involving two classes, or multiclass, involving more than two classes. Common algorithms used for classification include Logistic Regression, Decision Trees, Support Vector Machines, and Neural Networks.</p></li>
<li><p><strong>Regression</strong> Regression tasks predict a continuous quantity. Unlike classification, where the outcomes are discrete labels, regression models predict a numeric value. Examples of regression tasks include predicting the price of a house based on its features, forecasting stock prices, or estimating the age of a person from a photograph. The goal is to find the relationship or correlation between the input features and the continuous output variable. Linear regression is the most basic form of regression, but there are more complex models like Polynomial Regression, Ridge Regression, Lasso Regression, and Regression Trees.</p></li>
</ul>
<p>Both classification and regression are foundational to supervised learning, addressing different types of predictive modeling problems. Classification is used when the output is a category, while regression is used when the output is a numeric value. The choice between classification and regression depends on the nature of the target variable you are trying to predict. Supervised learning algorithms learn from labeled data, refining their models to minimize error and improve prediction accuracy on new, unseen data.</p>
<section id="classification-metrics" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="classification-metrics"><span class="header-section-number">10.2.1</span> Classification Metrics</h3>
<section id="confusion-matrix" class="level4" data-number="10.2.1.1">
<h4 data-number="10.2.1.1" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">10.2.1.1</span> Confusion matrix</h4>
<p><a href="https://en.wikipedia.org/wiki/Confusion_matrix" class="uri">https://en.wikipedia.org/wiki/Confusion_matrix</a></p>
<p>Four entries in the confusion matrix:</p>
<ul>
<li>TP: number of true positives</li>
<li>FN: number of false negatives</li>
<li>FP: number of false positives</li>
<li>TN: number of true negatives</li>
</ul>
<p>Four rates from the confusion matrix with actual (row) margins:</p>
<ul>
<li>TPR: TP / (TP + FN). Also known as sensitivity.</li>
<li>FNR: FN / (TP + FN). Also known as miss rate.</li>
<li>FPR: FP / (FP + TN). Also known as false alarm, fall-out.</li>
<li>TNR: TN / (FP + TN). Also known as specificity.</li>
</ul>
<p>Note that TPR and FPR do not add up to one. Neither do FNR and FPR.</p>
<p>Four rates from the confusion matrix with predicted (column) margins:</p>
<ul>
<li>PPV: TP / (TP + FP). Also known as precision.</li>
<li>FDR: FP / (TP + FP).</li>
<li>FOR: FN / (FN + TN).</li>
<li>NPV: TN / (FN + TN).</li>
</ul>
</section>
<section id="measure-of-classification-performance" class="level4" data-number="10.2.1.2">
<h4 data-number="10.2.1.2" class="anchored" data-anchor-id="measure-of-classification-performance"><span class="header-section-number">10.2.1.2</span> Measure of classification performance</h4>
<p>Measures for a given confusion matrix:</p>
<ul>
<li>Accuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.</li>
<li>Recall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.</li>
<li>Precision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.</li>
<li>F-beta score: Harmonic mean of precision and recall with <span class="math inline">\(\beta\)</span> chosen such that recall is considered <span class="math inline">\(\beta\)</span> times as important as precision, <span class="math display">\[
(1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}
{\beta^2 \text{precision} + \text{recall}}
\]</span> See <a href="https://stats.stackexchange.com/questions/221997/why-f-beta-score-define-beta-like-that">stackexchange post</a> for the motivation of <span class="math inline">\(\beta^2\)</span>.</li>
</ul>
<p>When classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.</p>
<ul>
<li>Increasing from <span class="math inline">\((0, 0)\)</span> to <span class="math inline">\((1, 1)\)</span>.</li>
<li>Best classification passes <span class="math inline">\((0, 1)\)</span>.</li>
<li>Classification by random guess gives the 45-degree line.</li>
<li>Area between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.</li>
<li>Area under the curve (AUC) of ROC thus provides an important metric of classification results.</li>
</ul>
</section>
</section>
<section id="cross-validation" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">10.2.2</span> Cross-validation</h3>
<ul>
<li>Goal: strike a bias-variance tradeoff.</li>
<li>K-fold: hold out each fold as testing data.</li>
<li>Scores: minimized to train a model</li>
</ul>
<p>Cross-validation is an important measure to prevent over-fitting. Good in-sample performance does not necessarily mean good out-sample performance. A general work flow in model selection with cross-validation is as follows.</p>
<ul>
<li>Split the data into training and testing</li>
<li>For each candidate model <span class="math inline">\(m\)</span> (with possibly multiple tuning parameters)
<ul>
<li>Fit the model to the training data</li>
<li>Obtain the performance measure <span class="math inline">\(f(m)\)</span> on the testing data (e.g., CV score, MSE, loss, etc.)</li>
</ul></li>
<li>Choose the model <span class="math inline">\(m^* = \arg\max_m f(m)\)</span>.</li>
</ul>
<!-- ## Decision Tree -->
</section>
</section>
<section id="decision-trees" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">10.3</span> Decision Trees</h2>
</section>
<section id="boosted-tree" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="boosted-tree"><span class="header-section-number">10.4</span> Boosted Tree</h2>
<p>Boosted trees are a powerful ensemble technique in machine learning that combines multiple weak learners, typically decision trees, to form a strong learner. The essence of the boosting approach is to fit sequential models, where each new model attempts to correct errors made by the previous ones. Gradient boosting, one of the most popular boosting methods, optimizes a loss function over weak learners by iteratively adding trees that correct the residuals of the combined ensemble.</p>
<section id="introduction-1" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">10.4.1</span> Introduction</h3>
<p>Boosted trees build on the concept of boosting, an ensemble technique that aims to create a strong classifier from a number of weak classifiers. In the context of boosted trees, the weak learners are decision trees, usually of a fixed size, which are added sequentially to the model. The key idea is to improve the model iteratively by focusing on examples that are hard to predict, thus enhancing the overall predictive accuracy of the model.</p>
</section>
<section id="boosting-process" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="boosting-process"><span class="header-section-number">10.4.2</span> Boosting Process</h3>
<p>In the context of gradient boosted trees, the ensemble model is built iteratively, where each new tree is fitted to the residual errors made by the previous ensemble of trees. The aim is to minimize a loss function, and each tree incrementally improves the model by reducing the loss. The ensemble model after <span class="math inline">\(M\)</span> trees can be described more accurately as:</p>
<p>The final predictive model is <span class="math inline">\(F_M(x)\)</span>, which represents the accumulated predictions of all <span class="math inline">\(M\)</span> trees, adjusted by their respective learning rates.</p>
<p>The process can be delineated as follows:</p>
<ul>
<li>Initialization: Begin with a base model <span class="math inline">\(F_0(x)\)</span>, often the mean of the target variable for regression or the log odds for classification.</li>
<li>Iterative Boosting:
<ul>
<li>At each stage <span class="math inline">\(m\)</span>, compute the pseudo-residuals, which are the gradients of the loss function with respect to the predictions of the current model, <span class="math inline">\(F_{m-1}(x)\)</span>.</li>
<li>Fit a new tree, <span class="math inline">\(h_m(x)\)</span>, to these pseudo-residuals.</li>
<li>Update the model by adding this new tree, weighted by a learning rate <span class="math inline">\(\lambda\)</span>, to minimize the loss, resulting in <span class="math inline">\(F_m(x) =
F_{m-1}(x) + \lambda \cdot h_m(x)\)</span>.</li>
</ul></li>
<li>Final Model: After <span class="math inline">\(M\)</span> iterations, the ensemble model <span class="math inline">\(F_M(x)\)</span> represents the sum of the initial model and the incremental improvements made by each of the <span class="math inline">\(M\)</span> trees, where each tree is fitted to correct the residuals of the model up to that point.</li>
</ul>
<p>Key Concepts</p>
<ul>
<li>Loss Function (<span class="math inline">\(L\)</span>): A measure of how far the ensemble’s predictions are from the actual values. Common choices include the squared error for regression and logistic loss for classification.</li>
<li>Learning Rate (<span class="math inline">\(\lambda\)</span>): A small positive number that scales the contribution of each tree. It helps in controlling the speed of learning and prevents overfitting.</li>
</ul>
<p>This iterative approach, focusing on correcting the errors of the ensemble up to the current step, distinguishes gradient boosting from other ensemble methods and allows for a nuanced adjustment of the model to the data.</p>
</section>
<section id="boosting-vs-bagging-in-ensemble-learning" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="boosting-vs-bagging-in-ensemble-learning"><span class="header-section-number">10.4.3</span> Boosting vs Bagging in Ensemble Learning</h3>
<p>Boosting and bagging are foundational ensemble learning techniques in machine learning, designed to improve the accuracy of predictive models by combining the strengths of multiple weaker models. Despite their common goal, they differ significantly in methodology and application.</p>
<p>Boosting builds models in a sequential manner, focusing each subsequent model on correcting the errors made by the previous ones. The process initiates with a base model, with each new model added aiming to correct its predecessor, culminating in a weighted sum of all models.</p>
<ul>
<li><strong>Sequential Model Building</strong>: Models are built sequentially, each correcting the error of the ensemble before it.</li>
<li><strong>Focus on Misclassification</strong>: Boosting prioritizes instances misclassified by earlier models, adapting to the “harder” cases.</li>
<li><strong>Variance Reduction</strong>: It mainly reduces model variance, carefully avoiding overfitting despite increasing complexity.</li>
</ul>
<p>Examples include AdaBoost, Gradient Boosting, and XGBoost.</p>
<p>Bagging, or Bootstrap Aggregating, constructs multiple models independently and combines their predictions through averaging or majority voting. Each model is trained on a randomly drawn subset of the data, allowing for parallel model construction.</p>
<ul>
<li><strong>Parallel Model Building</strong>: Models are built independently, enabling parallelization.</li>
<li><strong>Uniform Attention</strong>: All instances are equally considered, with the diversity of the ensemble reducing variance.</li>
<li><strong>Bias and Variance Reduction</strong>: Effective at reducing both, but particularly good at cutting down variance in complex models.</li>
</ul>
<p>Random Forest is a prominent example of bagging.</p>
<section id="differences" class="level4" data-number="10.4.3.1">
<h4 data-number="10.4.3.1" class="anchored" data-anchor-id="differences"><span class="header-section-number">10.4.3.1</span> Differences</h4>
<ul>
<li><strong>Model Dependency</strong>: Boosting’s models are interdependent, building upon the errors of their predecessors, unlike bagging’s independent models.</li>
<li><strong>Error Correction Focus</strong>: Boosting directly targets previous errors, while bagging reduces error through diversity.</li>
<li><strong>Computational Complexity</strong>: Boosting can be more computationally intensive due to its sequential nature.</li>
<li><strong>Overfitting Risk</strong>: Boosting may overfit on noisy data, whereas bagging remains robust, especially as the number of models increases.</li>
</ul>
<p>Understanding the distinctions between boosting and bagging is crucial for selecting the appropriate ensemble method for specific machine learning tasks. Both strategies offer unique advantages and can be applied based on the problem, data characteristics, and desired outcomes.</p>
</section>
</section>
</section>
<section id="handling-imbalanced-data" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="handling-imbalanced-data"><span class="header-section-number">10.5</span> Handling Imbalanced Data</h2>
<p>Handling imbalanced data is a critical task in machine learning, particularly in classification problems where the distribution of instances across the known classes is uneven. This imbalance can significantly bias the model’s training process, leading to poor predictive performance, especially for the minority class. Understanding and addressing this issue is crucial for building effective and fair models.</p>
<section id="imbalanced-data" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="imbalanced-data"><span class="header-section-number">10.5.1</span> Imbalanced Data</h3>
<p>Imbalanced data refers to a situation where the number of observations in one class significantly outweighs those in one or more other classes. This is a common scenario in various domains:</p>
<ul>
<li>In fraud detection, legitimate transactions vastly outnumber fraudulent ones.</li>
<li>In medical diagnosis, the dataset might have more negative results (no disease) than positive results (disease presence).</li>
<li>In email filtering, spam emails are less common than non-spam.</li>
</ul>
<p>Such disparities can lead to models that are overly biased towards the majority class, as they tend to optimize overall accuracy by simply predicting the majority class.</p>
</section>
<section id="approaches-to-handling-imbalanced-data" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="approaches-to-handling-imbalanced-data"><span class="header-section-number">10.5.2</span> Approaches to Handling Imbalanced Data</h3>
<ul>
<li><p>Resampling Techniques:</p>
<ul>
<li>Oversampling Minority Class: Enhancing the representation of the minority class by replicating instances or generating synthetic samples. SMOTE (Synthetic Minority Over-sampling Technique) is a popular method for creating synthetic samples <span class="citation" data-cites="chawla2002smote">(<a href="references.html#ref-chawla2002smote" role="doc-biblioref">Chawla et al. 2002</a>)</span>.</li>
<li>Undersampling Majority Class: Reducing the size of the majority class to balance the dataset. Careful selection or clustering methods are employed to retain information <span class="citation" data-cites="liu2008exploratory">(<a href="references.html#ref-liu2008exploratory" role="doc-biblioref">Liu, Wu, and Zhou 2008</a>)</span>.</li>
</ul></li>
<li><p>Cost-sensitive Learning: Adjusting the classification cost to make errors on the minority class more impactful than errors on the majority class. This approach often involves modifying the algorithm to be more sensitive to the minority class <span class="citation" data-cites="elkan2001foundations">(<a href="references.html#ref-elkan2001foundations" role="doc-biblioref">Elkan 2001</a>)</span>.</p></li>
<li><p>Ensemble Methods: Leveraging multiple models to improve classification, particularly of the minority class. Techniques such as Balanced Random Forest <span class="citation" data-cites="chen2004using">(<a href="references.html#ref-chen2004using" role="doc-biblioref">Chen, Liaw, and Breiman 2004</a>)</span> and AdaBoost <span class="citation" data-cites="sun2007cost">(<a href="references.html#ref-sun2007cost" role="doc-biblioref">Sun, Wong, and Kamel 2007</a>)</span> have been adapted for imbalanced datasets.</p></li>
<li><p>Algorithmic Adjustments: Some algorithms have built-in mechanisms for dealing with imbalanced data, such as adjusting class weights. This is evident in algorithms like SVM and logistic regression, where class weights can be inversely proportional to class frequencies <span class="citation" data-cites="king2001logistic">(<a href="references.html#ref-king2001logistic" role="doc-biblioref">King and Zeng 2001</a>)</span>.</p></li>
</ul>
</section>
<section id="smote" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="smote"><span class="header-section-number">10.5.3</span> SMOTE</h3>
<p>The Synthetic Minority Over-sampling Technique (SMOTE) offers a nuanced approach to mitigating the challenges posed by imbalanced datasets. Unlike simple oversampling techniques that replicate minority class instances, SMOTE generates synthetic samples in a feature space, enhancing the diversity and representation of the minority class without directly copying existing observations <span class="citation" data-cites="chawla2002smote">(<a href="references.html#ref-chawla2002smote" role="doc-biblioref">Chawla et al. 2002</a>)</span>.</p>
<section id="algorithm" class="level4" data-number="10.5.3.1">
<h4 data-number="10.5.3.1" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">10.5.3.1</span> Algorithm</h4>
<p>SMOTE’s algorithm can be succinctly described using the following steps for each minority class sample <span class="math inline">\(x\)</span>:</p>
<ol type="1">
<li>Identify the <span class="math inline">\(k\)</span> nearest neighbors in the minority class for <span class="math inline">\(x\)</span>.</li>
<li>Randomly choose one of these <span class="math inline">\(k\)</span> neighbors, call it <span class="math inline">\(x_{\text{nn}}\)</span>.</li>
<li>Generate a synthetic sample <span class="math inline">\(x_{\text{new}}\)</span> along the line connecting <span class="math inline">\(x\)</span> and <span class="math inline">\(x_{\text{nn}}\)</span>. Mathematically, <span class="math inline">\(x_{\text{new}} = x + \lambda (x_{\text{nn}} - x)\)</span>, where <span class="math inline">\(\lambda\)</span> is a random number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</li>
</ol>
<p>This process is repeated until the class distribution is deemed balanced.</p>
<p>Several software tools and libraries have made implementing SMOTE straightforward in various programming environments. In Python, the <code>imbalanced-learn</code> library provides an efficient and flexible implementation of SMOTE and its variants, allowing seamless integration with scikit-learn workflows. R users can utilize the <code>DMwR</code> package, which includes functions for applying SMOTE to datasets. These tools not only offer basic SMOTE functionality but also support advanced variants and allow for easy experimentation with different oversampling strategies.</p>
</section>
<section id="applications-considerations-and-variants" class="level4" data-number="10.5.3.2">
<h4 data-number="10.5.3.2" class="anchored" data-anchor-id="applications-considerations-and-variants"><span class="header-section-number">10.5.3.2</span> Applications, Considerations, and Variants</h4>
<p><strong>Applications</strong>: SMOTE’s approach to enriching datasets by synthesizing new examples has proven beneficial in various fields. For instance, in fraud detection, where fraudulent transactions are rare <span class="citation" data-cites="dalpozzolo2015calibrating">(<a href="references.html#ref-dalpozzolo2015calibrating" role="doc-biblioref">Dal Pozzolo et al. 2015</a>)</span>, and in medical diagnostics, where certain conditions may be underrepresented in datasets <span class="citation" data-cites="johnson2019survey">(<a href="references.html#ref-johnson2019survey" role="doc-biblioref">Johnson and Khoshgoftaar 2019</a>)</span>.</p>
<p><strong>Considerations</strong>: While SMOTE enhances the representation of the minority class, it may also lead to overfitting, especially if the minority class is dispersed or if there’s significant overlap with the majority class <span class="citation" data-cites="guo2017learning">(<a href="references.html#ref-guo2017learning" role="doc-biblioref">Guo et al. 2017</a>)</span>. Choosing the right variant of SMOTE and adjusting its parameters requires careful consideration of the dataset’s characteristics.</p>
<p><strong>Advanced Variants</strong>: Specific challenges in datasets have led to the development of SMOTE variants like Borderline-SMOTE, which generates samples closer to the decision boundary to improve classifier sensitivity to the minority class <span class="citation" data-cites="han2005borderline">(<a href="references.html#ref-han2005borderline" role="doc-biblioref">Han, Wang, and Mao 2005</a>)</span>. Each variant addresses particular issues, such as noise or the risk of generating ambiguous synthetic samples.</p>
</section>
<section id="conclusion" class="level4" data-number="10.5.3.3">
<h4 data-number="10.5.3.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.5.3.3</span> Conclusion</h4>
<p>SMOTE and its variants offer a sophisticated approach to mitigating the challenges posed by imbalanced datasets. Through synthetic sample generation, these methods enhance the diversity and representation of the minority class, facilitating improved model accuracy and generalizability. The choice of variant and parameters should be guided by the dataset’s characteristics and the problem context, underscoring the importance of careful experimentation and validation.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-chawla2002smote" class="csl-entry" role="listitem">
Chawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002. <span>“<span>SMOTE</span>: Synthetic Minority over-Sampling Technique.”</span> <em>Journal of Artificial Intelligence Research</em> 16: 321–57.
</div>
<div id="ref-chen2004using" class="csl-entry" role="listitem">
Chen, Chao, Andy Liaw, and Leo Breiman. 2004. <span>“Using Random Forest to Learn Imbalanced Data.”</span> University of California, Berkeley.
</div>
<div id="ref-dalpozzolo2015calibrating" class="csl-entry" role="listitem">
Dal Pozzolo, Andrea, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. 2015. <span>“Calibrating Probability with Undersampling for Unbalanced Classification.”</span> In <em>2015 IEEE Symposium Series on Computational Intelligence</em>, 159–66. IEEE.
</div>
<div id="ref-elkan2001foundations" class="csl-entry" role="listitem">
Elkan, Charles. 2001. <span>“The Foundations of Cost-Sensitive Learning.”</span> In <em>Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence</em>, 973–78. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-guo2017learning" class="csl-entry" role="listitem">
Guo, Haixiang, Yijing Li, Jennifer Shang, Mingyun Gu, Yuanyue Huang, and Bing Gong. 2017. <span>“Learning from Class-Imbalanced Data: Review of Methods and Applications.”</span> <em>Expert Systems with Applications</em> 73: 220–39.
</div>
<div id="ref-han2005borderline" class="csl-entry" role="listitem">
Han, Hui, Wen-Yuan Wang, and Bing-Huan Mao. 2005. <span>“Borderline-SMOTE: A New over-Sampling Method in Imbalanced Data Sets Learning.”</span> In <em>International Conference on Intelligent Computing</em>, 878–87. Springer.
</div>
<div id="ref-johnson2019survey" class="csl-entry" role="listitem">
Johnson, Justin M., and Taghi M. Khoshgoftaar. 2019. <span>“Survey on Deep Learning with Class Imbalance.”</span> <em>Journal of Big Data</em> 6 (1): 27.
</div>
<div id="ref-king2001logistic" class="csl-entry" role="listitem">
King, Gary, and Langche Zeng. 2001. <span>“Logistic Regression in Rare Events Data.”</span> <em>Political Analysis</em> 9 (2): 137–63.
</div>
<div id="ref-liu2008exploratory" class="csl-entry" role="listitem">
Liu, Xu-Ying, Jianxin Wu, and Zhi-Hua Zhou. 2008. <span>“Exploratory Undersampling for Class-Imbalance Learning.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</em> 39 (2): 539–50.
</div>
<div id="ref-sun2007cost" class="csl-entry" role="listitem">
Sun, Yanmin, Andrew KC Wong, and Mohamed S Kamel. 2007. <span>“Cost-Sensitive Boosting for Classification of Imbalanced Data.”</span> <em>Pattern Recognition</em> 40 (12): 3358–78.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./statsmod.html" class="pagination-link" aria-label="Statistical Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Statistical Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./exercises.html" class="pagination-link" aria-label="Exercises">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Exercises</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>