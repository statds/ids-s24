{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Models\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Statistical modeling is a cornerstone of data science, offering tools to\n",
        "understand complex relationships within data and to make predictions. Python,\n",
        "with its rich ecosystem for data analysis, features the `statsmodels` package—\n",
        "a comprehensive library designed for statistical modeling, tests, and data\n",
        "exploration. `statsmodels` stands out for its focus on classical statistical\n",
        "models and compatibility with the Python scientific stack (`numpy`, `scipy`,\n",
        "`pandas`).\n",
        "\n",
        "### Installation of `statsmodels`\n",
        "\n",
        "To start with statistical modeling, ensure `statsmodels` is installed:\n",
        "\n",
        "Using pip:\n",
        "\n",
        "```bash\n",
        "pip install statsmodels\n",
        "```\n",
        "\n",
        "### Features of `statsmodels`\n",
        "\n",
        "Package `statsmodels` offers a comprehensive range of statistical models and\n",
        "tests, making it a powerful tool for a wide array of data analysis tasks:\n",
        "\n",
        "- **Linear Regression Models**: Essential for predicting quantitative \n",
        "  responses, these models form the backbone of many statistical analysis \n",
        "  operations.\n",
        "  \n",
        "- **Generalized Linear Models (GLM)**: Expanding upon linear models, GLMs \n",
        "  allow for response variables that have error distribution models other than \n",
        "  a normal distribution, catering to a broader set of data characteristics.\n",
        "  \n",
        "- **Time Series Analysis**: This includes models like ARIMA for analyzing \n",
        "  and forecasting time-series data, as well as more complex state space \n",
        "  models and seasonal decompositions.\n",
        "  \n",
        "- **Nonparametric Methods**: For data that does not fit a standard \n",
        "  distribution, `statsmodels` provides tools like kernel density estimation \n",
        "  and smoothing techniques.\n",
        "  \n",
        "- **Statistical Tests**: A suite of hypothesis testing tools allows users \n",
        "  to rigorously evaluate their models and assumptions, including diagnostics \n",
        "  for model evaluation.\n",
        "\n",
        "Integrating `statsmodels` into your data science workflow enriches your \n",
        "analytical capabilities, allowing for both exploratory data analysis and \n",
        "complex statistical modeling.\n",
        "\n",
        "\n",
        "## Generalized Linear Models\n",
        "\n",
        "Generalized Linear Models (GLM) extend the classical linear regression to\n",
        "accommodate response variables, that follow distributions other than the\n",
        "normal distribution. GLMs consist of three main components:\n",
        "\n",
        "+ **Random Component**: This specifies the distribution of the \n",
        "response variable $Y$. It is assumed to be from the exponential family of \n",
        "distributions, such as Binomial for binary data and Poisson for count data.\n",
        "+ **Systematic Component**: This consists of the linear predictor, \n",
        "a linear combination of unknown parameters and explanatory variables. It \n",
        "is denoted as $\\eta = X\\beta$, where $X$ represents the explanatory \n",
        "variables, and $\\beta$ represents the coefficients.\n",
        "+ **Link Function**: The link function, $g$, provides the \n",
        "relationship between the linear predictor and the mean of the distribution \n",
        "function. For a GLM, the mean of $Y$ is related to the linear predictor \n",
        "through the link function as $\\mu = g^{-1}(\\eta)$.\n",
        "\n",
        "\n",
        "Generalized Linear Models (GLM) adapt to various data types through the\n",
        "selection of appropriate link functions and probability distributions. Here,\n",
        "we outline four special cases of GLM: normal regression, logistic regression,\n",
        "Poisson regression, and gamma regression.\n",
        "\n",
        "+ Normal Regression (Linear Regression).\n",
        "In normal regression, the response variable has a normal distribution. The\n",
        "identity link function ($g(\\mu) = \\mu$) is typically used, making this case\n",
        "equivalent to classical linear regression.\n",
        "    - **Use Case**: Modeling continuous data where residuals are normally distributed.\n",
        "    - **Link Function**: Identity ($g(\\mu) = \\mu$)\n",
        "    - **Distribution**: Normal\n",
        "+ Logistic Regression.\n",
        "Logistic regression is used for binary response variables. It employs the\n",
        "logit link function to model the probability that an observation falls into\n",
        "one of two categories.\n",
        "    - **Use Case**: Binary outcomes (e.g., success/failure).\n",
        "    - **Link Function**: Logit ($g(\\mu) = \\log\\frac{\\mu}{1-\\mu}$)\n",
        "    - **Distribution**: Binomial\n",
        "+ Poisson Regression.\n",
        "Poisson regression models count data using the Poisson distribution. It's\n",
        "ideal for modeling the rate at which events occur.\n",
        "    - **Use Case**: Count data, such as the number of occurrences of an event.\n",
        "    - **Link Function**: Log ($g(\\mu) = \\log(\\mu)$)\n",
        "    - **Distribution**: Poisson\n",
        "+ Gamma Regression.\n",
        "Gamma regression is suited for modeling positive continuous variables, \n",
        "especially when data are skewed and variance increases with the mean.\n",
        "    - **Use Case**: Positive continuous outcomes with non-constant variance.\n",
        "    - **Link Function**: Inverse ($g(\\mu) = \\frac{1}{\\mu}$)\n",
        "    - **Distribution**: Gamma\n",
        "\n",
        "Each GLM variant addresses specific types of data and research questions,\n",
        "enabling precise modeling and inference based on the underlying data\n",
        "distribution.\n",
        "\n",
        "\n",
        "## Statistical Modeling with `statsmodels`\n",
        "\n",
        "This section was written by Leon Nguyen.\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Hello! My name is Leon Nguyen (they/she) and I am a second-year undergraduate \n",
        "student studying Statistical Data Science and Mathematics at the University \n",
        "of Connecticut, aiming to graduate in Fall 2025. One of my long-term goals is to \n",
        "make the field of data science more accessible to marginalized communities and \n",
        "minority demographics. My research interests include data visualization and \n",
        "design. Statistical modeling is one of the most fundamental skills required \n",
        "for data science, and it's important to have a solid understanding of how \n",
        "models work for interpretable results. \n",
        "\n",
        "The `statsmodels` Python package offers a diverse range of classes and functions \n",
        "tailored for estimating various statistical models, conducting statistical tests, \n",
        "and exploring statistical data. Each estimator provides an extensive array of \n",
        "result statistics, rigorously tested against established statistical packages to \n",
        "ensure accuracy. This presentation will focus on the practical applications of the \n",
        "statistical modeling aspect.\n",
        "\n",
        "### Key Features and Capabilities\n",
        "\n",
        "Some key features and capabilities of `statsmodels` are:\n",
        "\n",
        "+ Generalized Linear Models\n",
        "+ Diagnostic Tests\n",
        "+ Nonparametric methods\n",
        "\n",
        "In this presentation, we will work with practical applications of statistical \n",
        "modeling in `statsmodels`. We will briefly cover how to set up linear, logistic, \n",
        "and Poisson regression models, and touch upon kernel density estimation and \n",
        "diagnostics. By the end of this presentation, you should be able to understand how \n",
        "to use `statsmodels` to analyze your own datasets using these fundamental \n",
        "techniques.\n",
        "\n",
        "### Installation and Setup\n",
        "\n",
        "To install `statsmodels`, use `pip install statsmodels` or `conda install statsmodels`, depending on whether you are using pip or conda.\n",
        "\n",
        "One of the major benefits of using `statsmodels` is their compatability with \n",
        "other commnonly used packages, such as `NumPy`, `SciPy`, and `Pandas`. \n",
        "These packages provide foundational scientific computing functionalities that \n",
        "are crucial for working with `statsmodels`. To ensure everything is set up \n",
        "correctly, import the necessary libraries at the beginning of your script:"
      ],
      "id": "402c8424"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "82666554",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some minimum dependencies:\n",
        "\n",
        "+ Python >= 3.8\n",
        "+ NumPy >= 1.18\n",
        "+ SciPy >= 1.4\n",
        "+ Pandas >= 1.0\n",
        "+ Patsy >= 0.5.2\n",
        "\n",
        "The last item listed above, `patsy`, is a Python library that provides \n",
        "simple syntax for specifying statistical models in Python. It allows \n",
        "users to define linear models using a formula syntax similar to the formulas \n",
        "used in R and other statistical software. More `patsy` documentation can be \n",
        "found [here](https://patsy.readthedocs.io/en/latest/). This library is not used \n",
        "this demonstration, but is still worth noting.\n",
        "\n",
        "### Importing Data\n",
        "\n",
        "There are a few different options to import data. For example, `statsmodels` \n",
        "documentation demonstrates how to importing from a CSV file hosted online from \n",
        "the [Rdatasets repository](https://github.com/vincentarelbundock/Rdatasets/):"
      ],
      "id": "94397a88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reads the 'avocado' dataset from the causaldata package into df\n",
        "df0 = sm.datasets.get_rdataset(dataname='avocado', package=\"causaldata\").data\n",
        "# We will be using this dataset later!\n",
        "\n",
        "# Print out the first five rows of our dataframe\n",
        "print(df0.head())"
      ],
      "id": "f937ab60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also read directly from a local CSV file with `pandas`. For example, we will \n",
        "be using the NYC 311 request rodent data:"
      ],
      "id": "81b0124d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reads the csv file into df\n",
        "df = pd.read_csv('data/rodent_2022-2023.csv')\n",
        "\n",
        "# Brief data pre-processing\n",
        "# Time reformatting\n",
        "df['Created Date'] = pd.to_datetime(df['Created Date'], format = \"%m/%d/%Y %I:%M:%S %p\")\n",
        "df['Closed Date'] = pd.to_datetime(df['Closed Date'], format = \"%m/%d/%Y %I:%M:%S %p\")\n",
        "df['Created Year'] = df['Created Date'].dt.year\n",
        "df['Created Month'] = df['Created Date'].dt.month\n",
        "# Response time\n",
        "df['Response Time'] = df['Closed Date'] - df['Created Date'] \n",
        "df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600) # in hours\n",
        "# Remove unspecified borough rows\n",
        "df = df.drop(df[df['Borough']=='Unspecified'].index)\n",
        "# Remove 'other' open data channel type rows\n",
        "df = df.drop(df[df['Open Data Channel Type']=='OTHER'].index)\n",
        "\n",
        "print(df.head())"
      ],
      "id": "84b27b0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Troubleshooting\n",
        "\n",
        "Whenever you are having problems with `statsmodels`, you can access the \n",
        "official documentation by visiting [this link](https://www.statsmodels.org/stable/index.html). \n",
        "If you are working in a code editor, you can also run the following in a code cell:"
      ],
      "id": "0aed7fe2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sm.webdoc() \n",
        "# Opens the official documentation page in your browser"
      ],
      "id": "ffd0c46d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To look for specific documentation, for example `sm.GLS`, you can run the following:"
      ],
      "id": "9b09bf7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sm.webdoc(func=sm.GLS, stable=True)\n",
        "# func : string* or function to search for documentation \n",
        "# stable : (True) or development (False) documentation, default is stable\n",
        "\n",
        "# *Searching via string has presented issues?"
      ],
      "id": "f6819c81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical Modeling and Analysis\n",
        "\n",
        "Constructing statistical models with `statsmodels` generally follows a step-by-step \n",
        "process: \n",
        "\n",
        "1. **Import necessary libraries**: This includes both `numpy` and `pandas`, as well \n",
        "as `statsmodels.api` itself (`sm`).\n",
        "\n",
        "1. **Load** the data: This could be data from the `rdataset` repository, local \n",
        "csv files, or other formats. In general, it's best practice to load your data \n",
        "into a `pandas` DataFrame so that it can easily be manipulated using `pandas` \n",
        "functions.\n",
        "\n",
        "1. **Prepare the data**: This involves converting variables into appropriate \n",
        "types (e.g., categorical into factors), handling missing values, and creating \n",
        "appropriate interaction terms.\n",
        "\n",
        "1. **Define** our model: what model is the appropriate representation of our \n",
        "research question? This could be an OLS regression (`sm.OLS`), logistic \n",
        "regression (`sm.Logit`), or any number of other models depending on the \n",
        "nature of our data.\n",
        "   \n",
        "1. **Fit** the model to our data: we use the `.fit()` method which takes as input \n",
        "our dependent variable and independent variables.\n",
        "   \n",
        "1. **Analyze** the results of the model: this is where we can get things like \n",
        "parameter estimates, standard errors, p-values, etc. We use the `.summary()` \n",
        "method to print out these statistics.\n",
        "\n",
        "\n",
        "### Generalized Linear Models\n",
        "\n",
        "GLM models allow us to construct a linear relationship between the response and \n",
        "predictors, even if their underlying relationship is not linear. This is done via \n",
        "a link function, which is a transformation which links the response variable to a \n",
        "linear model. \n",
        "\n",
        "Key points of GLMs:\n",
        "\n",
        "+ Data should be independent and random.\n",
        "+ The response variable $Y$ does not need to be normally distributed, but the \n",
        "distribution is from an exponential family (e.g. binomial, Poisson, normal).\n",
        "+ GLMs do not assume a linear relationship between the response variable and \n",
        "the explanatory variables, but assume a linear relationship between \n",
        "the transformed expected response in terms of the link function and the \n",
        "explanatory variables.\n",
        "+ GLMs are useful when the range of your response variable is constrained \n",
        "and/or the variance is not constant or normally distributed. \n",
        "+ GLM models transform the response variable to allow the fit to be done by \n",
        "least squares. The transformation done on the response variable is defined by \n",
        "the link function.\n",
        "\n",
        "#### Linear Regression\n",
        "\n",
        "Simple and muliple linear regression are special cases where the expected value \n",
        "of the dependent value is equal to a linear combination of predictors. In other \n",
        "words, the link function is the identity function $g[E(Y)]=E(Y)$. Make sure \n",
        "assumptions for linear regression hold before proceeding. The model for \n",
        "linear regression is given by:\n",
        "$$y_i = X_i\\beta + \\epsilon_i$$\n",
        "where $X_i$ is a vector of predictors for individual $i$, and $\\beta$ is a vector of coefficients that define this linear combination.\n",
        "\n",
        "We will be working with the `avocado` dataset from the package `causaldata` \n",
        "which contains information about the average price and total amount of avocados \n",
        "that were sold in California from 2015-2018. `AveragePrice` of a single avocado \n",
        "is our predictor, and `TotalVolume` is our outcome variable as a count of avocados. \n",
        "\n",
        "Here is an application of SLR with `statsmodels`:"
      ],
      "id": "2fc60523"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We can use .get_rdataset() to load data into Python from a repositiory of R packages.\n",
        "df1 = sm.datasets.get_rdataset('avocado', package=\"causaldata\").data\n",
        "\n",
        "# Fit regression model\n",
        "results1 = smf.ols('TotalVolume ~ AveragePrice', data=df1).fit()\n",
        "\n",
        "# Analyze results\n",
        "print(results1.summary())"
      ],
      "id": "944ed743",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can interpret some values:\n",
        "\n",
        "+ **coef:** the coefficient of `AveragePrice` tells us how much adding one unit of \n",
        "`AveragePrice` changes the predicted value of `TotalVolume`. An important \n",
        "interpretation is that if `AveragePrice` was to increase by one unit, on average \n",
        "we could expect `TotalVolume` to change by this coefficient based on this linear \n",
        "model. This makes sense since higher prices should result in a smaller amount of avocados \n",
        "sold.\n",
        "+ **P>|t|:** p-value to test significant effect of the predictor on the response, \n",
        "compared to a significance level $\\alpha=0.05$. When this p-value $\\leq \\alpha$, \n",
        "we would reject the null hypothesis that there is no effect of `AveragePrice` on `TotalVolume`, \n",
        "and conclude that `AveragePrice` has a statistically significant effect on `TotalVolume`. \n",
        "+ **R-squared:** indicates the proportion of variance explained by the \n",
        "predictors (in this case just `AveragePrice`). If it's close to 1 then most \n",
        "of the variability in `TotalVolume` is explained by `AveragePrice`, which is good! \n",
        "However, only about 44.1% of the variability is explained, so this model could \n",
        "use some improvement.\n",
        "+ **Prob (F-statistic):** indicates whether or not the linear regression model \n",
        "provides a better fit to a dataset than a model with no predictors. Assuming a \n",
        "significance level of 0.05, we would reject the null hypothesis (model with just \n",
        "the intercept does just as well with a model with predictors) since our F-value \n",
        "probability is less than 0.05. We know that `AveragePrice` gives at least some significant \n",
        "information about `TotalVolume`. (This makes more sense in MLR where you are considering \n",
        "multiple predictors.)\n",
        "+ **Skew:** measures asymmetry of a distribution, which can be positive, negative, or zero.\n",
        "If skewness is positive, the distribution is more skewed to the right; if negative, then \n",
        "to the left. We ideally want a skew value of zero in a normal distribution. \n",
        "+ **Kurtosis:** a measure of whether or not a distribution is heavy-tailed or light-tailed \n",
        "relative to a normal distribution. For a normal distribution, we expect a kurtosis of 3. \n",
        "If our kurtosis is greater than 3, there are more outliers on the tails. If less than 3, \n",
        "then there are less.\n",
        "+ **Prob (Jarque-Bera):** indicates whether or not the residuals are normally distributed, \n",
        "which is required for the OLS linear regression model. In this case the test \n",
        "rejects the null hypothesis that the residuals come from a normal distribution. This \n",
        "is concerning because non-normality can lead to misleading conclusions and incorrect \n",
        "standard errors. \n",
        "\n",
        "#### Logistic Regression\n",
        "\n",
        "Logistic regression is used when the response variable is binary. The \n",
        "response distribution is logistic which means it has support (input) on $(0,1)$ and  \n",
        "is invertible. The log-odds link function is defined as $\\log\\left(\\frac{\\mu}{1-\\mu}\\right)$, where $\\mu$ is the predicted probability.\n",
        "\n",
        "Here we have an example from our `rodents` dataset, where the response variable \n",
        "`Under 3h` indicates whether the response time for a 311 service request was under 3 \n",
        "hours. 1 indicates that the response time is less than 3 hours, and 0 indications \n",
        "greater than or equal to 3 hours. We are creating a logistic regression model that \n",
        "can be used to estimate the odds ratio of 311 requests having a response time under 3 \n",
        "hours based on `Borough` and `Open Data Channel Type` (method of how 311 service request \n",
        "was submitted) as predictors. "
      ],
      "id": "c21725b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loaded the dataset in a previous cell as df\n",
        "# Create binary variable\n",
        "df['Under 3h'] = (df['Response Time'] < 3).astype(int)\n",
        "\n",
        "# Convert the categorical variable to dummy variables\n",
        "df = df.loc[:, ['Borough', 'Open Data Channel Type', 'Under 3h']]\n",
        "df = pd.get_dummies(df, dtype = int)\n",
        "\n",
        "# Remove reference dummy variables\n",
        "df.drop(\n",
        "  columns=['Borough_QUEENS', 'Open Data Channel Type_MOBILE'], \n",
        "  axis=1, \n",
        "  inplace=True\n",
        ")"
      ],
      "id": "6dc556f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this regression to run properly, we needed to create $k-1$ dummy \n",
        "variables with $k$ levels in a given predictor. Here we have two categorical \n",
        "variables that we used `pd.get_dummies()` function to change from a categorical \n",
        "variable into dummy variables. We then dropped one dummy variable level from each \n",
        "category: `'Borough_QUEENS'` and `'Open Data Channel Type_MOBILE'`."
      ],
      "id": "5fef26b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Drop all rows with NaN values\n",
        "df.dropna(inplace = True)\n",
        "\n",
        "# Fit the logistic regression model using statsmodels\n",
        "Y = df['Under 3h']\n",
        "X = sm.add_constant(df.drop(columns = 'Under 3h', axis=1))\n",
        "# need to consider constant manually\n",
        "\n",
        "logitmod = sm.Logit(Y, X)\n",
        "result = logitmod.fit(maxiter=30)\n",
        "# Summary of the fitted model\n",
        "print(result.summary())"
      ],
      "id": "13ab9cf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ **coef:** the coefficients of the independent variables in the logistic regression equation are interpreted a little bit differently than linear regression; for example, if `borough_MANHATTAN` increases by one unit and all else is held constant, we expect the *log odds* to decrease by 0.7192 units. According to this model, we can expect it is less likely for response time to be under three hours for a 311 service request in Manhattan compared to Queens (reference level). On the other hand, if \n",
        "`borough_BRONX` increases by one unit and all else is held constant, we expect the log odds to \n",
        "increase by 0.1047 units. We can expect it is more likely for response time to be under three hours \n",
        "for a 311 service request in the Bronx compared to Queens. If we want to look at comparisons between \n",
        "`Open Data Channel Type`, from this model, we can also see that 311 requests in the dataset that were \n",
        "submitted via phone call are more likely to have a response time under three hours compared to those \n",
        "that were submitted via mobile. \n",
        "\n",
        "+ **Log-Likelihood:** the natural logarithm of the Maximum Likelihood Estimation(MLE) function. MLE is the optimization process of finding the set of parameters that result in the best fit. Log-likelihood on its own doesn't give us a lot of information, but comparing this value from two different models with the same number of predictors can be useful. Higher log-likelihood indicates a better fit. \n",
        "\n",
        "+ **LL-Null:** the value of log-likelihood of the null model (model with no predictors, just intercept). \n",
        "\n",
        "+ **Pseudo R-squ.:** similar but not exact equivalent to the R-squared value in Least Squares linear regression. This is also known as McFadden's R-Squared, and is computed as $1-\\dfrac{L_1}{L_0}$, where $L_0$ is the log-likelihood of the null model and $L_1$ is that of the full model.\n",
        "\n",
        "+ **LLR p-value:** the p-value of log-likelihood ratio test statistic comparing the full model to the null model. Assuming a significance level $\\alpha$ of 0.05, if this p-value $\\leq \\alpha,$ then we reject the null hypothesis that the model is not significant. We reject the null hypothesis; thus we can conclude this model has predictors that are significant (non-zero coefficients).\n",
        "\n",
        "\n",
        "Another example:\n",
        "\n",
        "We will use the `macrodata` dataset directly from `statsmodels`, which contains \n",
        "information on macroeconomic indicators in the US across different quarters from 1959 to \n",
        "2009, such as unemployment rate, inflation rate, real gross domestic product, etc. \n",
        "I have created a binary variable `morethan5p` that has a value of 1 when the \n",
        "unemployment rate is more than 5% in a given quarter, and is 0 when it is equal \n",
        "to or less than 5%. We are creating a logistic regression model that can be used to \n",
        "estimate the odds ratio of the unemployment rate being greater than 5% based on \n",
        "`cpi` (end-of-quarter consumer price index) and `pop` (end-of-quarter population) \n",
        "as predictors. "
      ],
      "id": "c558ebe9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# df2 is an instance of a statsmodels dataset class\n",
        "df2 = sm.datasets.macrodata.load_pandas()\n",
        "# add binary variable\n",
        "df2.data['morethan5p'] = (df2.data['unemp']>5).apply(lambda x:int(x))\n",
        "# Subset data\n",
        "df2 = df2.data[['morethan5p','cpi','pop']]\n",
        "# Logit regression model\n",
        "model = smf.logit(\"morethan5p ~ cpi + pop\", df2)\n",
        "result2 = model.fit()\n",
        "summary = result2.summary()\n",
        "print(summary)"
      ],
      "id": "c386fa0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute odds ratios and other information by calling methods on the \n",
        "fitted result object. Below are the 95% confidence intervals of the odds ratio \n",
        "$e^{\\text{coef}}$ of each coefficient:"
      ],
      "id": "46b60b72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "odds_ratios = pd.DataFrame(\n",
        "    {\n",
        "        \"Odds Ratio\": result2.params,\n",
        "        \"Lower CI\": result2.conf_int()[0],\n",
        "        \"Upper CI\": result2.conf_int()[1],\n",
        "    }\n",
        ")\n",
        "odds_ratios = np.exp(odds_ratios)\n",
        "\n",
        "print(odds_ratios)"
      ],
      "id": "18f31ee2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note these are no longer *log odds* we are looking at! We estimate with 95% confidence \n",
        "that the true odds ratio lies between the lower CI and upper CI for each coefficient. \n",
        "A larger odds ratio is associated with a larger probability that the unemployment \n",
        "rate is greater than 5%.\n",
        "\n",
        "#### Poisson Regression\n",
        "\n",
        "This type of regression is best suited for modeling the how the mean of a discrete \n",
        "variable depends on one or more predictors.\n",
        "\n",
        "The log of the probability of success is modeled by:\n",
        "\n",
        "$\\log(\\mu) = b_0 + b_1x_1 + ... + b_kx_k$\n",
        "\n",
        "where $\\mu$ is the probability of success (the response variable). The intercept `b0` is \n",
        "assumed to be 0 if not provided in the model. We will use `.add_constant` to indicate \n",
        "that our model includes an intercept term.\n",
        "\n",
        "Let's use the `sunspots` dataset from `statsmodels`. This is a one variable dataset that counts \n",
        "the number of sunspots that occur in a given year (from 1700 - 2008). Note that the link function \n",
        "for Poisson regression is a log function, which means $\\log{E(Y)}=X\\beta.$\n",
        "\n",
        "We first load an instance \n",
        "of a `statsmodels` dataset class, analogous to a `pandas` dataframe:"
      ],
      "id": "3d653141"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3 = sm.datasets.sunspots.load_pandas()\n",
        "df3 = df3.data\n",
        "\n",
        "df3['YEAR'] = df3['YEAR'].apply(lambda x: x-1700)\n",
        "# YEAR is now number of years after 1700, scaling the data for better results\n",
        "df3['YEAR2'] = df3['YEAR'].apply(lambda x: x**2)\n",
        "# YEAR2 is YEAR squared, used as additional predictor\n",
        "\n",
        "X = sm.add_constant(df3[['YEAR','YEAR2']]) \n",
        "# .add_constant indicates that our model includes an intercept term\n",
        "Y = df3['SUNACTIVITY']\n",
        "\n",
        "print(df3[['YEAR','YEAR2','SUNACTIVITY']].head())"
      ],
      "id": "c3614804",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code above, we are altering our predictors a little bit from the \n",
        "orignal dataset; we are substracting the minimum year 1700 from all `YEAR` \n",
        "values so it is more centered. It is generally good practice to scale and \n",
        "center your data so that the model can have better fit. In our case this also \n",
        "aids the interpretability of the intercept coefficient we will see later. \n",
        "We are adding the varaible `YEAR2`, which is the number of years since \n",
        "1700 squared to see if there is some non-linear relationship that may exist. \n",
        "\n",
        "We can use the `.GLM` function with the `family='poisson'` argument to fit our \n",
        "model. Some important parameters:\n",
        "\n",
        "+ `data.endog` acts as a series of observations for the dependent variable $Y$\n",
        "+ `data.exog` acts as a series of observations for each predictor\n",
        "+ `family` specifies the distribution appropriate for the model"
      ],
      "id": "3e8cd531"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result3 = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n",
        "print(result3.summary())"
      ],
      "id": "43d91f00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ **coef:** In this model, increasing the `YEAR` seems to increase the log of the expected \n",
        "count of Sunspot activity (`SUNACTIVITY`) by a small amount; the expected count of sunspot \n",
        "activty increases by $e^{.0003}$ (note that increasing `YEAR` also increases `YEAR2` so we \n",
        "have to be careful with interpretability!) This model also suggests that the number of \n",
        "sunspots is for the year 1700 is estimated to be $e^{3.6781}\\approx39.57$, while the number of \n",
        "actual sunspots that year was 5.\n",
        "+ **Deviance:** two times the difference between the log-likelihood of a fitted GLM and \n",
        "the log-likelihood of a perfect model where fitted responses match observed responses. \n",
        "A greater deviance indicates a worse fit.\n",
        "+ **Pearson chi2:** measures the goodness-of-fit of the model based on the square deviations \n",
        "between observed and expected values based on the model. A large value suggests that the model \n",
        "does not fit well.\n",
        "\n",
        "### Diagnostic Tests\n",
        "\n",
        "Throughout the GLMs listed above, we can find different statistics to assess how well the model fits the data. They include:\n",
        "\n",
        "+ **Deviance**: Measures the goodness-of-fit by taking the difference between the log-likelihood of \n",
        "a fitted GLM and the log-likelihood of a perfect model where fitted responses match observed responses. \n",
        "A larger deviance indicates a worse fit for the model. This is a test statistic for Likelihood-ratio tests compared to a chi-squared distribution with $df=df_{\\text{full}}-df_{\\text{null}}$, for comparing \n",
        "a full model against a null model (or some reduced model) similar to a \n",
        "partial F-test. "
      ],
      "id": "be76bbfb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Poisson Regression Deviance:\", result3.deviance)"
      ],
      "id": "acd3241d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ **Pearson's chi-square test:** This tests whether the predicted probabilities from the model differ significantly from the observed counts. The test statistic is calculated by taking the difference between the null deviance (deviance of a model with just the intercept term) and residual deviance (how well the response variable can be predicted by a model with a given number of predictors). Large Pearson’s chi-squares indicate poor fit."
      ],
      "id": "9388e47c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Chi Squared Stat:\",result3.pearson_chi2)"
      ],
      "id": "99080c83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ **Residual Plots:** Like in linear regression, we can visually plot residuals to look for patterns that shouldn't be there. There are different types of residuals that we can look at, such as deviance residuals:"
      ],
      "id": "588d9b92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize=(8, 4))\n",
        "plt.scatter(df3['YEAR'],result3.resid_deviance)"
      ],
      "id": "adbec4f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nonparametric Models\n",
        "\n",
        "#### Kernel Density Estimation\n",
        "\n",
        "`statsmodels` has a non-parametric approach called kernel density estimation (KDE), \n",
        "which estimates the underlying probability of a given assortment of data points. \n",
        "KDE is used when you don't have enough data points to form a parametric model. \n",
        "It estimates the density of continuous random variables, or extrapolates some \n",
        "continuous function from discrete counts. KDE is a non-parametric way to estimate \n",
        "the underlying distribution of data. The KDE weights all the distances of all data \n",
        "points relative to every location. The more data points there are at a given \n",
        "location, the higher the KDE estimate at that location. Points closer to a given \n",
        "location are generally weighted more than those further away. The shape of the \n",
        "kernel function itself indicates how the point distances are weighted. For example, \n",
        "a uniform kernel function will give equal weighting across all values within a \n",
        "bandwidth, whereas a triangle kernel function gives weighting dependent on linear \n",
        "distance.\n",
        "\n",
        "KDE can be applied for univariate or multivariate data. `statsmodels` has two methods for this:\n",
        "- `sm.nonparametric.KDEunivariate`: For univariate data. This estimates the \n",
        "bandwidth using Scott’s rule unless specified otherwise. Much faster than \n",
        "using `.KDEMultivariate` due to its use of Fast Fourier Transforms on \n",
        "univariate, continuous data.\n",
        "- `sm.nonparametric.KDEMultivariate`: This applies to both univariate and \n",
        "multivariate data, but tends to be slower. Can use mixed types of data but requires specification.\n",
        "\n",
        "Here we will demonstrate how to apply it to univariate data, based off of \n",
        "examples provided in the [documentation](https://www.statsmodels.org/stable/examples/notebooks/generated/kernel_density.html#Comparing-kernel-functions). \n",
        "We will generate a histogram of based off of [geyser waiting time](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/faithful) data from Rdatasets. This dataset records the waiting time between \"Old Faithful\" geyser's eruptions in Yellowstone National Park. Our goal is to fit a KDE with a Gaussian kernel function to this data."
      ],
      "id": "f0a0fc75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data\n",
        "df5 = sm.datasets.get_rdataset(\"faithful\", \"datasets\")\n",
        "waiting_obs = df5.data['waiting'] \n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(8, 4))\n",
        "ax = fig.add_subplot()\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_xlabel(\"Time (min)\")\n",
        "\n",
        "ax.hist(\n",
        "    waiting_obs,\n",
        "    bins=25, \n",
        "    color=\"darkblue\",\n",
        "    edgecolor=\"w\", \n",
        "    alpha=0.8,\n",
        "    label=\"Histogram\"\n",
        ")\n",
        "\n",
        "ax.scatter(\n",
        "    waiting_obs,\n",
        "    np.abs(np.random.randn(waiting_obs.size)),\n",
        "    color=\"orange\",\n",
        "    marker=\"o\",\n",
        "    alpha=0.5,\n",
        "    label=\"Samples\",\n",
        ")\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, alpha=0.35)"
      ],
      "id": "a68b24b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we want to fit our KDE based on our `waiting_obs` sample:"
      ],
      "id": "6f5b2141"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kde = sm.nonparametric.KDEUnivariate(waiting_obs)\n",
        "kde.fit()  # Estimate the densities\n",
        "print(\"Estimated Bandwidth:\", kde.bw)  \n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(8, 4))\n",
        "ax1 = fig.add_subplot()\n",
        "ax1.set_ylabel(\"Count\")\n",
        "ax1.set_xlabel(\"Time (min)\")\n",
        "\n",
        "ax1.hist(\n",
        "    waiting_obs,\n",
        "    bins=25, \n",
        "    color=\"darkblue\",\n",
        "    edgecolor=\"w\", \n",
        "    alpha=0.8,\n",
        "    label=\"Histogram\",\n",
        ")\n",
        "\n",
        "ax1.scatter(\n",
        "    waiting_obs,\n",
        "    np.abs(np.random.randn(waiting_obs.size)),\n",
        "    color=\"orange\",\n",
        "    marker=\"o\",\n",
        "    alpha=0.5,\n",
        "    label=\"Waiting times\",\n",
        ")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(\n",
        "    kde.support, \n",
        "    kde.density, \n",
        "    lw=3, \n",
        "    label=\"KDE\")\n",
        "ax2.set_ylabel(\"Density\")\n",
        "\n",
        "# Joining legends\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
        "\n",
        "ax1.grid(True, alpha=0.35)"
      ],
      "id": "784f5268",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When fitting the KDE, a `kde.bw` or bandwidth parameter is returned.\n",
        "We can alter this to see how it affects the \n",
        "fit and smoothness of the curve. The smaller the bandwidth, the more jagged the \n",
        "estimated distribution becomes."
      ],
      "id": "1945bc21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(8, 4))\n",
        "ax1 = fig.add_subplot()\n",
        "ax1.set_ylabel(\"Count\")\n",
        "ax1.set_xlabel(\"Time (min)\")\n",
        "\n",
        "ax1.hist(\n",
        "    waiting_obs,\n",
        "    bins=25, \n",
        "    color=\"darkblue\",\n",
        "    edgecolor=\"w\", \n",
        "    alpha=0.8,\n",
        "    label=\"Histogram\"\n",
        ")\n",
        "\n",
        "ax1.scatter(\n",
        "    waiting_obs,\n",
        "    np.abs(np.random.randn(waiting_obs.size)),\n",
        "    color=\"orange\",\n",
        "    marker=\"o\",\n",
        "    alpha=0.5,\n",
        "    label=\"Samples\",\n",
        ")\n",
        "\n",
        "# Plot the KDE for various bandwidths\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Density\")\n",
        "\n",
        "for (bandwidth, color) in [(0.5,\"cyan\"), (4,\"#bbaa00\"), (8,\"#ff79ff\")]:\n",
        "    kde.fit(bw=bandwidth)  # Estimate the densities\n",
        "    ax2.plot(\n",
        "        kde.support,\n",
        "        kde.density,\n",
        "        \"--\",\n",
        "        lw=2,\n",
        "        color=color,\n",
        "        label=f\"KDE from samples, bw = {bandwidth}\",\n",
        "        alpha=0.9\n",
        "    )\n",
        "ax1.legend(loc=\"best\")\n",
        "ax2.legend(loc=\"best\")\n",
        "ax1.grid(True, alpha=0.35)"
      ],
      "id": "e5487fef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n",
        "\n",
        "* Installing `statsmodels`:\n",
        "    + <https://www.statsmodels.org/stable/install.html>\n",
        "\n",
        "* `Rdatasets` repository and `statsmodels` datasets:\n",
        "    + <https://github.com/vincentarelbundock/Rdatasets/blob/master/datasets.csv>\n",
        "    + <https://cran.r-project.org/web/packages/causaldata/causaldata.pdf>\n",
        "    + <https://hassavocadoboard.com/>\n",
        "    + <https://www.statsmodels.org/stable/datasets/index.html>\n",
        "    + <https://www.statsmodels.org/stable/datasets/generated/sunspots.html>\n",
        "    + <https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/faithful>\n",
        "\n",
        "* NYC 311 Service Request Data:\n",
        "    + <https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data>\n",
        "\n",
        "* Getting help with `statsmodels`:\n",
        "    + <https://www.statsmodels.org/stable/generated/statsmodels.tools.web.webdoc.html#statsmodels.tools.web.webdoc>\n",
        "    + <https://www.statsmodels.org/stable/endog_exog.html>\n",
        "\n",
        "* Loading data, model fit, and summary procedure:\n",
        "    + <https://www.statsmodels.org/stable/gettingstarted.html>\n",
        "\n",
        "* Summary Data Interpretation:\n",
        "    + <https://www.statology.org/a-simple-guide-to-understanding-the-f-test-of-overall-significance-in-regression/>\n",
        "    + <https://www.statology.org/linear-regression-p-value/>\n",
        "    + <https://www.statology.org/omnibus-test/>\n",
        "    + <https://www.statisticshowto.com/jarque-bera-test/>\n",
        "    + <https://www.statology.org/how-to-report-skewness-kurtosis/>\n",
        "    + <https://www.statology.org/interpret-log-likelihood/>\n",
        "    + <https://stackoverflow.com/questions/46700258/python-how-to-interpret-the-result-of-logistic-regression-by-sm-logit>\n",
        "    + <https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/>\n",
        "    + <https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/>\n",
        "    + <https://vulstats.ucsd.edu/chi-squared.html>\n",
        "    + <https://roznn.github.io/GLM/sec-deviance.html>\n",
        "\n",
        "* Generalized Linear Models:\n",
        "    + <https://sscc.wisc.edu/sscc/pubs/glm-r/>\n",
        "    + <https://online.stat.psu.edu/stat504/lesson/6/6.1>\n",
        "    + <https://www.mygreatlearning.com/blog/generalized-linear-models/>\n",
        "    + <https://www.statsmodels.org/stable/examples/notebooks/generated/glm.html>\n",
        "\n",
        "* Logistic Regression:\n",
        "    + <https://www.andrewvillazon.com/logistic-regression-python-statsmodels/>\n",
        "    + <https://towardsdatascience.com/how-to-interpret-the-odds-ratio-with-categorical-variables-in-logistic-regression-5bb38e3fc6a8>\n",
        "    + <https://towardsdatascience.com/a-simple-interpretation-of-logistic-regression-coefficients-e3a40a62e8cf>\n",
        "    + <https://www.statology.org/interpret-log-likelihood/>\n",
        "\n",
        "* Poisson Regression:\n",
        "    + <https://tidypython.com/poisson-regression-in-python/>\n",
        "\n",
        "* Non-parametric Methods:\n",
        "    + <https://mathisonian.github.io/kde/>\n",
        "    + <https://www.statsmodels.org/stable/nonparametric.html>\n",
        "    + <https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kde.KDEUnivariate.html>\n",
        "    + <https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html>\n",
        "    + <https://www.statsmodels.org/stable/examples/notebooks/generated/kernel_density.html>\n",
        "\n",
        "* Diagnostic tests:\n",
        "    + <https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests>\n",
        "    + <https://bookdown.org/ltupper/340f21_notes/deviance-and-residuals.html>\n",
        "    + <https://www.statology.org/null-residual-deviance/>\n",
        "\n",
        "* Data Visualization:\n",
        "    + <https://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend>\n"
      ],
      "id": "9d07740a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}