{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Import/Export\n",
        "\n",
        "Working with data is a fundamental aspect of data science in Python,\n",
        "with data import and export being crucial skills. Throughout, we will\n",
        "use the 311 service request data for illustrations, downloaded from\n",
        "the NYC Open Data as a `csv` file.\n",
        "\n",
        "## Using the `Pandas` Package\n",
        "\n",
        "The pandas library simplifies data manipulation and analysis. It's\n",
        "especially handy for dealing with CSV files.\n"
      ],
      "id": "7ab6131a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file name\n",
        "csvnm = \"data/rodent_2022-2023.csv\"\n",
        "\n",
        "# Specify the strings that indicate missing values\n",
        "# Q: How would you know these?\n",
        "na_values = [\n",
        "    \"\",\n",
        "    \"0 Unspecified\",\n",
        "    \"N/A\",\n",
        "    \"na\",\n",
        "    \"na na\",\n",
        "    \"Unspecified\",\n",
        "    \"UNKNOWN\",\n",
        "]\n",
        "\n",
        "def custom_date_parser(x):\n",
        "    return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(\n",
        "    csvnm,\n",
        "    na_values = na_values,\n",
        "    parse_dates = ['Created Date', 'Closed Date'], \n",
        "    date_parser = custom_date_parser,\n",
        "    dtype = {'Latitude': 'float32', 'Longitude': 'float32'},\n",
        ")\n",
        "\n",
        "# Strip leading and trailing whitespace from the column names\n",
        "df.columns = df.columns.str.strip()\n",
        "df.columns = df.columns.str.replace(' ', '_', regex = False).str.lower()\n",
        "\n",
        "# Drop the 'Location' since it is redundant\n",
        "# df.drop(columns=['Location'], inplace=True)"
      ],
      "id": "5da25619",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `pandas` package also provides some utility functions for quick\n",
        "summaries about the data frame."
      ],
      "id": "3b2d0302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.shape\n",
        "df.describe()\n",
        "df.isnull().sum()"
      ],
      "id": "e9d48ca3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are the unique values of `descriptor`?"
      ],
      "id": "ee6660f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.descriptor.unique()"
      ],
      "id": "a4c8b8b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filling Missing Values\n",
        "\n",
        "If geocodes are available but zip code is missing, we can use\n",
        "reverse geocoding to fill the zip code. This process involves\n",
        "querying a geocoding service with latitude and longitude to\n",
        "get the corresponding address details, including the ZIP code.\n",
        "This can be done with package `geopy`, which needs to be\n",
        "installed first: `pip install geopy`.\n"
      ],
      "id": "3a85a554"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "\n",
        "# Initialize the geocoder\n",
        "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
        "\n",
        "# Function for reverse geocoding\n",
        "def reverse_geocode(lat, lon):\n",
        "    try:\n",
        "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
        "        address = location.raw.get('address', {})\n",
        "        zip_code = address.get('postcode')\n",
        "        return zip_code\n",
        "    except (GeocoderTimedOut, GeocoderServiceError):\n",
        "        # Handle errors or timeouts\n",
        "        return None\n",
        "\n",
        "# Apply reverse geocoding to fill missing ZIP codes\n",
        "for index, row in df.iterrows():\n",
        "    if pd.isnull(row['incident_zip']) and pd.notnull(row['latitude']) and pd.notnull(row['longitude']):\n",
        "        df.at[index, 'incident_zip'] = reverse_geocode(row['latitude'], row['longitude'])\n",
        "\n",
        "# Note: This can be slow for large datasets due to API rate\n",
        "# limits and network latency"
      ],
      "id": "a442f837",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Appache `Arrow` Library\n",
        "\n",
        "To read and export data efficiently, leveraging the Apache `Arrow`\n",
        "library can significantly improve performance and storage efficiency,\n",
        "especially with large datasets. The IPC (Inter-Process Communication)\n",
        "file format in the context of Apache Arrow is a key component for\n",
        "efficiently sharing data between different processes, potentially\n",
        "written in different programming languages. Arrow's IPC mechanism is\n",
        "designed around two main file formats:\n",
        "\n",
        "+ Stream Format: For sending an arbitrary length sequence of Arrow\n",
        "record batches (tables). The stream format is useful for real-time\n",
        "data exchange where the size of the data is not known upfront and can\n",
        "grow indefinitely.\n",
        "+ File (or Feather) Format: Optimized for storage and memory-mapped\n",
        "access, allowing for fast random access to different sections of the\n",
        "data. This format is ideal for scenarios where the entire dataset is\n",
        "available upfront and can be stored in a file system for repeated\n",
        "reads and writes.\n",
        "\n",
        "\n",
        "Apache Arrow provides a columnar\n",
        "memory format for flat and hierarchical data, optimized for efficient\n",
        "data analytics. It can be used in Python through the `pyarrow`\n",
        "package. Here's how you can use Arrow to read, manipulate, and export\n",
        "data, including a demonstration of storage savings.\n",
        "\n",
        "\n",
        "First, ensure you have pyarrow installed on your computer (and\n",
        "preferrably, in your current virtual environment):\n",
        "```\n",
        "pip install pyarrow\n",
        "```\n",
        "\n",
        "\n",
        "Feather is a fast, lightweight, and easy-to-use binary file format for\n",
        "storing data frames, optimized for speed and efficiency, particularly\n",
        "for IPC and data sharing between Python and R."
      ],
      "id": "76c2a750"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.to_feather('data/rodent_2022-2023.feather')"
      ],
      "id": "f630d494",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the feather file back in:"
      ],
      "id": "30af5a7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dff = pd.read_feather(\"data/rodent_2022-2023.feather\")\n",
        "dff.shape"
      ],
      "id": "245ea1de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benefits of Using Feather:\n",
        "\n",
        "+ Efficiency: Feather is designed to support fast reading and writing\n",
        "of data frames, making it ideal for analytical workflows that need to\n",
        "exchange large datasets between Python and R.\n",
        "+ Compatibility: Maintains data type integrity across Python and R,\n",
        "ensuring that numbers, strings, and dates/times are correctly handled\n",
        "and preserved.\n",
        "+ Simplicity: The API for reading and writing Feather files is\n",
        "straightforward, making it accessible to users with varying levels of\n",
        "programming expertise.\n",
        "\n",
        "By using Feather format for data storage, you leverage a modern\n",
        "approach optimized for speed and compatibility, significantly\n",
        "enhancing the performance of data-intensive applications.\n",
        "\n",
        "## Accessing the Census Data with `uszipcode`\n",
        "\n",
        "First, ensure the DataFrame (df) is ready for merging with census\n",
        "data. Specifically, check that the `incident_zip` column is clean\n",
        "and consistent."
      ],
      "id": "2dd77ec0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df['incident_zip'].isnull().sum())\n",
        "# Standardize to 5-digit codes, if necessary\n",
        "df['incident_zip'] = df['incident_zip'].astype(str).str.zfill(5) "
      ],
      "id": "e3b1e311",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `uszipcode` package to get basic demographic data\n",
        "for each zip code. For more detailed or specific census data, \n",
        "using the `CensusData` package or direct API calls to the Census\n",
        "Bureau's API.\n",
        "\n",
        "\n",
        "The `uszipcode` package provides a range of information about\n",
        "ZIP codes in the United States. When you query a ZIP code using\n",
        "`uszipcode`, you can access various attributes related to\n",
        "demographic data, housing, geographic location, and more. Here\n",
        "are some of the key variables available at the ZIP code level:\n",
        "\n",
        "\n",
        "**emographic Information**\n",
        "\n",
        "+ `population`: The total population.\n",
        "+ `population_density`: The population per square kilometer.\n",
        "+ `housing_units`: The total number of housing units.\n",
        "+ `occupied_housing_units`: The number of occupied housing units.\n",
        "+ `median_home_value`: The median value of homes.\n",
        "+ `median_household_income`: The median household income.\n",
        "+ `age_distribution`: A breakdown of the population by age.\n",
        "\n",
        "**Geographic Information**\n",
        "\n",
        "+ `zipcode`: The ZIP code.\n",
        "+ `zipcode_type`: The type of ZIP code (e.g., Standard, PO Box).\n",
        "+ `major_city`: The major city associated with the ZIP code.\n",
        "+ `post_office_city`: The city name recognized by the U.S. Postal Service.\n",
        "+ `common_city_list`: A list of common city names for the ZIP code.\n",
        "+ `county`: The county in which the ZIP code is located.\n",
        "+ `state`: The state in which the ZIP code is located.\n",
        "+ `lat`: The latitude of the approximate center of the ZIP code.\n",
        "+ `lng`: The longitude of the approximate center of the ZIP code.\n",
        "+ `timezone`: The timezone of the ZIP code.\n",
        "\n",
        "**Economic and Housing Data**\n",
        "\n",
        "+ `land_area_in_sqmi`: The land area in square miles.\n",
        "+ `water_area_in_sqmi`: The water area in square miles.\n",
        "+ `occupancy_rate`: The rate of occupancy for housing units.\n",
        "+ `median_age`: The median age of the population.\n",
        "\n",
        "\n",
        "Install the `uszipcode` package into the current virtual environment\n",
        "by `pip install uszipcode`. \n",
        "\n",
        "Now let's work on the rodent sightings data.\n",
        "\n",
        "We will first clean the incident_zip column to ensure it only\n",
        "contains valid ZIP codes. Then, we will use a vectorized\n",
        "approach to fetch the required data for each unique ZIP code\n",
        "and merge this information back into the original `DataFrame`.\n"
      ],
      "id": "e010c307"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remove rows where 'incident_zip' is missing or not a valid ZIP code format\n",
        "valid_zip_df = df.dropna(subset=['incident_zip']).copy()\n",
        "valid_zip_df['incident_zip'] = valid_zip_df['incident_zip'].astype(str).str.zfill(5)\n",
        "unique_zips = valid_zip_df['incident_zip'].unique()"
      ],
      "id": "c7bfa54c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since `uszipcode` doesn't inherently support vectorized operations\n",
        "for multiple ZIP code queries, we'll optimize the process by\n",
        "querying each unique ZIP code once, then merging the results\n",
        "with the original `DataFrame`. This approach minimizes redundant\n",
        "queries for ZIP codes that appear multiple times.\n"
      ],
      "id": "f4053e8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from uszipcode import SearchEngine\n",
        "\n",
        "# Initialize the SearchEngine\n",
        "search = SearchEngine()\n",
        "\n",
        "# Fetch median home value and median household income for each unique ZIP code\n",
        "zip_data = []\n",
        "zip_data = []\n",
        "for zip_code in unique_zips:\n",
        "    result = search.by_zipcode(zip_code)\n",
        "    if result:  # Check if the result is not None\n",
        "        zip_data.append({\n",
        "            \"incident_zip\": zip_code,\n",
        "            \"median_home_value\": result.median_home_value,\n",
        "            \"median_household_income\": result.median_household_income\n",
        "        })\n",
        "    else:  # Handle the case where the result is None\n",
        "        zip_data.append({\n",
        "            \"incident_zip\": zip_code,\n",
        "            \"median_home_value\": None,\n",
        "            \"median_household_income\": None\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "zip_info_df = pd.DataFrame(zip_data)\n",
        "\n",
        "# Merge this info back into the original DataFrame based on 'incident_zip'\n",
        "merged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"incident_zip\")\n",
        "\n",
        "merged_df.columns"
      ],
      "id": "6f2129c5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}