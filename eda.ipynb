{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "\n",
        "## Descriptive Statistics\n",
        "\n",
        "> Presented by Joshua Lee\n",
        "\n",
        "When you first begin working with a new dataset, it is important \n",
        "to develop an understanding of the data's overall behavior. This\n",
        "is important for both understanding numerical and categorical \n",
        "data. \n",
        "\n",
        "For numeric data, we can develop this understanding through the use\n",
        "of **descriptive statistics**. The goal of descriptive statistics\n",
        "is to understand three primary elements of a given variable [2]:\n",
        "\n",
        "+ **distribution**\n",
        "+ **central tendency**\n",
        "+ **variability**\n",
        "\n",
        "### Variable Distributions\n",
        "\n",
        "Every random variable is given by a **probability distribution**, \n",
        "which is \"a mathematical function that describes the probability of \n",
        "different possible values of a variable\" [3]. \n",
        "\n",
        "There are a few common types of distributions which appear \n",
        "frequently in real-world data [3]:\n",
        "\n",
        "+ **Uniform**: \n",
        "+ **Poisson**:\n",
        "+ **Binomial**:\n",
        "+ **Normal and Standard Normal**:\n",
        "+ **Gamma**: \n",
        "+ **Chi-squared**:\n",
        "+ **Exponential**\n",
        "+ **Beta**\n",
        "+ **T-distribution**\n",
        "+ **F-distribution**\n",
        "\n",
        "Understanding the distribution of different variables in a given \n",
        "dataset can inform how we may decide to transform that data. \n",
        "For example, in the context of the `rodent` data, we are interested\n",
        "in the patterns which are associated with \"rodent\" complaints which \n",
        "occur. "
      ],
      "id": "62e90be5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "data = pd.read_feather(\"data/rodent_2022-2023.feather\")"
      ],
      "id": "dd71092c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have read in the data, we can examine the distributions of \n",
        "several important variables. Namely, let us examine a numerical \n",
        "variable which is associated with rodent sightings: "
      ],
      "id": "a7a4c584"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data.head(2).T"
      ],
      "id": "7db8fc74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this dataset, the most relevant numerical data to consider is the \n",
        "time between the opening of a rodent complaint and its closing. All\n",
        "of the other relevant variables are either geospatial or categorical:"
      ],
      "id": "f14c2a22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert strings into datetime objects\n",
        "data[\"closed_date\"] =  pd.to_datetime(data[\"closed_date\"],\n",
        "                                     format=\"%m/%d/%Y %I:%M:%S %p\")\n",
        "data[\"created_date\"] = pd.to_datetime(data[\"created_date\"],\n",
        "                                      format=\"%m/%d/%Y %I:%M:%S %p\")\n",
        "\n",
        "data[\"time_dif\"] = data[\"closed_date\"] - data[\"created_date\"]\n",
        "\n",
        "# set the time delta as the number of hours difference\n",
        "data[\"time_dif\"] = data[\"time_dif\"].dt.total_seconds()/3600\n",
        "data[\"time_dif\"]"
      ],
      "id": "6b2241c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a column describing the time difference between when \n",
        "a complaint is opened and closed. We can plot this distribution \n",
        "with `plotly` to provide a better visual representation of the \n",
        "distribution: \n",
        "\n",
        "> Note, every value in the data is shifted up 1 for plotting \n",
        "purposes. Fitting an exponential distribution with parameter \n",
        "$\\lambda=0$ exactly is not possible to fit precisely due to \n",
        "divide by $0$ errors. Additionally, this plot ignores the location \n",
        "parameter provided by output from `stats.expon.fit()` since \n",
        "the mean brought up significantly by outliers at the \n",
        "asbolute extremes of the distribution (the higher end)."
      ],
      "id": "b5c96e3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.graph_objects as go\n",
        "from scipy import stats\n",
        "\n",
        "# add a 1 to avoid weird zero errors\n",
        "response_dat2 = data[\"time_dif\"].dropna() + 1\n",
        "\n",
        "hist2 = go.Histogram(x=response_dat2, \n",
        "                    nbinsx=2500, \n",
        "                    opacity=0.75, \n",
        "                    name='response time', \n",
        "                    histnorm='probability density')\n",
        "\n",
        "# Calculate KDE\n",
        "scale, loc = stats.expon.fit(response_dat2.values)\n",
        "x_range = np.linspace(min(response_dat2), max(response_dat2), 10000)\n",
        "fitted_vals = stats.expon.pdf(x_range, loc=0.2, scale=scale)\n",
        "fitted_dist = go.Scatter(x=x_range, y=fitted_vals, mode=\"lines\", \n",
        "                         name=\"Fitted Exponential Distribution\")\n",
        "\n",
        "# Create a layout\n",
        "layout = go.Layout(title='Complaint Response Time Histogram and Density',\n",
        "                   xaxis=dict(title='Complaint Response Time (hours)', range=[0,100]),\n",
        "                   yaxis=dict(title='Density', range=[0,0.2]),\n",
        "                   bargap=0.1\n",
        "                  )\n",
        "\n",
        "# Create a figure and add both the histogram and KDE\n",
        "fig = go.Figure(data=[hist2, fitted_dist], layout=layout)\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "id": "edc24ff9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, there is a strong right skew (the majority of observations\n",
        "are concentrated at the lower end of the distribution, but there \n",
        "are a few observations at the extreme right end).\n",
        "\n",
        "Here, we use pandas plotting to generate a density estimation \n",
        "curve."
      ],
      "id": "96f617ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_range = np.linspace(response_dat2.min(), response_dat2.max(), 1000)\n",
        "response_dat2.plot.kde(ind=x_range)"
      ],
      "id": "d4d27202",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare this density curve to plots of the exponential \n",
        "distribution, and see that this variable (complaint response times)\n",
        "closely match an exponential distribution with a very high $\\lambda$\n",
        "parameter value. Below is a figure displaying a series of \n",
        "exponential distributions for different values of $\\lambda$:"
      ],
      "id": "2456c381"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the lambda parameters\n",
        "lambdas = [0.5, 1, 2, 4, 8]\n",
        "\n",
        "# Define the x range\n",
        "x = np.linspace(0, 2*np.pi, 1000)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the exponential distribution for each lambda\n",
        "for lam in lambdas:\n",
        "    y = lam * np.exp(-lam * x)\n",
        "    plt.plot(x, y, label=f'λ = {lam}')\n",
        "\n",
        "# Set the x-axis labels\n",
        "plt.xticks([np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], ['π/2', 'π', '3π/2', '2π'])\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "80b01a64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Central Tendency Measures\n",
        "\n",
        "Now that we have examined the distribution of the \n",
        "response time, it is appropriate to investigate the \n",
        "important measures of central tendency for the data.\n",
        "\n",
        "There are three main measures of central tendency which are \n",
        "used: \n",
        "\n",
        "+ **Mean**: The average or expected value of a random variable\n",
        "  + $\\overline{X} = (1/n)\\sum_{i=1}^{n} X_{i}$\n",
        "  (where $X_{i}\\text{s}$ are independent random samples from \n",
        "  the same distribution)\n",
        "+ **Median**: exact middle value of a random variable [5]\n",
        "  + For even $n$, $\\overset{\\sim}{X} = (1/2)[X_{(n/2+1)} + X_{(n/2)}]$\n",
        "  + For odd $n$, $\\overset{\\sim}{X} = X_{([n+1]/2)}$\n",
        "+ **Mode**: the most frequently occurring value of a random variable\n",
        "\n",
        "For the given variable (complaint response time), we can find \n",
        "each of the respective statistics using pandas: \n",
        "\n",
        "> NOTE: `pandas.Series.mode()` returns the most commonly occurring\n",
        "value in the `Series`, or a `Series` of the most commonly occurring\n",
        "*values* if there is a tie between multiple values. It does not \n",
        "calculate multiple modes in the case of a multi-modal distribution.\n",
        "Here, `Series.mode()` returns $0$ and $0.000\\dots$ so I elected to \n",
        "choose the first element of that series for display. "
      ],
      "id": "d7c9ca55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "central_tendency = pd.Series(\n",
        "    {\"Mean\": response_dat2.mean(), \n",
        "     \"Median\": response_dat2.median(), \n",
        "     \"Mode\": response_dat2.mode().iloc[0]}\n",
        ")\n",
        "central_tendency"
      ],
      "id": "ffec6c1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the most commonly occurring value (as is obvious from\n",
        "the density plot) is 0. This means that the time between when a \n",
        "rodent sighting complaint is filed and responded to (or closed) is most\n",
        "likely to be 0. Additionally, it implies that more than half of all \n",
        "data points have a complaint response time of zero since the median is \n",
        "zero as well. \n",
        "\n",
        "It makes sense that the mean is greater than the median in this case since\n",
        "the distribution is exponential and skewed to the right. \n",
        "\n",
        "### Variability Measures\n",
        "\n",
        "As with central tendency, there are also several relevant measures of \n",
        "variance [2]. These include: \n",
        "\n",
        "+ **range**: $X_{(n)} - X_{(1)}$ - the difference between the greatest\n",
        "  observed value and the smallest one.\n",
        "+ **standard deviation**: \n",
        "  $S = \\sqrt{(1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}}$ - \n",
        "  the average difference of values from the observed mean of a sample.\n",
        "+ **variance**: Square of the standard deviation of a sample\n",
        "  $S^{2} = (1/[n-1])\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}$\n",
        "+ **Interquartile Range**: $X_{[3/4]} - X_{[1/4]}$ where \n",
        "  $X_{[p]}$ is the $p\\text{th}$ sample quantile - \n",
        "  A measure of the difference between the 1st \n",
        "  and third quantiles of a distribution\n",
        "\n",
        " We can easily calculate all of these values using pandas in python [6]"
      ],
      "id": "d9c756bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quartiles = response_dat2.quantile([0.25, 0.75])\n",
        "iqr = quartiles[0.75] - quartiles[0.25]\n",
        "\n",
        "variability = pd.Series(\n",
        "    {\"range\": response_dat2.max() - response_dat2.min(), \n",
        "     \"standard deviation\": response_dat2.std(), \n",
        "     \"variance\": response_dat2.std()**2, \n",
        "     \"IQR\": iqr}\n",
        ")\n",
        "variability"
      ],
      "id": "7251fe80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use the interquartile range as a means to obtain \n",
        "a rudimentary measure of outliers in the data. Specifically, \n",
        "any observations which are a distance of $1.5 * IQR$ beyond the \n",
        "third or first quartiles. \n",
        "\n",
        "Seeing as the first quartile is also the minimum in this, case\n",
        "we only need to be concerned with outliers at the higher end \n",
        "of the spectrum. We calculate the upper fence for outliers\n",
        "as follows [5]:\n",
        "\n",
        "$\\text{upper fence } = X_{[0.75]} + 1.5\\cdot IQR$"
      ],
      "id": "a6c513f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "upper_lim = quartiles[0.75] + 1.5*iqr\n",
        "\n",
        "outliers = response_dat2[response_dat2 > upper_lim]\n",
        "outliers"
      ],
      "id": "c1d09ce9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the exponential nature of the distribution, it would \n",
        "be interesting to examine the patterns which occur in \n",
        "categorical variables to see if there may be any connections between\n",
        "those variables and the response time. It may also be useful to \n",
        "examine relationships between geospatial data and the response time.\n",
        "\n",
        "### Univariate Categorical Descriptive Statistics\n",
        "\n",
        "Descriptive statistics for categorical data are primarily aimed at \n",
        "understanding the rates of occurrence for different categorical \n",
        "variables. These include the following measures [7]:\n",
        "\n",
        "+ **frequencies**: number of occurrences\n",
        "+ **percentages / relative frequencies**: the percentage of observations \n",
        "  which have a given value for a categorical variable\n",
        "\n",
        "These sorts of metrics are often best represented by frequency \n",
        "distribution tables, pie-charts, and bar charts:\n",
        "\n",
        "For example, let us examine the categorical variable \"Borough\"\n",
        "from the rodent data:"
      ],
      "id": "07b51117"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a frequency distribution table\n",
        "counts = data[\"borough\"].value_counts()\n",
        "proportions = counts/len(data)\n",
        "cumulative_proportions = proportions.cumsum()\n",
        "\n",
        "frequency_table = pd.DataFrame(\n",
        "                    {\"Counts\": counts, \n",
        "                    \"Proportions\": proportions, \n",
        "                    \"Cumulative Proportion\": cumulative_proportions}\n",
        ")\n",
        "frequency_table"
      ],
      "id": "f0dbe697",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table demonstrates that the most significant proportion of \n",
        "rodent sightings occurred in the borough of Brooklyn. Additionally, \n",
        "it indicates that Manhattan and Brooklyn collectively represent more\n",
        "than half of all rodent sightings which occur, while Staten Island\n",
        "in particular represents a relatively small proportion. \n",
        "\n",
        "We can also use bar chart to represent this data: "
      ],
      "id": "0ded4fac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a bar chart\n",
        "fig = go.Figure(data=[go.Bar(x=counts.index, y=counts.values)])\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "id": "f3ddb96c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A pie-chart also serves as a good representation of the relative\n",
        "frequencies of categories:"
      ],
      "id": "d995efa6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = go.Figure(data=[go.Pie(labels=counts.index, values=counts.values, hole=.2)])\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "id": "9846a977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chi-Squared Significance Tests (Contingency Table Testing)\n",
        "\n",
        "In order to determine whether there exists a dependence between \n",
        "several categorical variables, we can use chi-squared contingency\n",
        "table testing. This is also referred to as the chi-squared test\n",
        "of independence [8]. We will examine this topic by investigating\n",
        "the relationship between the borough and the complaint descriptor\n",
        "variables in the rodents data.\n",
        "\n",
        "The first step in conducting a Chi-squared significance test is\n",
        "to construct a **contingency table**. \n",
        "\n",
        "> contingency tables are *frequency tables* of two variables\n",
        "which are presented simultaneously [8].\n",
        "\n",
        "This can be accomplished in python by utilizing the `pd.crosstab()`\n",
        "function"
      ],
      "id": "338449c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# produce a contingency table for viewing\n",
        "contingency_table_view = pd.crosstab(data[\"borough\"], \n",
        "                                     data[\"descriptor\"], \n",
        "                                     margins=True)\n",
        "\n",
        "# produce a contingency table for calculations\n",
        "contingency_table = pd.crosstab(data[\"borough\"], \n",
        "                                     data[\"descriptor\"], \n",
        "                                     margins=False)\n",
        "\n",
        "contingency_table_view"
      ],
      "id": "ad8be895",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have constructed the contingency table, we are ready to \n",
        "begin conducting the signficance tests (for independence of\n",
        "Borough and Descriptor). This requires that we compute the \n",
        "chi-squared statistic. \n",
        "\n",
        "There are multiple steps to computing the chi-squared statistic\n",
        "for this test, but the general test-statistic is computed \n",
        "as follows:\n",
        "\n",
        "$$\\chi_{rows-1 * cols-1}^{2} = \\sum_{cells} \\frac{(O - E)^{2}}{E}$$\n",
        "\n",
        "Here, $E = \\text{row sum} * \\text{col sum}/N$ stands for the expected \n",
        "value of each cell, and $O$ refers to the observed values. Note that \n",
        "$N$ refers to the total observations (the right and lower-most \n",
        "cell value in the contingency table above)\n",
        "\n",
        "First, let's calculate the expected values. This can be accomplished\n",
        "by performing the outer product of row sums and column sums for\n",
        "the contingency table: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{row\\_margins} = \\langle r_{1}, r_{2}, \\dots, r_{n}\\rangle \\\\\n",
        "\\text{col\\_margins} = \\langle c_{1}, c_{2}, \\dots, c_{m}\\rangle \\\\\n",
        "\\text{row\\_margins} \\otimes \\text{col\\_margins} = \\left[\\begin{array}{cccc}\n",
        "    r_{1}c_{1} & r_{1}c_{2} & \\dots & r_{1}c_{m} \\\\\n",
        "    r_{2}c_{1} & r_{2}c_{2} & \\dots & r_{2}c_{m} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    r_{n}c_{1} & r_{n}c_{2} & \\dots & r_{n}c_{m}\n",
        "\\end{array}\\right]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In python this is calculated as: "
      ],
      "id": "8b5c4ce3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "row_margins = contingency_table_view[\"All\"]\n",
        "col_margins = contingency_table_view.T[\"All\"]\n",
        "total = contingency_table_view[\"All\"][\"All\"]\n",
        "\n",
        "expected = np.outer(row_margins, col_margins)/total\n",
        "pd.DataFrame(expected, columns=contingency_table_view.columns).set_index(\n",
        "    contingency_table_view.index\n",
        ")"
      ],
      "id": "ed1b1624",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The chi-squared statistic can be calculated directly from the \n",
        "(component-wise) squared difference between the original contingency\n",
        "table and the expected values presented above divided by the \n",
        "total number of observations. However, we can also use the `scipy.stats` package to perform the contingency test automatically. \n",
        "\n",
        "Before performing this test, let us also examine the relavent \n",
        "hypotheses to this significance test. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& H_{0}: \\text{Rodent complaint type reported and Borough are independent} \\\\\n",
        "& H_{1}: H_{0} \\text{ is false.}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "We assume a significance level of $\\alpha=0.05$ for this test:\n",
        "\n",
        "> NOTE: the contingency table **without row margins** is used for \n",
        "calculating the chi-squared test."
      ],
      "id": "d29483ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "chi2_val, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "pd.Series({\n",
        "    \"Chi-Squared Statistic\": chi2_val, \n",
        "    \"P-value\": p, \n",
        "    \"degrees of freedom\": dof\n",
        "})"
      ],
      "id": "79ae47c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a plot to demonstrate the location of the chi-squared\n",
        "statistic with respect to the chi-squared distribution"
      ],
      "id": "027dc0b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.arange(0, 45, 0.001)\n",
        "# x2 = np.arange(59, 60, 0.001)\n",
        "\n",
        "plt.plot(x, stats.chi2.pdf(x, df=20), label=\"df: 20\", color=\"red\")\n",
        "# plt.fill_between(x, x**4, color=\"red\", alpha=0.5)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ],
      "id": "6b41995f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- "
      ],
      "id": "d099e086"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import chi2\n",
        "\n",
        "max_chi_val = 59.0\n",
        "x_range = np.arange(0, 60, 0.001)\n",
        "fig = px.histogram(x=x_range, \n",
        "                   y=chi2.pdf(x_range, df=dof), \n",
        "                   labels={\"x\":\"Chi-Squared Value\", \n",
        "                           \"y\":\"Density\"}, \n",
        "                   title=\"Chi-Squared Distribution (df = {})\".format(dof))\n",
        "# create a a scatter plot of values from chi2 to chi2 (a single point)\n",
        "# and going from 0 to the y value at the critical point - a vertical\n",
        "# line\n",
        "fig.add_trace(go.Scatter(x=[max_chi_val, max_chi_val],\n",
        "                         y=[0,chi2.pdf(max_chi_val, df=dof)], \n",
        "              mode=\"lines\", \n",
        "              name=\"Critical Value\", \n",
        "              line=dict(color=\"red\", dash=\"dash\")))\n",
        "fig.update_layout(shapes=[dict(type=\"rect\", \n",
        "                               x0=max_chi_val, \n",
        "                               x1=20, \n",
        "                               y0=0,\n",
        "                               y1=chi2.pdf(max_chi_val, df=dof), \n",
        "                          fillcolor=\"rgba(0, 100, 80, 0.2)\", \n",
        "                          line=dict(width=0))], \n",
        "                  annotations=[dict(x=max_chi_val + 0.5, \n",
        "                                    y=0.02, \n",
        "                                    text=\"Area of Interest\", \n",
        "                                    showarrow=False, \n",
        "                                    font=dict(size=10, color=\"black\"))])\n",
        "fig.show()"
      ],
      "id": "50d6633e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see from the figure, the critical value we obtain (2034)\n",
        "is exceptionally far beyond the bounds of the distribution, that \n",
        "there must be a significant dependence relationship between the \n",
        "borough and the rodent incident type which is reported. \n",
        "\n",
        "Moreover, the p-value returned for this test is 0.00000, meaning that\n",
        "there is virtually 0 probability that such observations would be \n",
        "made given that the borough and rodent incident type reported were \n",
        "independent.\n",
        "\n",
        "### Sources\n",
        "\n",
        "1. [towardsdatascience.com - Exploratory data analysis](https://towardsdatascience.com/a-data-scientists-essential-guide-to-exploratory-data-analysis-25637eee0cf6)\n",
        "2. [scribbr.com - Descriptive statistics](https://www.scribbr.com/statistics/descriptive-statistics/)\n",
        "3. [scribbr.com - Probability Distributions](https://www.scribbr.com/statistics/probability-distributions/)\n",
        "4. [mathisfun.com - Median definition](https://www.mathsisfun.com/definitions/median.html)\n",
        "5. [stats.libretexts - outliers and sample quantiles](https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/06%3A_Random_Samples/6.06%3A_Order_Statistics#:~:text=The%20sample%20quantile%20of%20order%20p%20%3D%201,the%20third%20quartile%20and%20is%20frequently%20denoted%20q3.)\n",
        "6. [datagy.io - calculating IQR in python](https://datagy.io/pandas-iqr/#:~:text=To%20calculate%20the%20interquartile%20range%20for%20a%20Pandas,a%20look%20at%20what%20this%20looks%20like%20below%3A)\n",
        "7. [curtin university - descriptive statistics for categorical data](https://uniskills.library.curtin.edu.au/numeracy/statistics/descriptive/#:~:text=Descriptive%20statistics%20used%20to%20analyse%20data%20for%20a,size%29%20obtained%20from%20the%20variable%E2%80%99s%20frequency%20distribution%20table.)\n",
        "8. [dwstockburger.com - hypothesis testing with contingency tables](https://www.dwstockburger.com/Introbook/sbk22.htm#:~:text=Hypothesis%20tests%20may%20be%20performed%20on%20contingency%20tables,differentially%20distributed%20over%20levels%20of%20the%20column%20variables%3F)\n",
        "9. [askpython.com - chi-squared testing in python](https://www.askpython.com/python/examples/chi-square-test)\n",
        "10. [sphweb - Hypotheses for chi-squared tests](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistesting-chisquare/bs704_hypothesistesting-chisquare_print.html)\n",
        "\n",
        "## Hypothesis Testing with scipy.stats\n",
        "\n",
        "This section was written by Isabelle Perez. \n",
        "\n",
        "### Introduction \n",
        "Hello! My name is Isabelle Perez and I am a junior Mathematics-Statistics major and \n",
        "Computer Science minor. I am interested in learning about data science topics and \n",
        "have an interest in how statistics can improve sports analytics, specifically in \n",
        "baseball. Today's topic is the `scipy.stats` package and the many hypothesis tests \n",
        "that we can perform using it. \n",
        "\n",
        "### `Scipy.stats` \n",
        "The package `scipy.stats` is a subpackage of `Scipy` and contains many methods useful\n",
        "for statistics such as probability distributions, summary statistics, statistical tests,\n",
        "etc. The focus of this presentation will be on some of the many hypothesis tests that can \n",
        "be easily conducted using `scipy.stats` and will provide examples of situations in \n",
        "which they may be useful.  \n",
        "\n",
        "Firstly, ensure `Scipy` \n",
        "is installed by using `pip install scipy` or  \n",
        "`conda install -c anaconda scipy`. \n",
        "\n",
        "To import the package, use the command `import scipy.stats`. \n",
        "\n",
        "### Basic Statistical Hypothesis Tests \n",
        "#### Two-sample t-test \n",
        "\n",
        "| H~0~: $\\mu_1 = \\mu_2$  \n",
        "| H~1~: $\\mu_1 \\neq$ or $>$ or $<$ $\\mu_2$   \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.ttest_ind(sample_1, sample_2)`  \n",
        "\n",
        "**Assumptions:** Observations are independent and identically distributed (i.i.d), \n",
        "normally distributed, and the two samples have equal variances. \n",
        "\n",
        "**Optional Parameters:**\n",
        "\n",
        "* `nan_policy` can be set to `propagate` (return `nan`), `raise` (raise `ValueError`),  \n",
        "or `omit` (ignore null values).\n",
        "* `alternative` can be `two-sided` (default), `less`, or `greater`. \n",
        "* `equal_var` is a boolean representing whether the variances of the two samples are \n",
        "equal  \n",
        "(default is True). \n",
        "* `axis` defines the axis along which the statistic should be computed (default is 0). \n",
        "\n",
        "**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. \n",
        "\n",
        "#### Paired t-test \n",
        "\n",
        "| H~0~: $\\mu_1 = \\mu_2$\n",
        "| H~1~: $\\mu_1 \\neq$ or $>$ or $<$ $\\mu_2$  \n",
        " \n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.ttest_rel(sample_1, sample_2)` \n",
        "\n",
        "**Assumptions:** Observations are i.i.d, normally distributed, and related, \n",
        "and the two samples have equal variances. The input arrays must also be \n",
        "of the same size since the observations are paired.  \n",
        "\n",
        "**Optional Parameters:** Can use `nan_policy` or `alternative`. \n",
        "\n",
        "**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. \n",
        "Also has a method called `confidence_interval` with input parameter `confidence_level`\n",
        "that returns a tuple with the confidence interval around the difference in \n",
        "population means of the two samples.  \n",
        "\n",
        "#### ANOVA \n",
        "\n",
        "| H~0~: $\\mu_1 = \\mu_2 = ... = \\mu_n$\n",
        "| H~1~: at least two $\\mu_i$ are not equal  \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.f_oneway(sample_1, sample_2, ..., sample_n)`\n",
        "\n",
        "**Assumptions:** Samples are i.i.d., normally distributed, and the samples have\n",
        "equal variances. \n",
        "\n",
        "**Errors:**  \n",
        "\n",
        "* Raises `ConstantInputWarning` if all values in each of the inputs are \n",
        "identical. \n",
        "* Raises `DegenerateDataWarning` if any input has length $0$ or all inputs\n",
        "have length $1$. \n",
        "\n",
        "**Returns:** The F-statistic and a corresponding p-value. \n",
        "\n",
        "#### Example: Comparison of Mean Response Times by Borough  \n",
        "Looking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests,  \n",
        "there are many ways a two-sample t-test may be useful. For example, we can consider \n",
        "samples drawn from different boroughs and perform this hypothesis test to \n",
        "identify whether their mean response times differ. If so, this may suggest \n",
        "that some boroughs are being underserviced.  "
      ],
      "id": "7f9c7f0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import scipy.stats   \n",
        "\n",
        "# read in file \n",
        "df = pd.read_csv('data/rodent_2022-2023.csv')  \n",
        "\n",
        "# data cleaning - change dates to timestamp object\n",
        "df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
        "df['Closed Date'] = pd.to_datetime(df['Closed Date'])\n",
        "\n",
        "# add column Response Time \n",
        "df['Response Time'] = df['Closed Date'] - df['Created Date']\n",
        "\n",
        "# convert data to total seconds\n",
        "df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600)    "
      ],
      "id": "9d20a9f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the two-sample t-test assumes the data is drawn from a normal distribution,  \n",
        "we need to ensure the samples we are comparing are normally distributed. According \n",
        "to the Central Limit theorem, the distribution of sample means from repeated \n",
        "samples of a population will be roughly normal. Therefore, we can take 100 samples \n",
        "of each borough's response times, measure the mean of each sample, and perform the \n",
        "hypothesis test on the arrays of sample means. "
      ],
      "id": "61aab187"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# select Bronx and Queens boroughs \n",
        "df_mhtn = df[df['Borough'] == 'MANHATTAN']['Response Time'] \n",
        "df_queens = df[df['Borough'] == 'QUEENS']['Response Time']  \n",
        "\n",
        "mhtn_means = []\n",
        "queens_means = []\n",
        "\n",
        "# create samples of sampling means \n",
        "for i in range(100): \n",
        "  sample1 = df_mhtn.sample(1000, replace = True)\n",
        "  mhtn_means.append(sample1.mean())\n",
        "\n",
        "  sample2 = df_queens.sample(1000, replace = True) \n",
        "  queens_means.append(sample2.mean())  \n",
        "\n",
        "# plot distribution of sample means for Manhattan\n",
        "plt.hist(mhtn_means)\n",
        "plt.xlabel('Mean Response Times for Manhattan')\n",
        "plt.ylabel('Value Counts')\n",
        "plt.show()\n",
        "\n",
        "# plot distribution of sample means for Queens \n",
        "plt.hist(queens_means) \n",
        "plt.xlabel('Mean Response Times for Queens')\n",
        "plt.ylabel('Value Counts')\n",
        "plt.show() "
      ],
      "id": "f8e3659c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to check if the variances of the two samples are equal. "
      ],
      "id": "a635fd29"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert to numpy array \n",
        "mhtn_means = np.array(mhtn_means)\n",
        "queens_means = np.array(queens_means)\n",
        "\n",
        "print('Mean, variance for Manhattan', (mhtn_means.mean(), mhtn_means.std() ** 2))\n",
        "print('Mean, variance for Queens:', (queens_means.mean(), queens_means.std() ** 2))"
      ],
      "id": "a57d06a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the ratio of the variances is less than $2$, it is safe to assume equal variances.  "
      ],
      "id": "63e72e64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_ttest = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True)\n",
        "\n",
        "print('t-statistic:', result_ttest.statistic)\n",
        "print('p-value:', result_ttest.pvalue) \n",
        "# print('degrees of freedom:', result_ttest.df) "
      ],
      "id": "f352e9fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At an alpha level of $0.05$, the p-value allows us to reject the null hypothesis\n",
        "and conclude that there is a statistically significant difference in the mean of \n",
        "sample means drawn from rodent sighting response times for Manhattan compared to Queens. "
      ],
      "id": "00c88024"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_ttest2 = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True, \n",
        "                                                            alternative = 'less') \n",
        "\n",
        "print('t-statistic:', result_ttest2.statistic)\n",
        "print('p-value:', result_ttest2.pvalue) \n",
        "# print('degrees of freedom:', result_ttest2.df) "
      ],
      "id": "342ed85b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also set the alternative equal to `less` to test if the mean of sample means\n",
        "drawn from the Manhattan response times is less than that of sample means drawn from\n",
        "Queens response times. At the alpha level of $0.05$, we can also reject this null\n",
        "hypothesis and conclude that the mean of sample means is less for Manhattan than \n",
        "it is for Queens. \n",
        "\n",
        "### Normality \n",
        "#### Shapiro-Wilk Test \n",
        "\n",
        "| H~0~: data is drawn from a normal distribution  \n",
        "| H~1~: data is not drawn from a normal distribution     \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.shapiro(sample)` \n",
        "\n",
        "**Assumptions:** Observations are i.i.d. \n",
        "\n",
        "**Returns:** The test statistic and corresponding p-value. \n",
        "\n",
        "* More appropriate for smaller sample sizes ($<50$). \n",
        "* The closer the test statistic is to $1$, the closer it is to a normal \n",
        "distribution, with $1$ being a perfect match.   \n",
        "\n",
        "#### NormalTest \n",
        "\n",
        "| H~0~: data is drawn from a normal distribution  \n",
        "| H~1~: data is not drawn from a normal distribution   \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.normaltest(sample)`  \n",
        "\n",
        "**Assumptions:** Observations are i.i.d. \n",
        "\n",
        "**Optional Parameters:** Can use `nan_policy`. \n",
        "\n",
        "**Returns:** The test-statistic $s^2 + k^2$, where $s^2$ is from the `skewtest`\n",
        "and $k$ is from the `kurtosistest`, and a corresponding p-value\n",
        "\n",
        "This test is based on D'Agostino and Pearson's test which combines skew\n",
        "and kurtosis (heaviness of the tail or how much data resides in the tails). \n",
        "The test compares the skewness and kurtosis of the sample to that of a normal \n",
        "distribution, which are $0$ and $3$, respectively.\n",
        "\n",
        "#### Example: Distribution of Response Times \n",
        "It can be useful to identify the distribution of a population because it gives\n",
        "us the ability to summarize the data more efficiently. We can identify \n",
        "whether or not the distribution of a sample of response times  \n",
        "from the rodent sighting dataset is normal by conducting a normality test using `scipy.stats`. "
      ],
      "id": "2251ec6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# take a sample from Response Time column \n",
        "resp_time_samp = df['Response Time'].sample(10000, random_state = 0)  \n",
        "\n",
        "results_norm = scipy.stats.normaltest(resp_time_samp, nan_policy = 'propagate')\n",
        "\n",
        "print('test statistic:', results_norm.statistic) \n",
        "print('p-value:', results_norm.pvalue)"
      ],
      "id": "6e502d04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because there are null values in the sample data, if we set the `nan_policy` \n",
        "to `propagate`, both the test statistic and p-value will return as `nan`. \n",
        "If we still want to obtain results when there is missing data, we must set the\n",
        "`nan_policy` to `omit`. "
      ],
      "id": "45c8ad3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results_norm2 = scipy.stats.normaltest(resp_time_samp, nan_policy = 'omit') \n",
        "\n",
        "print('test statistic:', results_norm2.statistic) \n",
        "print('p-value:', results_norm2.pvalue)"
      ],
      "id": "698b3c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At an alpha level of $0.05$, the p-value allows us to reject the null hypothesis\n",
        "and conclude that the data is not drawn from a normal distribution. We can further\n",
        "show this by plotting the data in a histogram. "
      ],
      "id": "33c96e4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bins = [i for i in range(int(resp_time_samp.min()), int(resp_time_samp.max()), 300)]\n",
        "\n",
        "plt.hist(resp_time_samp, bins = bins)\n",
        "plt.xlabel('Response Times')\n",
        "plt.ylabel('Value Counts')\n",
        "plt.show()"
      ],
      "id": "304fcd85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation   \n",
        "#### Pearson's Correlation     \n",
        "\n",
        "| H~0~: the correlations is $0$\n",
        "| H~1~: the correlations is $\\neq$, $<$, or $> 0$ \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.pearsonr(sample_1, sample_2)`\n",
        "\n",
        "**Assumptions:** Observations are i.i.d, normally distributed, and the two samples\n",
        "have equal variances. \n",
        "\n",
        "**Optional Parameters:** Can use `alternative`. \n",
        "\n",
        "**Errors:** \n",
        "\n",
        "* Raises `ConstantInputWarning` if either input has all constant values. \n",
        "* Raises `NearConstantInputWarning` if \n",
        "`np.linalg.norm(x - mean(x)) < 1e-13 * np.abs(mean(x))`. \n",
        "\n",
        "**Returns:** The correlation coefficient and a corresponding p-value. It also \n",
        "has the `confidence_interval` method. \n",
        "\n",
        "#### Chi-Squared Test \n",
        "\n",
        "| H~0~: the two variables are independent of one another\n",
        "| H~1~: a dependency exists between the two variables \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.chi2_contingency(table)` \n",
        "\n",
        "**Assumptions:** The cells in the table contain frequencies, the levels of each\n",
        "variable are mutually exclusive, and observations are independent. [2]\n",
        "\n",
        "**Returns:** The test statistic, a corresponding p-value, the degrees of freedom, \n",
        "and an array of expected frequencies from the table. \n",
        "\n",
        "* `dof = table.size - sum(table.shape) + table.ndim - 1` \n",
        "\n",
        "#### Example: Analyzing the Relationship Between Season and Response Time \n",
        "\n",
        "One way in which the chi-squared test may prove useful with the 2022-2023 311 Service\n",
        "Request rodent sighting data is by allowing us to identify dependencies between \n",
        "different variables, categorical ones in particular. For example, we can choose a borough\n",
        "and test whether the season in which the request was created is independent of the \n",
        "type of sighting, using the `Descriptor` column. "
      ],
      "id": "4cc5c14e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# return season based off month of created date\n",
        "def get_season(month): \n",
        "  if month in [3, 4, 5]:\n",
        "    return 'Spring'\n",
        "  elif month in [6, 7, 8]: \n",
        "    return 'Summer'\n",
        "  elif month in [9, 10, 11]: \n",
        "    return 'Fall'\n",
        "  elif month in [12, 1, 2]:\n",
        "    return 'Winter' \n",
        "\n",
        "# add column for season \n",
        "df['Season'] = df['Created Date'].dt.month.apply(get_season)\n",
        "\n",
        "# create df for Brooklyn \n",
        "df_brklyn = df[df['Borough'] == 'BROOKLYN'] \n",
        "\n",
        "freq_table_2 = pd.crosstab(df_brklyn.Season, df_brklyn.Descriptor) \n",
        "\n",
        "freq_table_2"
      ],
      "id": "f983bfbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results_chi2 = scipy.stats.chi2_contingency(freq_table_2)\n",
        "\n",
        "print('test statistic:', results_chi2.statistic)\n",
        "print('p-value:', results_chi2.pvalue)\n",
        "print('degrees of freedom:', results_chi2.dof) "
      ],
      "id": "26c6def8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At an alpha level of $0.05$, the p-value allows us to reject the null hypothesis\n",
        "and conclude that the `Season` and `Descriptor` columns are indeed dependent for \n",
        "Brooklyn. This can also be confirmed by plotting the descriptor frequencies in a \n",
        "stacked bar chart, where the four seasons represent different colored bars."
      ],
      "id": "7a56e99e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_labels = ['Condition', 'M.Sighting', 'R.Sighting', 'Bite', 'Signs of Rodents']\n",
        "\n",
        "freq_table_2.rename(columns = {'Condition Attracting Rodents': 'Condition', \n",
        "                      'Rat Sighting': 'R.Sighting', 'Mouse Sighting': 'M.Sighting', \n",
        "                      'Rodent Bite - PCS Only': 'Bite'},\n",
        "                      inplace = True)\n",
        "\n",
        "freq_table_2.T.plot(kind = 'bar', stacked = True) "
      ],
      "id": "e3a68d44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bar chart above shows that the ranking of each season by number of rodent \n",
        "sightings is consistent across all five types of rodent sightings. This further suggests\n",
        "that there exists dependency between season and rodent sighting in Brooklyn. \n",
        "\n",
        "### Nonparametric Hypothesis Tests   \n",
        "#### Mann-Whitney U (Wilcoxon Rank-Sum) Test  \n",
        "\n",
        "| H~0~: distribution of population 1 $=$ distribution of population 2\n",
        "| H~1~: distribution of population 1 $\\neq$ or $>$ or $<$ distribution of population 2   \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.mannwhitneyu(sample_1, sample_2)` \n",
        "\n",
        "**Assumptions:** Observations are independent and ordinal.  \n",
        "\n",
        "**Optional Parameters:** \n",
        "\n",
        ":::{}\n",
        "* `alternative` can allow us to test if one sample has a distribution that is \n",
        "stochastically less than or greater than that of the second sample.   \n",
        "* Can use `nan_policy`. \n",
        "* `method` selects how the p-value is calculated and can be set to \n",
        "`asymptotic`, `exact`, or `auto`. \n",
        "  + `asymptotic` corrects for ties and compares the standardized test statistic\n",
        "  to the normal distribution. \n",
        "  + `exact` does not correct for ties and computes the exact p-value.\n",
        "  + `auto` (default) chooses `exact` when there are no ties and the \n",
        "  size of one sample is $<=8$, `asymptotic` otherwise. \n",
        "::: \n",
        "\n",
        "**Returns:** The Mann-Whitney U Statistic corresponding with the first sample \n",
        "and a corresponding p-value. \n",
        "\n",
        ":::{}\n",
        "* The statistic corresponding to the second sample is not returned but can \n",
        "be calculated as `sample_1.shape * sample_2.shape - U1` where `U1` is the \n",
        "test statistic associated with `sample_1`. \n",
        "* For large sample sizes, the distribution can be assumed to be approximately\n",
        "normal, so the statisic can be measured as $z = \\frac{U-\\mu_{U}}{\\sigma_{U}}$. \n",
        "* To adjust for ties, the standard deviation is calculated as follows: \n",
        "\n",
        "$\\sigma_{U} = \\sqrt{\\frac{n_{1}n_{2}}{12}((n + 1) - \n",
        "\\frac{\\sum_{k = 1}^{K}(t^{3}_{k} - t_{k})}{n(n - 1)})}$, where $t_{k}$ is the number of ties. \n",
        "\n",
        "* Non-parametric version of the two-sample t-test. \n",
        "* If the underlying distributions have similar shapes, the test is \n",
        "essentially a comparison of medians. [5]\n",
        "::: \n",
        "\n",
        "#### Wilcoxon Signed-Rank test\n",
        "\n",
        "| H~0~: distribution of population 1 $=$ distribution of population 2\n",
        "| H~1~: distribution of population 1 $\\neq$ or $>$ or $<$ distribution of population 2  \n",
        "\n",
        "&NewLine;  \n",
        "\n",
        "**Code:** `scipy.stats.wilcoxon(sample_1, sample_2)` or  \n",
        "`scipy.statss.wilcoxon(sample_diff, None)` \n",
        "\n",
        "**Assumptions:** Observations are independent, ordinal, and the samples are paired.\n",
        "\n",
        "**Optional Parameters:** \n",
        "\n",
        "* `zero-method` chooses how to handle pairs with the same value. \n",
        "  + `wilcox` (default) doesn't include these pairs. \n",
        "  + `pratt` drops ranks of pairs whose difference is $0$. \n",
        "  + `zsplit` includes pairs and assigns half the ranks into the positive\n",
        "  group and the other half in the negative group. \n",
        "* Can use `alternative` and `nan_policy`. \n",
        "* `alternative` allows us to identify whether the distribution of the difference\n",
        "is stochastically greater than or less than a distribution symmetric about $0$. \n",
        "* `method` selects how the p-value is calculated. \n",
        "  + `exact` computes the exact p-value. \n",
        "  + `approx` finds the p-value by standardizing the test statistic. \n",
        "  + `auto` (default) chooses `exact` when the sizes of the samples are $<=50$\n",
        "  and `approx` otherwise. \n",
        "\n",
        "**Returns:** The test statistic, a corresponding p-value, and the calculated\n",
        "z-score when the `method` is `approx`. \n",
        "\n",
        "* Non-parametric version of the paired t-test.  \n",
        "\n",
        "#### Kruskal-Wallis H-Test \n",
        "\n",
        "| H~0~: all populations have the same distribution \n",
        "| H~1~: $>=2$ populations are distributed differently  \n",
        "\n",
        "&NewLine; \n",
        "\n",
        "**Code:** `scipy.stats.kruskal(sample_1, ..., sample_n)`\n",
        "\n",
        "**Assumptions:** Observations are independent, ordinal, and each sample has \n",
        "$>=5$ observations. [3]\n",
        "\n",
        "**Optional Parameters:** Can use `nan_policy`.\n",
        "\n",
        "**Returns:** The Kruskal-Wallis H-statisic (corrected for ties) and a \n",
        "corresponding p-value. \n",
        "\n",
        "* Non-parametric version of ANOVA. \n",
        "\n",
        "#### Example: Distribution of Response Times for 2022 vs. 2023 \n",
        "\n",
        "We can use the Mann-Whitney test to compare the distributions of response times\n",
        "from our rodent data. For example, we can split the data into two groups, one for 2022\n",
        "and the other for 2023, to compare their distributions. "
      ],
      "id": "89e90125"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create dfs for 2022 and 2023\n",
        "df_2022 = df[df['Created Date'].dt.year == 2022]['Response Time'] \n",
        "df_2023 = df[df['Created Date'].dt.year == 2023]['Response Time']\n",
        "\n",
        "# perform test with H_0 df_2022 > df_2023\n",
        "results_mw = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n",
        "                                                      alternative = 'greater')\n",
        "\n",
        "# perform test with H_0 df_2022 < df_2023\n",
        "results_mw2 = scipy.stats.mannwhitneyu(df_2022, df_2023, nan_policy = 'omit', \n",
        "                                                        alternative = 'less')\n",
        "\n",
        "print('test statistic:', results_mw.statistic)\n",
        "print('p-value:', results_mw.pvalue)\n",
        "print()\n",
        "print('test statistic:', results_mw2.statistic)\n",
        "print('p-value:', results_mw2.pvalue)"
      ],
      "id": "4796d781",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At an alpha level of $0.05$, the p-value of $1$ is too large to reject the null hypothesis,\n",
        "therefore we cannot conclude that the distribution of response times for 2022 is \n",
        "stochastically greater than that for 2023. But when we set the alternative to `less`, our\n",
        "p-value is small enough to conclude that the distribution of response times for 2022\n",
        "is stochastically greater than the distribution of response times for 2023. "
      ],
      "id": "a0718771"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bins = [i for i in range(5, 500, 50)]\n",
        "plt.hist(df_2022, label = 2022, bins = bins, color = 'red') \n",
        "plt.hist(df_2023, label = 2023, bins = bins, color = 'blue', \n",
        "                                                  alpha = 0.5)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "id": "05d0b026",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This small subset of data confirms the results of the one-sided hypothesis test, showing \n",
        "that in general, the counts of response times for 2022 are greater than those for 2023, \n",
        "suggesting the distribution for 2022 is stochastically larger than that of 2023 data. \n",
        "\n",
        "#### Example: Distribution of Response Times by Season \n",
        "\n",
        "Similar to the previous, example, we can use a non-parametric test to compare the distribution \n",
        "of response times by season. Because in this case we have four samples to compare, we need\n",
        "to use the Kruskal Wallis H-Test. "
      ],
      "id": "0781c46a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_summer = df[df['Season'] == 'Summer']['Response Time']\n",
        "df_spring = df[df['Season'] == 'Spring']['Response Time']\n",
        "df_fall = df[df['Season'] == 'Fall']['Response Time']\n",
        "df_winter = df[df['Season'] == 'Winter']['Response Time']\n",
        "\n",
        "results_kw = scipy.stats.kruskal(df_summer, df_spring, df_fall, df_winter,\n",
        "                                                                nan_policy = 'omit')\n",
        "\n",
        "print('test statistic:', results_kw.statistic) \n",
        "print('p-value:', results_kw.pvalue)"
      ],
      "id": "14316afb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At an alpha level of $0.05$, the p-value of $0.0496$ is just small enough to reject the \n",
        "null hypothesis, suggesting that the distribution of response times differs by season, \n",
        "but not by much.   \n",
        "\n",
        "### References \n",
        "1. <https://docs.scipy.org/doc/scipy/reference/stats.html> (`scipy.stats` documentation)\n",
        "2. <https://libguides.library.kent.edu/SPSS/ChiSquare> \n",
        "3. <https://library.virginia.edu/data/articles/getting-started-with-the-kruskal-wallis-test>\n",
        "4. <https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/>\n",
        "5. <https://library.virginia.edu/data/articles/the-wilcoxon-rank-sum-test>  \n",
        "\n",
        "## Exploring NYC Rodent Dataset\n",
        "\n",
        "> This section was written by Xingye Zhang\n",
        "\n",
        "The main goal of my presentation is to show the process of \n",
        "‘transforming raw dataset’ into ‘compelling insights’ using various data \n",
        "visualizing examples. And most importantly, I wish to get you guys ‘engaged’\n",
        "and ‘come up with your insights’ about visualizing NYC dataset throughout\n",
        "the process of exploring.\n",
        "\n",
        "### Personal Introduction\n",
        "My name is Xingye Zhang, you can call me Austin, which may be easier to pronounce.\n",
        "I'm from China and currently a senior majoring in Statistics and Economics. I plan\n",
        "to graduate next semester, having taken a gap semester previously.\n",
        "\n",
        "My experience with Python is quite recent. I had my first Python course in ECON \n",
        "prior to this course and I just started to learn how to use Github and Vs code \n",
        "in this semester.\n",
        "\n",
        "Please feel free to interrupt if you have any questions or notice I made a mistake.\n",
        "I'm glad to answer your questions and learn from you guys!\n",
        "\n",
        "### Dataset Format Selection\n",
        "Why 'Feather'?\n",
        "\n",
        "* Speed: Feather files are faster to read and write than CSV files.\n",
        "\n",
        "* Efficiency in Storage: Feather files are often smaller in size than CSV files. \n",
        "\n",
        "* Support for Large Datasets: Feather files can handle large datasets more \n",
        "  efficiently.\n",
        "\n",
        "### Dataset Cleaning"
      ],
      "id": "bf6ec5da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import basic packages\n",
        "import pandas as pd\n",
        "# Pyarrow is better for reading feather file\n",
        "import pyarrow.feather as pya\n",
        "\n",
        "# Load the original dataset\n",
        "rodent_data = pya.read_feather('data/rodent_2022-2023.feather')\n",
        "\n",
        "# Print columns in order to avoid 'Keyerror'\n",
        "column_names = rodent_data.columns.tolist()\n",
        "print(column_names)"
      ],
      "id": "0b6481b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Checking Columns**\n",
        "\n",
        "**Conclusion:**: \n",
        "There are no columns with identical data, but some columns are highly correlated.\n",
        "\n",
        "**Empty Columns:**  'Facility Type', 'Due Date', 'Vehicle Type',\n",
        "'Taxi Company Borough','Taxi Pick Up Location', 'Bridge Highway Name', \n",
        "'Bridge Highway Direction', 'Road Ramp', 'Bridge Highway Segment'.\n",
        "\n",
        "**Columns we can remove to clean data:**  'Agency Name', 'Street Name', \n",
        "'Landmark', 'Intersection Street 1', 'Intersection Street 2', 'Park Facility Name',\n",
        "'Park Borough', 'Police Precinct', 'Facility Type', 'Due Date', 'Vehicle Type',\n",
        "'Taxi Company Borough', 'Taxi Pick Up Location', 'Bridge Highway Name',\n",
        "'Bridge Highway Direction', 'Road Ramp', 'Bridge Highway Segment'.\n",
        "\n",
        "\n",
        "**2. Using reverse geocoding to fill the missing zip code**"
      ],
      "id": "8d8e9106"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find the missing zip code\n",
        "missing_zip = rodent_data['zip_codes'].isnull()\n",
        "missing_borough = rodent_data['borough'].isnull()\n",
        "missing_zip_borough_correlation = (missing_zip == missing_borough).all()\n",
        "\n",
        "# Use reverse geocoding to fill the missing zip code\n",
        "geocode_available = not (rodent_data['latitude'].isnull().any() \n",
        "                    or rodent_data['longitude'].isnull().any())\n",
        "\n",
        "missing_zip_borough_correlation, geocode_available"
      ],
      "id": "c50bc49c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Clean the Original Data**"
      ],
      "id": "b0f2b879"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Removing redundant columns\n",
        "columns_to_remove = ['agency_name', 'street_name', 'landmark',\n",
        "                     'intersection_street_1', 'intersection_street_2',\n",
        "                     'park_facility_name', 'park_borough',\n",
        "                     'police_precinct', 'facility_type', 'due_date',\n",
        "                     'vehicle_type', 'taxi_company_borough', \n",
        "                     'taxi_pick_up_location', 'police_precinct',\n",
        "                     'bridge_highway_name', 'bridge_highway_direction', \n",
        "                     'road_ramp','bridge_highway_segment']\n",
        "\n",
        "cleaned_data = rodent_data.drop(columns=columns_to_remove)\n",
        "\n",
        "#Create the file_path\n",
        "file_path = 'data/cleaned_rodent_data.feather'\n",
        "\n",
        "# Feather Export (removing non-supported types like datetime)\n",
        "cleaned_data['created_date'] = cleaned_data['created_date'].astype(str)\n",
        "cleaned_data['closed_date'] = cleaned_data['closed_date'].astype(str)\n",
        "cleaned_data.to_feather(file_path)\n",
        "\n",
        "# Check the cleaned columns\n",
        "print(cleaned_data.columns)"
      ],
      "id": "6d31d7f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorizing the Columns\n",
        "Highly suggest to use 'Chatgpt' first and then revise it yourself.\n",
        "\n",
        "* Identification Information: 'Unique Key'.\n",
        "\n",
        "* Temporal Information: 'Created Date', 'Closed Date'.\n",
        "\n",
        "* Agency Information: 'Agency'.\n",
        "\n",
        "* Complaint Details: 'Complaint Type', 'Descriptor', \n",
        "  'Resolution Description', 'Resolution Action Updated Date'.\n",
        "\n",
        "* Location and Administrative Information: 'Location Type', \n",
        "  'Incident Zip', 'Incident Address', 'Cross Street 1', 'Cross Street 2',\n",
        "  'City','Borough', 'Community Board', 'Community Districts',\n",
        "  'Borough Boundaries', 'BBL'. 'City Council Districts',\n",
        "  'Police Precincts'.\n",
        "\n",
        "* Geographical Coordinates: 'X Coordinate (State Plane)',\n",
        "  'Y Coordinate (State Plane)', 'Location'.\n",
        "\n",
        "* Communication Channels: 'Open Data Channel Type'.\n",
        "\n",
        "### Question based on Dataset\n",
        "**Agency:**\n",
        "1. Temporal Trends in Rodent Complaints\n",
        "2. Relationship between Rodent Complaints\n",
        "  Location Types\n",
        "3. Spatial Analysis of Rodent Complaints\n",
        "\n",
        "**Complainer:**\n",
        "1. Agency Resolution Time\n",
        "2. Impact of Rodent Complaints on City Services:\n",
        "\n",
        "### Temporal Trends in Rodent Complaints"
      ],
      "id": "fd16b4c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import basic packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'created_date' is in datetime format and extract 'Year' and 'Month'\n",
        "cleaned_data['created_date'] = pd.to_datetime(cleaned_data['created_date'], \n",
        "errors='coerce')\n",
        "cleaned_data['Year'] = cleaned_data['created_date'].dt.year\n",
        "cleaned_data['Month'] = cleaned_data['created_date'].dt.month\n",
        "\n",
        "# Use data from year 2023 as example\n",
        "data_2023 = cleaned_data[cleaned_data['Year'] == 2023]\n",
        "\n",
        "# Group by Month to get the count of complaints\n",
        "mon_complaints_23= data_2023.groupby('Month').size().reset_index(name='Counts')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.barplot(data=mon_complaints_23, x='Month', y='Counts')\n",
        "plt.title('Monthly Rodent Complaints in 2023')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n",
        "                       'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.tight_layout()"
      ],
      "id": "7cb85823",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Seasonal Trend**"
      ],
      "id": "a3e888cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Categorize month into seasons\n",
        "def categorize_season(month):\n",
        "    if month in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    elif month in [9, 10, 11]:\n",
        "        return 'Fall'\n",
        "    else:  # Months 12, 1, 2\n",
        "        return 'Winter'\n",
        "\n",
        "# Applying the function to create a 'Season' column\n",
        "data_2023 = cleaned_data[cleaned_data['Year'] == 2023].copy()\n",
        "data_2023['Season'] = data_2023['Month'].apply(categorize_season)\n",
        "\n",
        "# Grouping by Season to get the count of complaints\n",
        "season_com_2023 = data_2023.groupby('Season').size().reset_index(name='Counts')\n",
        "\n",
        "# Ordering the seasons for the plot\n",
        "season_order = ['Spring', 'Summer', 'Fall', 'Winter']\n",
        "season_com_2023['Season'] = pd.Categorical(season_com_2023['Season'],\n",
        "                            categories=season_order, ordered=True)\n",
        "season_com_2023 = season_com_2023.sort_values('Season')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.barplot(data=season_com_2023, x='Season', y='Counts')\n",
        "plt.title('Seasonal Rodent Complaints in 2023')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "aa1b1fcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comparing 2022 and 2023 Seasonal Trend**"
      ],
      "id": "0696a88c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter data for two specific years, e.g., 2022 and 2023\n",
        "data_filtered = cleaned_data[cleaned_data['Year'].isin([2022, 2023])]\n",
        "\n",
        "# Group by Year and Month to get the count of complaints\n",
        "mon_counts = data_filtered.groupby(['Year', \n",
        "'Month']).size().reset_index(name='Counts')\n",
        "\n",
        "# Pivot the data for easy plotting\n",
        "mon_counts_pivot = mon_counts.pivot(index='Month', columns='Year', \n",
        "                   values='Counts')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.lineplot(data=mon_counts_pivot)\n",
        "plt.title('Comparison of Monthly Rodent Complaints between 2022 and 2023')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \n",
        "                          'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.legend(title='Year', labels=mon_counts_pivot.columns)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e713c79b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comparing Temporal Trends from Boroughs in 2023**"
      ],
      "id": "05242d2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_2023 = cleaned_data[cleaned_data['Year'] == 2023]\n",
        "\n",
        "# Group by Month and Borough to get the count of complaints\n",
        "mon_borough_counts = data_2023.groupby(['Month',\n",
        "'borough']).size().reset_index(name='Counts')\n",
        "\n",
        "# Pivot the data for easy plotting\n",
        "mon_borough_counts_pivot = mon_borough_counts.pivot(index='Month', \n",
        "                           columns='borough', values='Counts')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.lineplot(data=mon_borough_counts_pivot)\n",
        "plt.title('Monthly Trend of Rodent Complaints by Borough in 2023')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \n",
        "                          'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "plt.legend(title='Borough')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "6a1d66be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Adding the Location Types**"
      ],
      "id": "c3ea6aa8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "from plotnine.exceptions import PlotnineWarning\n",
        "from plotnine import (\n",
        "    ggplot, aes, geom_line, geom_point, facet_wrap,\n",
        "    labs, theme, element_text, scale_x_continuous\n",
        ")\n",
        "\n",
        "# Suppress specific Plotnine warnings\n",
        "warnings.filterwarnings('ignore', category=PlotnineWarning)\n",
        "\n",
        "# get the count of complaints per month per location type and borough\n",
        "monthly_data = (data_2023.groupby(['borough', 'location_type', 'Month'])\n",
        "                               .size()\n",
        "                               .reset_index(name='Counts'))\n",
        "\n",
        "# Create the plot with adjusted figure size and legend properties\n",
        "plot = (ggplot(monthly_data, aes(x='Month', y='Counts', color='location_type')) +\n",
        "        geom_line() +\n",
        "        geom_point() +\n",
        "        facet_wrap('~borough', scales='free_y', ncol=3) +\n",
        "        labs(x='Month', y='Number of Complaints', color='Location Type', \n",
        "             title='Monthly Rodent Complaints by Location Type and Borough') +\n",
        "        scale_x_continuous(breaks=range(2, 13, 2)) +\n",
        "        theme(\n",
        "            figure_size=(20, 10),  # Adjusted figure size\n",
        "            text=element_text(size=10),\n",
        "            legend_position='right',\n",
        "            axis_text_x=element_text(rotation=0, hjust=0.5)\n",
        "            # Removed subplots_adjust\n",
        "        )\n",
        ")\n",
        "\n",
        "# Save the plot to a file with high resolution\n",
        "plot.save('rodent_complaints_plot.jpeg', width=20, height=10, dpi=300)\n",
        "\n",
        "# Corrected way to show the plot\n",
        "plot.show()"
      ],
      "id": "35605e05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Graph\n",
        "**Plotly Example of Monthly Rodents Complaints in Bronx**"
      ],
      "id": "5452c441"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "# Replace with the path to your dataset\n",
        "data = pya.read_feather('data/cleaned_rodent_data.feather')\n",
        "\n",
        "# Convert 'Created Date' to datetime and extract 'Year' and 'Month'\n",
        "data['created_date'] = pd.to_datetime(data['created_date'], errors='coerce')\n",
        "data['Year'] = data['created_date'].dt.year.astype(int)\n",
        "data['Month'] = data['created_date'].dt.month\n",
        "\n",
        "# Filter the dataset for the years 2022 and 2023\n",
        "data_filtered = data[(data['Year'] == 2022) | (data['Year'] == 2023)]\n",
        "\n",
        "# Further filter to only include the Bronx borough\n",
        "data_bronx = data_filtered[data_filtered['borough'] == 'BRONX'].copy()\n",
        "\n",
        "# Combine 'Year' and 'Month' to a 'Year-Month' format for more granular plotting\n",
        "data_bronx['Year-Month'] = (data_bronx['Year'].astype(str) \n",
        "                          + '-' + data_bronx['Month'].astype(str).str.pad(2, \n",
        "                          fillchar='0'))\n",
        "\n",
        "# Group data by 'Year-Month' and 'Location Type' and count the complaints.\n",
        "monthly_data_bronx = (data_bronx.groupby(['Year-Month', 'location_type'], \n",
        "                    as_index=False)\n",
        "                    .size()\n",
        "                    .rename(columns={'size': 'Counts'}))\n",
        "\n",
        "# Create an interactive plot with Plotly Express\n",
        "fig = px.scatter(monthly_data_bronx, x='Year-Month', y='Counts', \n",
        "                color='location_type',\n",
        "                size='Counts', hover_data=['location_type'],\n",
        "                title='Monthly Rodent Complaints by Location Type in Bronx')\n",
        "\n",
        "# Adjust layout for better readability\n",
        "fig.update_layout(\n",
        "    height=400, width=750,\n",
        "    legend_title='Location Type',\n",
        "    xaxis_title='Year-Month',\n",
        "    yaxis_title='Number of Complaints',\n",
        "    # Rotate the labels on the x-axis for better readability\n",
        "    xaxis=dict(tickangle=45)  \n",
        ")\n",
        "\n",
        "# Show the interactive plot\n",
        "fig.show()"
      ],
      "id": "3cde9e0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Map using Google"
      ],
      "id": "4cf73ab7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shapely for converting latitude/longtitude to geometry\n",
        "from shapely.geometry import Point \n",
        "# To create GeodataFrame\n",
        "import geopandas as gpd\n",
        "\n",
        "# cutting the length of dataset to avoid over-capacity\n",
        "sub_data = data.iloc[:len(data)//20] # Shorten dataset for illustration.\n",
        "\n",
        "# Drop rows with missing latitude or longitude to match the lengths\n",
        "sub_data_cleaned = sub_data.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "# creating geometry using shapely (removing empty points)\n",
        "geometry = [Point(xy) for xy in zip(sub_data_cleaned[\"longitude\"], \\\n",
        "            sub_data_cleaned[\"latitude\"]) if not Point(xy).is_empty]\n",
        "\n",
        "# creating geometry column to be used by geopandas\n",
        "geometry2 = gpd.points_from_xy(sub_data_cleaned[\"longitude\"],\n",
        "            sub_data_cleaned[\"latitude\"])\n",
        "\n",
        "# coordinate reference system.\n",
        "crs = \"EPSG:4326\"\n",
        "\n",
        "# Create GeoDataFrame directly using geopandas points_from_xy utility\n",
        "rodent_geo = gpd.GeoDataFrame(sub_data_cleaned,\n",
        "                              crs=crs, \n",
        "                              geometry=gpd.points_from_xy(\n",
        "                                       sub_data_cleaned['longitude'],\n",
        "                                       sub_data_cleaned['latitude']))\n",
        "\n",
        "rodent_geo.plot(column='borough', legend=True)"
      ],
      "id": "586a0c1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converts timestamps into strings for JSON serialization\n",
        "rodent_geo['created_date'] = rodent_geo['created_date'].astype(str)\n",
        "rodent_geo['closed_date'] = rodent_geo['closed_date'].astype(str)\n",
        "\n",
        "map = rodent_geo.explore(column='borough', legend=True)\n",
        "map"
      ],
      "id": "831cae50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tips in using this map**\n",
        "- Due to the length of information shown in 'resolution_description', and\n",
        "  the amount of total columns, the information are hard to be shown fully\n",
        "  and clearly.\n",
        "\n",
        "- Please drag the google map to keep the coordinates at the left side of \n",
        "  the google map, so that the information could be shown on the right side. \n",
        "\n",
        "- In this case, the information shown could be more readable and organized.\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "For more information see the following: \n",
        "\n",
        "* Plotly Basic Charts\n",
        "    + <https://plotly.com/python/basic-charts/>\n",
        "\n",
        "* Plotnine Tutorial\n",
        "    + <https://plotnine.org/>\n",
        "\n",
        "* GeoPandas Documentation\n",
        "    + <https://geopandas.org/en/stable/docs.html>\n",
        "\n",
        "* NYC Borough Data\n",
        "    + <https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm>\n",
        "\n",
        "* NYC Zip Code Data\n",
        "    + <https://data.beta.nyc/en/dataset/nyc-zip-code-tabulation-areas/resource/894e9162-871c-4552-a09c-c6915d8783fb>\n"
      ],
      "id": "2bda9373"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}