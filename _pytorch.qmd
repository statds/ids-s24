## Feedforward Neural Networks With PyTorch

> By Charitarth Chugh

### Introduction to Neural Networks

Neural Networks are a type of machine learning algorithm that consist of neurons that are inspired from the neural
connections in the brain. These neurons are connected by edges, modeling the synapse and forming a directed, weighted
graph.

```{dot}
digraph NeuralNetwork {

    rankdir=LR
    splines=line

        node [fixedsize=true, label=""];

        subgraph cluster_0 {
        color=white;
        node [style=solid,color=blue4, shape=circle];
        x1 x2 x3 x4;
        label = "layer 1 (Input layer)";
    }

    subgraph cluster_1 {
        color=white;
        node [style=solid,color=red2, shape=circle];
        a12 a22 a32;
        label = "layer 2 (hidden layer)";
    }

    subgraph cluster_2 {
        color=white;
        node [style=solid,color=seagreen2, shape=circle];
        l21,l22;
        label="layer 3 (hidden layer)";
    }
    subgraph cluster_3 {
        color=white;
        node [style=solid,color=yellow, shape=circle];
        0;
        label="layer 4 (output layer)";
    }
        x1 -> a12;
        x1 -> a22;
        x1 -> a32;
        x2 -> a12;
        x2 -> a22;
        x2 -> a32;
        x3 -> a12;
        x3 -> a22;
        x3 -> a32;
        x4 -> a12;
        x4 -> a22;
        x4 -> a32;


        a12 -> l21
        a22 -> l21
        a32 -> l21
        a12 -> l22
        a22 -> l22
        a32 -> l22

        l21 -> 0
        l22 -> 0
}
```

The use of neural networks are quite vast, as they form the core of 'deep learning'. Applicatons can be found image and
speech recognition, natural language processing, predictive analytics, robotics, and more.

### Core Concepts

#### Neurons & Layers

##### Neurons

Neurons are individually a mathematical operation that can be modelled using the following function:

$$
\begin{equation}
y=\phi (\sum_{j=0}^{m}W_{j}x_j)
\label{basic_neuron}\tag{1}
\end{equation}
$$

Where:

- $\phi$ is the [activation function](#activation-functions)
- $x$ is the input signals to the neuron.
- $W$ is the weight matrix.

> Note: Usually, the $x_0$ input is assigned the value +1, which makes it a bias input. This leaves only m actual inputs
> to the neuron, from $x_1$ to $x_m$

Fundementally this is not too different from the plain linear equation $y=Ax+b$, as $x_0$ is the bias/intercept term and
$A$ is analogous to $W$ as the weight.

##### Layers

Layers are a way of organizing neurons. Neurons in one layer cannot connect to each other, but can connect to all the neurons in the layers that come before and after the current layer.
As a result, equation $\eqref{basic_neuron}$ changes slightly to become:

$$
y_{k}=\varphi\left( \sum_{j=0}^{m} w_{k j} x_{j} \right)
\label{layer_neuron}\tag{2}
$$

Where $k$ represents the $k^{th}$ neuron in a layer.

Here is a visualization of it:  
![Neuron in a Layer](https://upload.wikimedia.org/wikipedia/commons/b/b0/Artificial_neuron.png)

<a href="https://creativecommons.org/licenses/by-sa/2.0" title="Creative Commons Attribution-Share Alike 2.0">CC BY-SA 2.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=125222">Link</a>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ls1dJqZtI7w?si=WZfBEA_uR0RGOEWI" 
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen>
</iframe>

> Terminology: Hidden layers are the layers between the input and output layers

#### Activation Functions

Activation functions tranform models from a linear operation to a non-linear operations. Without this, there is no point of
having multiple nuerons connect to each other since effectively the network can be represented as one big linear function. By introducing non-lineararity, we are able to model more complex functions.

##### ReLU

The Rectified Linear Unit is the simplest form of nonlinearity we can add to a network and is represented by the following
function:

$$
f(x) = \begin{cases}
    x & \text{if } x > 0, \\
    0 & \text{otherwise}.
\end{cases} \qquad\qquad
$$

##### Sigmoid

The sigmoid function is another common non-linear function that can be modelled by the following function:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

#### Loss Function & Metrics

Now that a model can be defined, it is essential to be able to train it in order to learn $W$. In order to do this,
optimization algorithms such as [Gradient Descent](#gradient-descent), which rely on finding the loss (otherwise known as
the cost).

##### Loss

The loss function quantifies how well a model's predictions match the actual target values. It calculates the inconsistency
between predicted and actual values, providing a measure of error. The goal during training is to minimize this loss
function, thereby improving the model's performance.

Common loss functions vary depending on the task at hand. For regression problems, Mean Squared Error (MSE) or Mean Absolute
Error (MAE) are often used. In classification tasks, Cross-Entropy Loss, also known as Log Loss, is frequently employed.

> Loss (specifically validation loss) should rarely be zero. That indicates gross overfitting (the model would perform well
> on sample data, but won't do well in the real world).

##### Metrics (a sidenote)

Metrics are evaluation criteria used to assess the performance of a model. Unlike the loss function, which is utilized
during training to adjust the model's parameters, metrics are employed after training to gauge how well the model performs
on unseen data. Metrics provide insights into different aspects of model performance, such as accuracy, precision, recall,
F1-score, etc.

Choosing appropriate metrics depends on the specific task and requirements. For instance, in a binary classification
problem, accuracy might be a primary metric, but if the classes are imbalanced, precision, recall, or F1-score might provide
a more comprehensive assessment.

##### Gradient Descent, Automatic Differentiation, and Backpropogation

These form the core of how Neural Networks work.

###### Gradient Descent

Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the parameters of
the model. It operates by calculating the gradient of the loss function with respect to the model's parameters and updating
the parameters in the opposite direction of the gradient, moving towards the minimum of the loss function.

##### Automatic Differentiation

Automatic Differentiation is a computational technique used to efficiently and accurately evaluate gradients of functions.
It automatically breaks down complex functions into elementary operations, allowing the calculation of derivatives with
respect to input variables or parameters. This is crucial for training neural networks, as it enables efficient computation
of gradients required by optimization algorithms like Gradient Descent.

##### Backpropagation

Backpropagation is a specific algorithm for efficiently computing gradients of the loss function with respect to the
parameters of a neural network. It utilizes the chain rule of calculus to propagate gradients backward through the network,
starting from the output layer and moving towards the input layer. This allows for efficient calculation of gradients at
each layer, enabling gradient-based optimization algorithms to update the model parameters effectively during training.
Backpropagation is fundamental to training deep neural networks and is the cornerstone of modern deep learning techniques.

### PyTorch

PyTorch is a machine learning library for tensor computation and deep neural networks.

#### Tensors

Tensors are PyTorch's name for multidimensional arrays (ndarray) and are not related to the terminology used in physics. They are used to store weights and do computation on dedicated hardware.

#### Dataset

A dataset in PyTorch defines how to fetch your data (and optionally preprocess it).
Here is a basic example:

```py
from torch.utils.data import Dataset
class Dataset(Dataset):
    def __init__(self, X, y):

        self.X = torch.tensor(X.values, dtype=torch.float32)
        self.y = torch.tensor(y.values, dtype=torch.float32)
        # Here we can preprocess the dataset.
        # It is not preferred to preprocess the dataset in __getitem__  as it will make training slow.
    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

This is quite similar to how other iterable classes in Python are created.

#### Dataloaders

Due to the data hungry nature of deep learning architectures, quite a lot of data is processed. Not all of it can be
processed at the same time, due to hardware limitations. As a result, data is split into batches for processing.

```py
from torch.utils.data import Dataloader
dl = DataLoader(dataset, batch_size=20, shuffle=True)
```

### Full Example

```{python}
import torch
import pandas as pd
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score
from sklearn.datasets import load_iris
from random import seed
from tqdm.notebook import tqdm, trange
from torchinfo import summary
import matplotlib.pyplot as plt
seed(42)
```

Loading the dataset from scikit-learn

```{python}
df = load_iris(as_frame=True).frame
df
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['target']), df['target'], test_size=0.2)
```

```{python}
class IrisDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X.values, dtype=torch.float32)
        self.y = torch.tensor(y.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

Define the model

```{python}
class IrisModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.input = nn.Linear(4, 8)
        self.hidden = nn.Linear(8, 8)
        self.act = nn.ReLU()
        self.output = nn.Linear(8, 3)

    def forward(self, x):
        x = self.act(self.input(x))
        x = self.act(self.hidden(x))
        x = self.output(x)
        return x
```

Defining the dataset and dataloaders

```{python}
train_ds = IrisDataset(X_train, y_train)
test_ds = IrisDataset(X_test, y_test)
train_dl = DataLoader(train_ds, batch_size=20, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=2)
```

Define the model, loss function, optimizer, and number of epochs

```{python}
n_epochs = 800
criterion = nn.CrossEntropyLoss()
model = IrisModel()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
summary(model)
```

The training loop

```{python}
history = []
for epoch in range(n_epochs):
    for i, (X_batch, y_batch) in enumerate(train_dl):
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch.long())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    history.append(loss.item())
```

Plot the change in loss

```{python}
plt.plot(history);
```

```{python}
y_pred = model(test_ds.X)
y_pred = torch.argmax(y_pred, axis=1)
print(accuracy_score(test_ds.y, y_pred))
print(f1_score(test_ds.y, y_pred, average='macro'))
```

## References

1. [d2l.ai](https://d2l.ai)
2. [nnfs.io](https://nnfs.io)
   1. [Layer animation](https://youtu.be/Ls1dJqZtI7w)
3. The following Wikipedia articles: <!--TODO:MARKDOWN LINKS-->
   1. [Loss function](https://en.wikipedia.org/wiki/Loss_function)
   2. [Artificial Neuron](https://en.wikipedia.org/wiki/Artificial_neuron)
   3. [Nueral Network](<https://en.wikipedia.org/wiki/Neural_network_(machine_learning)>)
